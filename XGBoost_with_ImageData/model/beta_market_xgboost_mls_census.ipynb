{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-22T18:51:12.105453Z",
     "start_time": "2025-12-22T18:50:11.594267Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CONFIG\n",
    "UNIFIED_DATA_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_Data_Beta_Markets.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "MIN_PRICE = 100000\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Tree depth options to test\n",
    "TREE_DEPTHS = [6, 8, 10, 12, 14, 16]\n",
    "\n",
    "# Price tiers\n",
    "PRICE_TIERS = {\n",
    "    'very_low': (0, 200000),\n",
    "    'low': (200000, 300000),\n",
    "    'lower_mid': (300000, 400000),\n",
    "    'mid': (400000, 500000),\n",
    "    'upper_mid': (500000, 650000),\n",
    "    'high': (650000, 850000),\n",
    "    'very_high': (850000, 1500000),\n",
    "    'ultra_high': (1500000, np.inf)\n",
    "}\n",
    "\n",
    "# Column name mappings - mapped to YOUR actual column names\n",
    "COLUMN_MAPPINGS = {\n",
    "    'living_sqft': 'sumlivingareasqft',\n",
    "    'lot_sqft': 'lotsizesqft',\n",
    "    'year_built': 'yearbuilt',\n",
    "    'bedrooms': 'bedrooms',\n",
    "    'full_baths': 'bathfull',\n",
    "    'half_baths': 'bathspartialnbr',\n",
    "    'garage_spaces': 'garageparkingnbr',\n",
    "    'latitude': 'situslatitude',\n",
    "    'longitude': 'situslongitude',\n",
    "    'fireplace_code': 'fireplacecode',\n",
    "    'price': 'currentsalesprice'\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# FEATURE DEFINITIONS\n",
    "# ============================================================\n",
    "\n",
    "# Core property features (standardized names that will be mapped)\n",
    "BASE_FEATURES = [\n",
    "    'living_sqft', 'lot_sqft', 'year_built', 'bedrooms',\n",
    "    'full_baths', 'half_baths', 'garage_spaces',\n",
    "    'latitude', 'longitude', 'fireplace_code'\n",
    "]\n",
    "\n",
    "# Census features (REMOVED pct_white per user request)\n",
    "CENSUS_FEATURES = [\n",
    "    'pct_bachelors_degree', 'median_household_income',\n",
    "    'median_home_value', 'pct_owner_occupied', 'unemployment_rate',\n",
    "    'median_age', 'poverty_rate',\n",
    "    'median_gross_rent', 'median_earnings_total'\n",
    "]\n",
    "\n",
    "# Political features\n",
    "POLITICAL_FEATURES = [\n",
    "    'per_gop', 'per_dem', 'per_point_diff'\n",
    "]\n",
    "\n",
    "# Image boolean features (keep as-is, assuming they exist)\n",
    "IMAGE_BOOLEAN = [\n",
    "    'has_hardwood_floors',\n",
    "    'has_granite_countertops',\n",
    "    'has_stainless_steel_appliances',\n",
    "    'has_fireplace',\n",
    "    'has_attached_two_car_garage',\n",
    "    'has_vaulted_ceiling',\n",
    "    'has_open_floor_plan',\n",
    "    'has_updated_fixtures',\n",
    "    'has_crown_molding',\n",
    "    'has_double_vanity',\n",
    "    'has_white_cabinetry',\n",
    "    'has_recessed_lighting',\n",
    "    'has_formal_dining_area',\n",
    "    'has_curb_appeal',\n",
    "    'has_covered_front_porch',\n",
    "    'has_ceiling_fan',\n",
    "    'has_neutral_paint',\n",
    "    'has_tile_flooring',\n",
    "    'has_good_natural_light',\n",
    "    'has_mature_trees',\n",
    "    'has_large_windows',\n",
    "    'has_fenced_yard',\n",
    "    'has_brick_facade',\n",
    "    'has_driveway',\n",
    "    'has_mature_landscaping'\n",
    "]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LOAD AND PREP\n",
    "# ============================================================\n",
    "\n",
    "def load_and_prep(filepath):\n",
    "    \"\"\"Load and prep data with correct column mappings.\"\"\"\n",
    "    print(f\"Loading {filepath}...\")\n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "\n",
    "    # Convert to lowercase for consistent mapping\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    print(f\"Total records: {len(df):,}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "\n",
    "    # Get price column\n",
    "    price_col = COLUMN_MAPPINGS['price']\n",
    "\n",
    "    if price_col not in df.columns:\n",
    "        raise ValueError(f\"Price column '{price_col}' not found in data\")\n",
    "\n",
    "    print(f\"Using price column: '{price_col}'\")\n",
    "\n",
    "    # Filter by price\n",
    "    df = df[df[price_col] >= MIN_PRICE].copy()\n",
    "    print(f\"Records after ${MIN_PRICE:,} price filter: {len(df):,}\")\n",
    "\n",
    "    # Map base feature columns and create engineered features\n",
    "    living_col = COLUMN_MAPPINGS['living_sqft']\n",
    "    lot_col = COLUMN_MAPPINGS['lot_sqft']\n",
    "    year_col = COLUMN_MAPPINGS['year_built']\n",
    "    bed_col = COLUMN_MAPPINGS['bedrooms']\n",
    "    garage_col = COLUMN_MAPPINGS['garage_spaces']\n",
    "\n",
    "    # Engineer features using actual column names\n",
    "    if living_col in df.columns and bed_col in df.columns:\n",
    "        df['sqft_per_bedroom'] = df[living_col] / (df[bed_col] + 1)\n",
    "\n",
    "    if lot_col in df.columns and living_col in df.columns:\n",
    "        df['lot_to_living_ratio'] = df[lot_col] / (df[living_col] + 1)\n",
    "\n",
    "    if year_col in df.columns:\n",
    "        df['property_age'] = 2024 - df[year_col]\n",
    "\n",
    "    if garage_col in df.columns:\n",
    "        df['has_garage'] = (df[garage_col] > 0).astype('int8')\n",
    "\n",
    "    if living_col in df.columns:\n",
    "        df['log_sqft'] = np.log1p(df[living_col])\n",
    "\n",
    "    # Assign price tiers\n",
    "    df['price_tier'] = df[price_col].apply(\n",
    "        lambda p: next((t for t, (l, h) in PRICE_TIERS.items() if l <= p < h), 'ultra_high'))\n",
    "\n",
    "    return df, price_col\n",
    "\n",
    "\n",
    "def get_available_features(df):\n",
    "    \"\"\"Get features that actually exist in the dataframe.\"\"\"\n",
    "    all_features = []\n",
    "    missing_by_group = {}\n",
    "\n",
    "    # Base features - use mapped column names\n",
    "    base_available = []\n",
    "    base_missing = []\n",
    "    for feature in BASE_FEATURES:\n",
    "        actual_col = COLUMN_MAPPINGS.get(feature, feature)\n",
    "        if actual_col in df.columns:\n",
    "            base_available.append(actual_col)\n",
    "        else:\n",
    "            base_missing.append(feature)\n",
    "\n",
    "    all_features.extend(base_available)\n",
    "    missing_by_group['BASE'] = base_missing\n",
    "\n",
    "    # Census, Political, Image features - check directly\n",
    "    for group_name, feature_list in [('CENSUS', CENSUS_FEATURES),\n",
    "                                     ('POLITICAL', POLITICAL_FEATURES),\n",
    "                                     ('IMAGE_BOOLEAN', IMAGE_BOOLEAN)]:\n",
    "        available = [f for f in feature_list if f in df.columns]\n",
    "        missing = [f for f in feature_list if f not in df.columns]\n",
    "        all_features.extend(available)\n",
    "        missing_by_group[group_name] = missing\n",
    "\n",
    "    # Add engineered features\n",
    "    engineered = ['sqft_per_bedroom', 'lot_to_living_ratio', 'property_age',\n",
    "                  'has_garage', 'log_sqft']\n",
    "    all_features.extend([f for f in engineered if f in df.columns])\n",
    "\n",
    "    return all_features, missing_by_group\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING WITH DEPTH TUNING\n",
    "# ============================================================\n",
    "\n",
    "def train_single_depth(train_df, test_df, features, price_col, max_depth):\n",
    "    \"\"\"Train model with specific max_depth.\"\"\"\n",
    "\n",
    "    # Impute using training data only\n",
    "    train_medians = train_df[features].median()\n",
    "    train_filled = train_df[features].fillna(train_medians)\n",
    "    test_filled = test_df[features].fillna(train_medians)\n",
    "\n",
    "    X_train, y_train = train_filled.values, train_df[price_col].values\n",
    "    X_test, y_test = test_filled.values, test_df[price_col].values\n",
    "\n",
    "    # Log transform\n",
    "    y_train_model = np.log1p(y_train)\n",
    "\n",
    "    # Train model\n",
    "    model = XGBRegressor(\n",
    "        objective='reg:quantileerror',\n",
    "        quantile_alpha=0.5,\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=max_depth,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        tree_method='hist',\n",
    "        verbosity=0\n",
    "    )\n",
    "    model.fit(X_train, y_train_model, verbose=False)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = np.expm1(model.predict(X_test))\n",
    "\n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return model, mae, mape, r2, y_test, y_pred\n",
    "\n",
    "\n",
    "def train_tier_with_depth_tuning(tier_df, features, price_col, tier_name):\n",
    "    \"\"\"Train tier with multiple depths and select best.\"\"\"\n",
    "\n",
    "    # Split data FIRST\n",
    "    train_df, test_df = train_test_split(tier_df, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "    best_mape = float('inf')\n",
    "    best_depth = None\n",
    "    best_model = None\n",
    "    best_metrics = None\n",
    "    best_predictions = None\n",
    "\n",
    "    depth_results = []\n",
    "\n",
    "    # Test each depth\n",
    "    for depth in TREE_DEPTHS:\n",
    "        model, mae, mape, r2, y_test, y_pred = train_single_depth(\n",
    "            train_df.copy(), test_df.copy(), features, price_col, depth\n",
    "        )\n",
    "\n",
    "        depth_results.append({\n",
    "            'depth': depth,\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'r2': r2\n",
    "        })\n",
    "\n",
    "        # Track best\n",
    "        if mape < best_mape:\n",
    "            best_mape = mape\n",
    "            best_depth = depth\n",
    "            best_model = model\n",
    "            best_metrics = {\n",
    "                'n_train': len(train_df),\n",
    "                'n_test': len(test_df),\n",
    "                'mae': mae,\n",
    "                'mape': mape,\n",
    "                'r2': r2,\n",
    "                'best_depth': depth\n",
    "            }\n",
    "            best_predictions = pd.DataFrame({\n",
    "                'actual': y_test,\n",
    "                'predicted': y_pred,\n",
    "                'tier': tier_name\n",
    "            })\n",
    "\n",
    "    # Feature importance from best model\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'metrics': best_metrics,\n",
    "        'predictions': best_predictions,\n",
    "        'importance': importance,\n",
    "        'depth_results': pd.DataFrame(depth_results)\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution with depth tuning.\"\"\"\n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"STRATIFIED XGBoost AVM - WITH DEPTH TUNING\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Testing tree depths: {TREE_DEPTHS}\")\n",
    "\n",
    "    # Load data\n",
    "    df, price_col = load_and_prep(UNIFIED_DATA_PATH)\n",
    "\n",
    "    # Get available features\n",
    "    all_available, missing_by_group = get_available_features(df)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FEATURE AVAILABILITY\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    for group_name, missing in missing_by_group.items():\n",
    "        if group_name == 'BASE':\n",
    "            available_count = len([f for f in BASE_FEATURES if COLUMN_MAPPINGS.get(f, f) in df.columns])\n",
    "            total = len(BASE_FEATURES)\n",
    "        elif group_name == 'CENSUS':\n",
    "            available_count = len(CENSUS_FEATURES) - len(missing)\n",
    "            total = len(CENSUS_FEATURES)\n",
    "        elif group_name == 'POLITICAL':\n",
    "            available_count = len(POLITICAL_FEATURES) - len(missing)\n",
    "            total = len(POLITICAL_FEATURES)\n",
    "        else:  # IMAGE_BOOLEAN\n",
    "            available_count = len(IMAGE_BOOLEAN) - len(missing)\n",
    "            total = len(IMAGE_BOOLEAN)\n",
    "\n",
    "        print(f\"\\n{group_name}:\")\n",
    "        print(f\"  ✓ Available: {available_count}/{total}\")\n",
    "        if missing and len(missing) <= 10:\n",
    "            print(f\"  ✗ Missing: {', '.join(missing[:10])}\")\n",
    "        elif missing:\n",
    "            print(f\"  ✗ Missing: {len(missing)} features\")\n",
    "\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"TOTAL FEATURES: {len(all_available)}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    # Drop records with missing price\n",
    "    df = df.dropna(subset=[price_col])\n",
    "\n",
    "    # Train all tiers\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TRAINING MODELS BY PRICE TIER (WITH DEPTH TUNING)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    all_results = {}\n",
    "    all_preds = []\n",
    "    all_importance = []\n",
    "    all_depth_results = []\n",
    "\n",
    "    for tier_name, (low, high) in PRICE_TIERS.items():\n",
    "        tier_df = df[df['price_tier'] == tier_name].copy()\n",
    "\n",
    "        if len(tier_df) < 50:\n",
    "            print(f\"\\nSkip {tier_name}: {len(tier_df)} samples (need 50+)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{tier_name} (${low / 1000:.0f}K-${high / 1000:.0f}K): {len(tier_df):,} samples\")\n",
    "        print(f\"  Testing depths {TREE_DEPTHS}...\", end=\" \")\n",
    "\n",
    "        # Train with depth tuning\n",
    "        result = train_tier_with_depth_tuning(tier_df, all_available, price_col, tier_name)\n",
    "        all_results[tier_name] = result\n",
    "        all_preds.append(result['predictions'])\n",
    "\n",
    "        m = result['metrics']\n",
    "        print(f\"✓ Best depth={m['best_depth']}\")\n",
    "        print(f\"  → MAE=${m['mae']:,.0f} | MAPE={m['mape']:.2f}% | R²={m['r2']:.3f}\")\n",
    "\n",
    "        # Collect importance\n",
    "        result['importance']['tier'] = tier_name\n",
    "        all_importance.append(result['importance'])\n",
    "\n",
    "        # Collect depth results\n",
    "        result['depth_results']['tier'] = tier_name\n",
    "        all_depth_results.append(result['depth_results'])\n",
    "\n",
    "    # Overall metrics\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"OVERALL PERFORMANCE\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    preds = pd.concat(all_preds, ignore_index=True)\n",
    "    overall_mae = mean_absolute_error(preds['actual'], preds['predicted'])\n",
    "    overall_mape = np.mean(np.abs((preds['actual'] - preds['predicted']) / preds['actual']) * 100)\n",
    "    overall_r2 = r2_score(preds['actual'], preds['predicted'])\n",
    "\n",
    "    print(f\"\\nMAE: ${overall_mae:,.0f}\")\n",
    "    print(f\"MAPE: {overall_mape:.2f}%\")\n",
    "    print(f\"R²: {overall_r2:.4f}\")\n",
    "    print(f\"Time: {time.time() - start:.1f}s\")\n",
    "\n",
    "    # Optimal depths by tier\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"OPTIMAL TREE DEPTHS BY TIER\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    for tier_name, result in all_results.items():\n",
    "        best_depth = result['metrics']['best_depth']\n",
    "        mape = result['metrics']['mape']\n",
    "        print(f\"{tier_name:15} → depth={best_depth:2d} (MAPE={mape:.2f}%)\")\n",
    "\n",
    "    # Top features across all tiers\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TOP 20 MOST IMPORTANT FEATURES (AVERAGE ACROSS TIERS)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    importance_df = pd.concat(all_importance, ignore_index=True)\n",
    "    avg_importance = importance_df.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\nFeature                              Importance\")\n",
    "    print(\"-\" * 70)\n",
    "    for feature, imp in avg_importance.head(20).items():\n",
    "        print(f\"{feature:<35} {imp:.4f}\")\n",
    "\n",
    "    # Save results\n",
    "    import os\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    preds.to_csv(f\"{OUTPUT_DIR}/predictions_depth_tuned.csv\", index=False)\n",
    "    avg_importance.to_csv(f\"{OUTPUT_DIR}/feature_importance_depth_tuned.csv\")\n",
    "\n",
    "    # Save metrics by tier with optimal depth\n",
    "    metrics_df = pd.DataFrame([\n",
    "        {'tier': k, **v['metrics']}\n",
    "        for k, v in all_results.items()\n",
    "    ])\n",
    "    metrics_df.to_csv(f\"{OUTPUT_DIR}/metrics_by_tier_depth_tuned.csv\", index=False)\n",
    "\n",
    "    # Save depth comparison results\n",
    "    depth_comparison = pd.concat(all_depth_results, ignore_index=True)\n",
    "    depth_comparison.to_csv(f\"{OUTPUT_DIR}/depth_comparison_by_tier.csv\", index=False)\n",
    "\n",
    "    print(f\"\\n✓ Saved results to {OUTPUT_DIR}\")\n",
    "    print(\"  - predictions_depth_tuned.csv\")\n",
    "    print(\"  - feature_importance_depth_tuned.csv\")\n",
    "    print(\"  - metrics_by_tier_depth_tuned.csv\")\n",
    "    print(\"  - depth_comparison_by_tier.csv\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"✓ Used {len(all_available)} features\")\n",
    "    print(f\"✓ Tested {len(TREE_DEPTHS)} tree depths per tier\")\n",
    "    print(f\"✓ Trained on {len(preds)} test predictions\")\n",
    "    print(f\"✓ Average MAPE: {overall_mape:.2f}%\")\n",
    "    print(f\"✓ Removed pct_white (race-related feature)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STRATIFIED XGBoost AVM - WITH DEPTH TUNING\n",
      "======================================================================\n",
      "Testing tree depths: [6, 8, 10, 12, 14, 16]\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_Data_Beta_Markets.csv...\n",
      "Total records: 5,000\n",
      "Total columns: 270\n",
      "Using price column: 'currentsalesprice'\n",
      "Records after $100,000 price filter: 1,979\n",
      "\n",
      "======================================================================\n",
      "FEATURE AVAILABILITY\n",
      "======================================================================\n",
      "\n",
      "BASE:\n",
      "  ✓ Available: 10/10\n",
      "\n",
      "CENSUS:\n",
      "  ✓ Available: 9/9\n",
      "\n",
      "POLITICAL:\n",
      "  ✓ Available: 3/3\n",
      "\n",
      "IMAGE_BOOLEAN:\n",
      "  ✓ Available: 0/25\n",
      "  ✗ Missing: 25 features\n",
      "\n",
      "======================================================================\n",
      "TOTAL FEATURES: 27\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TRAINING MODELS BY PRICE TIER (WITH DEPTH TUNING)\n",
      "======================================================================\n",
      "\n",
      "very_low ($0K-$200K): 690 samples\n",
      "  Testing depths [6, 8, 10, 12, 14, 16]... ✓ Best depth=10\n",
      "  → MAE=$25,314 | MAPE=17.30% | R²=-0.096\n",
      "\n",
      "low ($200K-$300K): 488 samples\n",
      "  Testing depths [6, 8, 10, 12, 14, 16]... ✓ Best depth=12\n",
      "  → MAE=$24,882 | MAPE=10.55% | R²=-0.153\n",
      "\n",
      "lower_mid ($300K-$400K): 312 samples\n",
      "  Testing depths [6, 8, 10, 12, 14, 16]... ✓ Best depth=14\n",
      "  → MAE=$21,731 | MAPE=6.34% | R²=-0.058\n",
      "\n",
      "mid ($400K-$500K): 145 samples\n",
      "  Testing depths [6, 8, 10, 12, 14, 16]... ✓ Best depth=8\n",
      "  → MAE=$22,969 | MAPE=5.16% | R²=-0.050\n",
      "\n",
      "upper_mid ($500K-$650K): 150 samples\n",
      "  Testing depths [6, 8, 10, 12, 14, 16]... ✓ Best depth=12\n",
      "  → MAE=$42,206 | MAPE=7.58% | R²=-0.285\n",
      "\n",
      "high ($650K-$850K): 81 samples\n",
      "  Testing depths [6, 8, 10, 12, 14, 16]... ✓ Best depth=10\n",
      "  → MAE=$63,402 | MAPE=8.26% | R²=-0.616\n",
      "\n",
      "very_high ($850K-$1500K): 61 samples\n",
      "  Testing depths [6, 8, 10, 12, 14, 16]... ✓ Best depth=8\n",
      "  → MAE=$214,277 | MAPE=18.59% | R²=-0.573\n",
      "\n",
      "ultra_high ($1500K-$infK): 52 samples\n",
      "  Testing depths [6, 8, 10, 12, 14, 16]... ✓ Best depth=6\n",
      "  → MAE=$1,918,973 | MAPE=56.57% | R²=-0.244\n",
      "\n",
      "======================================================================\n",
      "OVERALL PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "MAE: $85,727\n",
      "MAPE: 13.04%\n",
      "R²: 0.5340\n",
      "Time: 60.5s\n",
      "\n",
      "======================================================================\n",
      "OPTIMAL TREE DEPTHS BY TIER\n",
      "======================================================================\n",
      "very_low        → depth=10 (MAPE=17.30%)\n",
      "low             → depth=12 (MAPE=10.55%)\n",
      "lower_mid       → depth=14 (MAPE=6.34%)\n",
      "mid             → depth= 8 (MAPE=5.16%)\n",
      "upper_mid       → depth=12 (MAPE=7.58%)\n",
      "high            → depth=10 (MAPE=8.26%)\n",
      "very_high       → depth= 8 (MAPE=18.59%)\n",
      "ultra_high      → depth= 6 (MAPE=56.57%)\n",
      "\n",
      "======================================================================\n",
      "TOP 20 MOST IMPORTANT FEATURES (AVERAGE ACROSS TIERS)\n",
      "======================================================================\n",
      "\n",
      "Feature                              Importance\n",
      "----------------------------------------------------------------------\n",
      "log_sqft                            0.0770\n",
      "per_point_diff                      0.0677\n",
      "property_age                        0.0671\n",
      "lot_to_living_ratio                 0.0669\n",
      "has_garage                          0.0662\n",
      "sqft_per_bedroom                    0.0662\n",
      "per_dem                             0.0614\n",
      "per_gop                             0.0575\n",
      "situslongitude                      0.0552\n",
      "bathfull                            0.0546\n",
      "yearbuilt                           0.0545\n",
      "situslatitude                       0.0544\n",
      "garageparkingnbr                    0.0532\n",
      "bathspartialnbr                     0.0524\n",
      "bedrooms                            0.0509\n",
      "lotsizesqft                         0.0452\n",
      "sumlivingareasqft                   0.0437\n",
      "fireplacecode                       0.0059\n",
      "median_gross_rent                   0.0000\n",
      "median_home_value                   0.0000\n",
      "\n",
      "✓ Saved results to /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\n",
      "  - predictions_depth_tuned.csv\n",
      "  - feature_importance_depth_tuned.csv\n",
      "  - metrics_by_tier_depth_tuned.csv\n",
      "  - depth_comparison_by_tier.csv\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "✓ Used 27 features\n",
      "✓ Tested 6 tree depths per tier\n",
      "✓ Trained on 399 test predictions\n",
      "✓ Average MAPE: 13.04%\n",
      "✓ Removed pct_white (race-related feature)\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f9aa789b73cb03e7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
