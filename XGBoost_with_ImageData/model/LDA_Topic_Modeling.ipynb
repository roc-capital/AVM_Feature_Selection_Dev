{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-19T07:06:53.323580Z",
     "start_time": "2025-12-19T07:06:51.852355Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = r'/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Main_MLS_w_Features_2025-12-18-1053.csv' # first set of 12K Data from Farbod\n",
    "\n",
    "df = pd.read_csv(file)\n",
    "df = df.rename(columns = {'PARSED_OUTPUT':'PARSED'})"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sh/kytnhf5923bgf4ntjsv_h9t80000gn/T/ipykernel_98379/292934338.py:5: DtypeWarning: Columns (328,344,359,365,375,378,380,394,395,399,401,402,404,405,406,466,470,473,479,481,484,492,493,502,512,514,515,516,519,527) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T22:42:44.905784Z",
     "start_time": "2025-12-18T22:42:08.410668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_features_with_lda(df, n1=300, n2=20, n3=20,\n",
    "                               dedupe_threshold=3, dedupe_top_n=2000,\n",
    "                               n_topics=10, lda_features=500):\n",
    "    \"\"\"\n",
    "    Extract features with LDA topic modeling - optimized for performance\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_topics : int\n",
    "        Number of latent topics to discover (e.g., 10 = Modern, Traditional, Luxury, etc.)\n",
    "    lda_features : int\n",
    "        Number of top features to use for LDA (recommend 300-1000)\n",
    "    \"\"\"\n",
    "    from collections import Counter, defaultdict\n",
    "    from itertools import combinations\n",
    "    import ast\n",
    "    import numpy as np\n",
    "    from rapidfuzz import process, distance\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    import pandas as pd\n",
    "\n",
    "    print(\"Parsing...\")\n",
    "    prop_feats = []\n",
    "    for s in df['PARSED'].values:\n",
    "        try:\n",
    "            d = ast.literal_eval(s) if pd.notna(s) else {}\n",
    "            feats = set()\n",
    "            for img in d.values():\n",
    "                feats.update(img.get('prominent_features', []))\n",
    "            prop_feats.append(frozenset(feats))\n",
    "        except:\n",
    "            prop_feats.append(frozenset())\n",
    "\n",
    "    # Count ALL features first\n",
    "    print(\"Counting all features...\")\n",
    "    all_singles = Counter()\n",
    "    for feats in prop_feats:\n",
    "        all_singles.update(feats)\n",
    "\n",
    "    # Dedupe top N features\n",
    "    if dedupe_top_n and dedupe_threshold > 0:\n",
    "        top_for_dedupe = [f for f, _ in all_singles.most_common(dedupe_top_n)]\n",
    "        print(f\"Deduplicating top {len(top_for_dedupe)} features...\")\n",
    "\n",
    "        canonical_map = {}\n",
    "        canonical_list = []\n",
    "\n",
    "        for feat in sorted(top_for_dedupe):\n",
    "            if not canonical_list:\n",
    "                canonical_map[feat] = feat\n",
    "                canonical_list.append(feat)\n",
    "                continue\n",
    "\n",
    "            match = process.extractOne(\n",
    "                feat.lower(),\n",
    "                [c.lower() for c in canonical_list],\n",
    "                scorer=distance.Levenshtein.distance,\n",
    "                score_cutoff=dedupe_threshold\n",
    "            )\n",
    "\n",
    "            if match:\n",
    "                canonical_map[feat] = canonical_list[match[2]]\n",
    "            else:\n",
    "                canonical_map[feat] = feat\n",
    "                canonical_list.append(feat)\n",
    "\n",
    "        for feat in all_singles:\n",
    "            if feat not in canonical_map:\n",
    "                canonical_map[feat] = feat\n",
    "    else:\n",
    "        canonical_map = {f: f for f in all_singles}\n",
    "\n",
    "    # Apply canonicalization\n",
    "    prop_feats = [frozenset(canonical_map[f] for f in feats) for feats in prop_feats]\n",
    "\n",
    "    # Count singles\n",
    "    print(\"Counting singles...\")\n",
    "    singles = Counter()\n",
    "    for feats in prop_feats:\n",
    "        singles.update(feats)\n",
    "    top_singles = [f for f, _ in singles.most_common(n1)]\n",
    "\n",
    "    # ========== LDA TOPIC MODELING ==========\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"RUNNING LDA TOPIC MODELING\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Get top features for LDA\n",
    "    lda_top_features = [f for f, _ in singles.most_common(lda_features)]\n",
    "    lda_feat_idx = {f: i for i, f in enumerate(lda_top_features)}\n",
    "\n",
    "    # Create document-term matrix\n",
    "    print(f\"Building document-term matrix with {len(lda_top_features)} features...\")\n",
    "    doc_term_matrix = np.zeros((len(df), len(lda_top_features)), dtype=np.int8)\n",
    "\n",
    "    for row, feats in enumerate(prop_feats):\n",
    "        for feat in feats:\n",
    "            if feat in lda_feat_idx:\n",
    "                doc_term_matrix[row, lda_feat_idx[feat]] = 1\n",
    "\n",
    "    # Fit LDA\n",
    "    print(f\"Fitting LDA with {n_topics} topics...\")\n",
    "    lda_model = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        max_iter=20,\n",
    "        learning_method='online',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Get topic distributions for each property\n",
    "    doc_topic_dist = lda_model.fit_transform(doc_term_matrix)\n",
    "\n",
    "    # Add topic columns to dataframe (OPTIMIZED - no fragmentation, no duplicates)\n",
    "    print(\"Adding topic distributions to dataframe...\")\n",
    "\n",
    "    # Drop any existing topic columns first to avoid duplicates\n",
    "    existing_topic_cols = [f'topic_{i+1}' for i in range(n_topics)] + ['dominant_topic']\n",
    "    df = df.drop(columns=[col for col in existing_topic_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    topic_cols = {}\n",
    "    for topic_idx in range(n_topics):\n",
    "        topic_cols[f'topic_{topic_idx+1}'] = doc_topic_dist[:, topic_idx]\n",
    "\n",
    "    topic_cols['dominant_topic'] = np.argmax(doc_topic_dist, axis=1) + 1\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(topic_cols, index=df.index)], axis=1)\n",
    "\n",
    "    # Analyze topics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TOP FEATURES BY TOPIC\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    topic_features = []\n",
    "    n_top_words = 15\n",
    "\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_feature_indices = topic.argsort()[-n_top_words:][::-1]\n",
    "        top_features = [lda_top_features[i] for i in top_feature_indices]\n",
    "        top_weights = [topic[i] for i in top_feature_indices]\n",
    "\n",
    "        print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "        for feat, weight in zip(top_features, top_weights):\n",
    "            print(f\"  {feat:40s} {weight:.4f}\")\n",
    "\n",
    "        topic_features.append({\n",
    "            'topic': topic_idx + 1,\n",
    "            'top_features': top_features,\n",
    "            'weights': top_weights\n",
    "        })\n",
    "\n",
    "    # Create topic summary dataframe (FIXED - ensure single values, not Series)\n",
    "    topic_summary_data = []\n",
    "    for t in topic_features:\n",
    "        topic_num = t['topic']\n",
    "        topic_summary_data.append({\n",
    "            'topic': topic_num,\n",
    "            'top_5_features': ', '.join(t['top_features'][:5]),\n",
    "            'num_properties': int((df['dominant_topic'] == topic_num).sum()),\n",
    "            'avg_topic_strength': float(df[f\"topic_{topic_num}\"].mean())\n",
    "        })\n",
    "\n",
    "    topic_summary = pd.DataFrame(topic_summary_data)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TOPIC SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(topic_summary.to_string(index=False))\n",
    "\n",
    "    # ========== CONTINUE WITH REGULAR FEATURE EXTRACTION ==========\n",
    "\n",
    "    # Create singles columns (OPTIMIZED - no fragmentation, no duplicates)\n",
    "    print(\"\\nCreating single feature columns...\")\n",
    "    feat_idx = {f: i for i, f in enumerate(top_singles)}\n",
    "    singles_data = np.zeros((len(df), len(top_singles)), dtype=np.int8)\n",
    "\n",
    "    for row, feats in enumerate(prop_feats):\n",
    "        for feat in feats:\n",
    "            if feat in feat_idx:\n",
    "                singles_data[row, feat_idx[feat]] = 1\n",
    "\n",
    "    # Build all single columns at once\n",
    "    single_cols = {}\n",
    "    for i, feat in enumerate(top_singles):\n",
    "        col_name = feat.replace(' ', '_')[:50]\n",
    "        # Ensure unique column names\n",
    "        if col_name in df.columns:\n",
    "            col_name = f\"{col_name}_{i}\"\n",
    "        single_cols[col_name] = singles_data[:, i]\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(single_cols, index=df.index)], axis=1)\n",
    "\n",
    "    # Pairs (OPTIMIZED - no fragmentation, no duplicates)\n",
    "    print(\"Counting pairs...\")\n",
    "    top_set = set(top_singles)\n",
    "    filtered_feats = [[f for f in feats if f in top_set] for feats in prop_feats]\n",
    "\n",
    "    pairs = Counter()\n",
    "    for feats in filtered_feats:\n",
    "        if len(feats) >= 2:\n",
    "            pairs.update(combinations(sorted(feats), 2))\n",
    "\n",
    "    top_pairs = [p for p, _ in pairs.most_common(n2)]\n",
    "\n",
    "    # Drop existing pair columns\n",
    "    existing_pair_cols = [f'pair_{i}' for i in range(1, n2+1)]\n",
    "    df = df.drop(columns=[col for col in existing_pair_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    pair_definitions = []\n",
    "    pair_cols = {}\n",
    "\n",
    "    for i, (f1, f2) in enumerate(top_pairs, 1):\n",
    "        i1, i2 = feat_idx[f1], feat_idx[f2]\n",
    "        pair_cols[f\"pair_{i}\"] = (singles_data[:, i1] & singles_data[:, i2]).astype(np.int8)\n",
    "        pair_definitions.append({\n",
    "            'column_name': f'pair_{i}',\n",
    "            'feature_1': f1,\n",
    "            'feature_2': f2,\n",
    "            'count': pairs[(f1, f2)]\n",
    "        })\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(pair_cols, index=df.index)], axis=1)\n",
    "\n",
    "    # Triples (OPTIMIZED - no fragmentation, no duplicates)\n",
    "    print(\"Counting triples...\")\n",
    "    triples = Counter()\n",
    "    for feats in filtered_feats:\n",
    "        if len(feats) >= 3:\n",
    "            triples.update(combinations(sorted(feats), 3))\n",
    "\n",
    "    top_triples = [t for t, _ in triples.most_common(n3)]\n",
    "\n",
    "    # Drop existing triple columns\n",
    "    existing_triple_cols = [f'trip_{i}' for i in range(1, n3+1)]\n",
    "    df = df.drop(columns=[col for col in existing_triple_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    triple_definitions = []\n",
    "    triple_cols = {}\n",
    "\n",
    "    for i, (f1, f2, f3) in enumerate(top_triples, 1):\n",
    "        i1, i2, i3 = feat_idx[f1], feat_idx[f2], feat_idx[f3]\n",
    "        triple_cols[f\"trip_{i}\"] = (singles_data[:, i1] & singles_data[:, i2] & singles_data[:, i3]).astype(np.int8)\n",
    "        triple_definitions.append({\n",
    "            'column_name': f'trip_{i}',\n",
    "            'feature_1': f1,\n",
    "            'feature_2': f2,\n",
    "            'feature_3': f3,\n",
    "            'count': triples[(f1, f2, f3)]\n",
    "        })\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(triple_cols, index=df.index)], axis=1)\n",
    "\n",
    "    # Conditions (OPTIMIZED - no fragmentation, no duplicates)\n",
    "    print(\"Adding conditions...\")\n",
    "    parsed = [ast.literal_eval(s) if pd.notna(s) else {} for s in df['PARSED'].values]\n",
    "\n",
    "    # Drop existing condition columns\n",
    "    condition_col_names = []\n",
    "    for pre in ['gran', 'high']:\n",
    "        for suf in ['_in', '_ex', '']:\n",
    "            condition_col_names.append(f'{pre}_c{suf}')\n",
    "    df = df.drop(columns=[col for col in condition_col_names if col in df.columns], errors='ignore')\n",
    "\n",
    "    condition_cols = {}\n",
    "    for pre, key in [('gran', 'granular_condition_num'), ('high', 'high_condition_num')]:\n",
    "        for suf, typ in [('_in', 'Indoor'), ('_ex', 'Exterior'), ('', None)]:\n",
    "            col_name = f'{pre}_c{suf}'\n",
    "            condition_cols[col_name] = [\n",
    "                np.mean([img[key] for img in d.values()\n",
    "                        if key in img and (not typ or img.get('image_type') == typ)])\n",
    "                if d else np.nan\n",
    "                for d in parsed\n",
    "            ]\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(condition_cols, index=df.index)], axis=1)\n",
    "\n",
    "    print(f\"\\n✓ Done: {df.shape}\")\n",
    "    print(f\"✓ Added {n_topics} topic columns: topic_1 to topic_{n_topics}\")\n",
    "    print(f\"✓ Added dominant_topic column\")\n",
    "    print(f\"✓ Added {len(single_cols)} single features\")\n",
    "    print(f\"✓ Added {len(pair_cols)} pair features\")\n",
    "    print(f\"✓ Added {len(triple_cols)} triple features\")\n",
    "    print(f\"✓ Added {len(condition_cols)} condition features\")\n",
    "    print(f\"✓ No fragmentation warnings!\")\n",
    "    print(f\"✓ No duplicate columns!\")\n",
    "\n",
    "    pairs_df = pd.DataFrame(pair_definitions)\n",
    "    triples_df = pd.DataFrame(triple_definitions)\n",
    "\n",
    "    return df, pairs_df, triples_df, topic_summary, lda_model\n",
    "\n",
    "\n",
    "# ========== USAGE ==========\n",
    "\n",
    "# Run with LDA (10 topics, using top 500 features)\n",
    "df, pairs, triples, topics, lda_model = extract_features_with_lda(\n",
    "    df,\n",
    "    n1=300,           # top single features\n",
    "    n2=20,            # top pairs\n",
    "    n3=20,            # top triples\n",
    "    dedupe_threshold=3,\n",
    "    dedupe_top_n=2000,\n",
    "    n_topics=10,      # number of latent topics\n",
    "    lda_features=500  # features to use for LDA\n",
    ")\n",
    "\n",
    "# Save results\n",
    "pairs.to_csv('pair_definitions.csv', index=False)\n",
    "triples.to_csv('triple_definitions.csv', index=False)\n",
    "topics.to_csv('topic_summary.csv', index=False)\n",
    "\n",
    "# Explore topics\n",
    "print(\"\\nProperties in Topic 1:\")\n",
    "print(df[df['dominant_topic'] == 1][['topic_1', 'topic_2', 'topic_3']].head())\n",
    "\n",
    "# Find properties with high Topic 3 score\n",
    "print(\"\\nTop properties for Topic 3:\")\n",
    "print(df.nlargest(10, 'topic_3')[['dominant_topic', 'topic_3']].head())"
   ],
   "id": "6ee65e0589489c20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing...\n",
      "Counting all features...\n",
      "Deduplicating top 2000 features...\n",
      "Counting singles...\n",
      "\n",
      "============================================================\n",
      "RUNNING LDA TOPIC MODELING\n",
      "============================================================\n",
      "Building document-term matrix with 500 features...\n",
      "Fitting LDA with 10 topics...\n",
      "iteration: 1 of max_iter: 20\n",
      "iteration: 2 of max_iter: 20\n",
      "iteration: 3 of max_iter: 20\n",
      "iteration: 4 of max_iter: 20\n",
      "iteration: 5 of max_iter: 20\n",
      "iteration: 6 of max_iter: 20\n",
      "iteration: 7 of max_iter: 20\n",
      "iteration: 8 of max_iter: 20\n",
      "iteration: 9 of max_iter: 20\n",
      "iteration: 10 of max_iter: 20\n",
      "iteration: 11 of max_iter: 20\n",
      "iteration: 12 of max_iter: 20\n",
      "iteration: 13 of max_iter: 20\n",
      "iteration: 14 of max_iter: 20\n",
      "iteration: 15 of max_iter: 20\n",
      "iteration: 16 of max_iter: 20\n",
      "iteration: 17 of max_iter: 20\n",
      "iteration: 18 of max_iter: 20\n",
      "iteration: 19 of max_iter: 20\n",
      "iteration: 20 of max_iter: 20\n",
      "Adding topic distributions to dataframe...\n",
      "\n",
      "============================================================\n",
      "TOP FEATURES BY TOPIC\n",
      "============================================================\n",
      "\n",
      "Topic 1:\n",
      "  standard bedroom size                    552.5699\n",
      "  Neutral finishes                         384.0098\n",
      "  Carpeted floor                           357.3529\n",
      "  carpet flooring                          284.9554\n",
      "  Neutral paint                            273.4145\n",
      "  Ceiling fan                              243.9788\n",
      "  vaulted ceiling                          215.8883\n",
      "  sliding glass door to exterior           200.0823\n",
      "  single sink vanity                       199.5419\n",
      "  fireplace                                198.2808\n",
      "  good curb appeal                         192.6319\n",
      "  standard fixtures                        177.2070\n",
      "  single window for natural light          172.7203\n",
      "  dark cabinetry                           168.1273\n",
      "  breakfast bar/peninsula                  164.2777\n",
      "\n",
      "Topic 2:\n",
      "  neutral decor                            581.6921\n",
      "  Hardwood flooring                        575.4120\n",
      "  stainless steel appliances               539.5289\n",
      "  Neutral finishes                         525.2864\n",
      "  Good natural light                       462.2441\n",
      "  natural light                            447.1463\n",
      "  Neutral paint                            427.1701\n",
      "  good lighting                            422.0625\n",
      "  tile floor                               389.2851\n",
      "  modern fixtures                          388.0310\n",
      "  granite counters                         380.8502\n",
      "  ample natural light                      371.3267\n",
      "  Ceiling fan                              324.1309\n",
      "  compact layout                           301.9180\n",
      "  modern finishes                          293.8085\n",
      "\n",
      "Topic 3:\n",
      "  brick exterior                           523.9755\n",
      "  basic fixtures                           366.0060\n",
      "  mature trees / landscaping               306.8968\n",
      "  mature trees providing shade             300.6854\n",
      "  moderate curb appeal                     279.6643\n",
      "  single-story ranch style                 276.4526\n",
      "  neutral tile finishes                    230.9251\n",
      "  level lawn area                          222.5181\n",
      "  mature shade tree                        220.2615\n",
      "  brick facade                             188.4041\n",
      "  well-maintained lawn and landscaping     185.1327\n",
      "  large front window                       169.7189\n",
      "  mature trees on lot                      149.9347\n",
      "  good ceiling height                      144.3177\n",
      "  brick fireplace                          144.2443\n",
      "\n",
      "Topic 4:\n",
      "  mature tree                              1124.6219\n",
      "  Hardwood flooring                        853.9202\n",
      "  Ceiling fan                              590.5894\n",
      "  natural light from window                586.0862\n",
      "  Good natural light                       585.8009\n",
      "  tile floor                               486.5322\n",
      "  level lawn                               464.1254\n",
      "  driveway access                          446.2613\n",
      "  fenced area                              439.4770\n",
      "  well-maintained lawn                     426.7176\n",
      "  mature landscaping                       422.5511\n",
      "  Neutral paint                            421.2700\n",
      "  large open lawn                          403.8501\n",
      "  storage shed                             349.7078\n",
      "  compact layout                           345.5863\n",
      "\n",
      "Topic 5:\n",
      "  formal dining area                       683.2420\n",
      "  chandelier lighting                      435.9658\n",
      "  window natural light                     331.9790\n",
      "  natural light from window                326.5985\n",
      "  Hardwood flooring                        309.4014\n",
      "  well-maintained landscaping              303.2073\n",
      "  good curb appeal                         286.0989\n",
      "  neutral updated finishes                 280.6197\n",
      "  en-suite bathroom access                 244.2822\n",
      "  attached garage visible                  211.5875\n",
      "  wood flooring                            200.3361\n",
      "  carpet flooring                          188.9982\n",
      "  mature trees / landscaping               187.5153\n",
      "  proximity to kitchen                     183.5095\n",
      "  neutral decor                            178.8132\n",
      "\n",
      "Topic 6:\n",
      "  Hardwood flooring                        1937.3838\n",
      "  stainless steel appliances               1542.3532\n",
      "  granite counters                         1307.7852\n",
      "  Ceiling fan                              1304.3226\n",
      "  tile floor                               1225.3393\n",
      "  Neutral finishes                         1162.7943\n",
      "  Carpeted floor                           1096.1195\n",
      "  ample natural light                      1011.9004\n",
      "  Neutral paint                            772.2825\n",
      "  recessed lighting                        748.6890\n",
      "  Good natural light                       738.2337\n",
      "  natural light from window                720.1366\n",
      "  large windows / natural light            622.3894\n",
      "  large window with natural light          618.3185\n",
      "  chandelier lighting                      551.0791\n",
      "\n",
      "Topic 7:\n",
      "  attached 2-car garage                    857.1957\n",
      "  concrete driveway                        762.1680\n",
      "  covered front porch                      477.6450\n",
      "  attached garage                          263.3164\n",
      "  two-story single-family home             260.6737\n",
      "  raised wood deck                         204.1014\n",
      "  tile or vinyl flooring                   183.8759\n",
      "  neutral painted walls                    169.5980\n",
      "  gravel driveway                          159.5129\n",
      "  vinyl siding exterior                    152.8538\n",
      "  two-story detached home                  145.2945\n",
      "  asphalt shingle roof                     141.4109\n",
      "  vinyl siding in good condition           135.3285\n",
      "  covered front porch/entry                132.5303\n",
      "  vinyl siding                             122.8601\n",
      "\n",
      "Topic 8:\n",
      "  Neutral paint                            1392.4312\n",
      "  driveway                                 890.3822\n",
      "  Carpeted floor                           836.8804\n",
      "  Ceiling fan                              826.2828\n",
      "  large window                             762.8964\n",
      "  tub-shower combo                         721.3263\n",
      "  fireplace                                714.2478\n",
      "  natural light                            686.8113\n",
      "  wood cabinetry                           602.1999\n",
      "  Neutral finishes                         575.6121\n",
      "  neutral walls                            537.8635\n",
      "  large mirror                             467.7220\n",
      "  tile floor                               463.9603\n",
      "  single window                            458.5638\n",
      "  vaulted ceiling                          414.0302\n",
      "\n",
      "Topic 9:\n",
      "  Hardwood flooring                        1191.5123\n",
      "  Ceiling fan                              1007.6552\n",
      "  chandelier                               927.4365\n",
      "  Carpeted floor                           925.5886\n",
      "  ample natural light                      894.3840\n",
      "  tile floor                               850.6991\n",
      "  large window                             846.0717\n",
      "  mature landscaping                       788.5277\n",
      "  double vanities                          786.8763\n",
      "  formal dining area                       727.5878\n",
      "  granite counters                         649.6795\n",
      "  recessed lighting                        570.5548\n",
      "  Neutral finishes                         566.6427\n",
      "  natural light                            531.3658\n",
      "  fireplace                                517.0796\n",
      "\n",
      "Topic 10:\n",
      "  Carpeted floor                           1343.4372\n",
      "  Neutral paint                            1174.0447\n",
      "  Ceiling fan                              1029.7216\n",
      "  Neutral finishes                         793.2571\n",
      "  laminate counters                        762.9254\n",
      "  carpet flooring                          716.4974\n",
      "  Good natural light                       665.8618\n",
      "  neutral wall color                       656.8389\n",
      "  Hardwood flooring                        647.0428\n",
      "  natural light from window                577.7997\n",
      "  window providing natural light           469.6592\n",
      "  single sink vanity                       456.2186\n",
      "  compact layout                           437.2735\n",
      "  tile floor                               436.4060\n",
      "  vinyl/tile floor                         434.3424\n",
      "\n",
      "============================================================\n",
      "TOPIC SUMMARY\n",
      "============================================================\n",
      " topic                                                                                                 top_5_features  num_properties  avg_topic_strength\n",
      "     1                        standard bedroom size, Neutral finishes, Carpeted floor, carpet flooring, Neutral paint            1602            0.058132\n",
      "     2             neutral decor, Hardwood flooring, stainless steel appliances, Neutral finishes, Good natural light             636            0.087467\n",
      "     3 brick exterior, basic fixtures, mature trees / landscaping, mature trees providing shade, moderate curb appeal             336            0.057094\n",
      "     4                     mature tree, Hardwood flooring, Ceiling fan, natural light from window, Good natural light             975            0.105558\n",
      "     5    formal dining area, chandelier lighting, window natural light, natural light from window, Hardwood flooring             162            0.054793\n",
      "     6                       Hardwood flooring, stainless steel appliances, granite counters, Ceiling fan, tile floor            2193            0.169092\n",
      "     7   attached 2-car garage, concrete driveway, covered front porch, attached garage, two-story single-family home             733            0.074844\n",
      "     8                                             Neutral paint, driveway, Carpeted floor, Ceiling fan, large window            1183            0.113653\n",
      "     9                                Hardwood flooring, Ceiling fan, chandelier, Carpeted floor, ample natural light            1661            0.130417\n",
      "    10                                Carpeted floor, Neutral paint, Ceiling fan, Neutral finishes, laminate counters            1877            0.148950\n",
      "\n",
      "Creating single feature columns...\n",
      "Counting pairs...\n",
      "Counting triples...\n",
      "Adding conditions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenny.lin/House_IQ_Setup/.venv1/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/jenny.lin/House_IQ_Setup/.venv1/lib/python3.13/site-packages/numpy/_core/_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Done: (11358, 1538)\n",
      "✓ Added 10 topic columns: topic_1 to topic_10\n",
      "✓ Added dominant_topic column\n",
      "✓ Added 300 single features\n",
      "✓ Added 20 pair features\n",
      "✓ Added 20 triple features\n",
      "✓ Added 6 condition features\n",
      "✓ No fragmentation warnings!\n",
      "✓ No duplicate columns!\n",
      "\n",
      "Properties in Topic 1:\n",
      "     topic_1   topic_2   topic_3\n",
      "3   0.100000  0.100000  0.100000\n",
      "8   0.100000  0.100000  0.100000\n",
      "10  0.959082  0.004547  0.004546\n",
      "12  0.100000  0.100000  0.100000\n",
      "41  0.100000  0.100000  0.100000\n",
      "\n",
      "Top properties for Topic 3:\n",
      "       dominant_topic   topic_3\n",
      "10398               3  0.819978\n",
      "1278                3  0.775000\n",
      "86                  3  0.774999\n",
      "4338                3  0.774999\n",
      "5844                3  0.774997\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T07:07:19.936775Z",
     "start_time": "2025-12-19T07:07:19.855412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_features_with_gsdmm(df, n1=300, n2=20, n3=20,\n",
    "                                 dedupe_threshold=3, dedupe_top_n=2000,\n",
    "                                 n_topics=10, gsdmm_features=500,\n",
    "                                 alpha=0.1, beta=0.1, n_iterations=30):\n",
    "    \"\"\"\n",
    "    Extract features with GSDMM topic modeling using gensim - optimized for performance\n",
    "    GSDMM (Movie Group Process) assigns each document to ONE dominant topic\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_topics : int\n",
    "        Number of clusters/topics to discover (e.g., 10 = Modern, Traditional, Luxury, etc.)\n",
    "    gsdmm_features : int\n",
    "        Number of top features to use for GSDMM (recommend 300-1000)\n",
    "    alpha : float\n",
    "        Dirichlet parameter for document-cluster distribution (default 0.1)\n",
    "    beta : float\n",
    "        Dirichlet parameter for cluster-word distribution (default 0.1)\n",
    "    n_iterations : int\n",
    "        Number of Gibbs sampling iterations (default 30)\n",
    "\n",
    "    Requires:\n",
    "    ---------\n",
    "    pip install gsdmm gensim\n",
    "    \"\"\"\n",
    "    from collections import Counter, defaultdict\n",
    "    from itertools import combinations\n",
    "    import ast\n",
    "    import numpy as np\n",
    "    from rapidfuzz import process, distance\n",
    "    import pandas as pd\n",
    "    from gsdmm import MovieGroupProcess\n",
    "    from gensim import corpora\n",
    "    from gensim.models import CoherenceModel\n",
    "\n",
    "    print(\"Parsing...\")\n",
    "    prop_feats = []\n",
    "    for s in df['PARSED'].values:\n",
    "        try:\n",
    "            d = ast.literal_eval(s) if pd.notna(s) else {}\n",
    "            feats = set()\n",
    "            for img in d.values():\n",
    "                feats.update(img.get('prominent_features', []))\n",
    "            prop_feats.append(frozenset(feats))\n",
    "        except:\n",
    "            prop_feats.append(frozenset())\n",
    "\n",
    "    # Count ALL features first\n",
    "    print(\"Counting all features...\")\n",
    "    all_singles = Counter()\n",
    "    for feats in prop_feats:\n",
    "        all_singles.update(feats)\n",
    "\n",
    "    # Dedupe top N features\n",
    "    if dedupe_top_n and dedupe_threshold > 0:\n",
    "        top_for_dedupe = [f for f, _ in all_singles.most_common(dedupe_top_n)]\n",
    "        print(f\"Deduplicating top {len(top_for_dedupe)} features...\")\n",
    "\n",
    "        canonical_map = {}\n",
    "        canonical_list = []\n",
    "\n",
    "        for feat in sorted(top_for_dedupe):\n",
    "            if not canonical_list:\n",
    "                canonical_map[feat] = feat\n",
    "                canonical_list.append(feat)\n",
    "                continue\n",
    "\n",
    "            match = process.extractOne(\n",
    "                feat.lower(),\n",
    "                [c.lower() for c in canonical_list],\n",
    "                scorer=distance.Levenshtein.distance,\n",
    "                score_cutoff=dedupe_threshold\n",
    "            )\n",
    "\n",
    "            if match:\n",
    "                canonical_map[feat] = canonical_list[match[2]]\n",
    "            else:\n",
    "                canonical_map[feat] = feat\n",
    "                canonical_list.append(feat)\n",
    "\n",
    "        for feat in all_singles:\n",
    "            if feat not in canonical_map:\n",
    "                canonical_map[feat] = feat\n",
    "    else:\n",
    "        canonical_map = {f: f for f in all_singles}\n",
    "\n",
    "    # Apply canonicalization\n",
    "    prop_feats = [frozenset(canonical_map[f] for f in feats) for feats in prop_feats]\n",
    "\n",
    "    # Count singles\n",
    "    print(\"Counting singles...\")\n",
    "    singles = Counter()\n",
    "    for feats in prop_feats:\n",
    "        singles.update(feats)\n",
    "    top_singles = [f for f, _ in singles.most_common(n1)]\n",
    "\n",
    "    # ========== GSDMM TOPIC MODELING ==========\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"RUNNING GSDMM TOPIC MODELING (Movie Group Process)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Get top features for GSDMM\n",
    "    gsdmm_top_features = [f for f, _ in singles.most_common(gsdmm_features)]\n",
    "    gsdmm_feat_set = set(gsdmm_top_features)\n",
    "\n",
    "    # Create document representations (list of word strings)\n",
    "    print(f\"Building document representations with {len(gsdmm_top_features)} features...\")\n",
    "    docs = []\n",
    "    for feats in prop_feats:\n",
    "        doc = [feat for feat in feats if feat in gsdmm_feat_set]\n",
    "        docs.append(doc)\n",
    "\n",
    "    # Create gensim dictionary\n",
    "    print(\"Creating gensim dictionary...\")\n",
    "    dictionary = corpora.Dictionary(docs)\n",
    "\n",
    "    # Fit GSDMM (Movie Group Process)\n",
    "    print(f\"Fitting GSDMM with {n_topics} topics...\")\n",
    "    print(f\"Parameters: alpha={alpha}, beta={beta}, iterations={n_iterations}\")\n",
    "\n",
    "    mgp = MovieGroupProcess(K=n_topics, alpha=alpha, beta=beta, n_iters=n_iterations)\n",
    "\n",
    "    # Fit the model\n",
    "    y = mgp.fit(docs, len(dictionary))\n",
    "\n",
    "    print(f\"GSDMM converged. Final number of active clusters: {len(set(y))}\")\n",
    "\n",
    "    # Get topic assignments for each document\n",
    "    doc_topic_assignment = np.array(y)\n",
    "\n",
    "    # Create topic distribution matrix (one-hot encoding for GSDMM)\n",
    "    # GSDMM is a hard clustering method - each doc belongs to ONE topic\n",
    "    doc_topic_dist = np.zeros((len(docs), n_topics))\n",
    "    for i, topic in enumerate(y):\n",
    "        doc_topic_dist[i, topic] = 1.0\n",
    "\n",
    "    # Add topic columns to dataframe (OPTIMIZED - no fragmentation, no duplicates)\n",
    "    print(\"Adding topic distributions to dataframe...\")\n",
    "\n",
    "    # Drop any existing topic columns first to avoid duplicates\n",
    "    existing_topic_cols = [f'topic_{i+1}' for i in range(n_topics)] + ['dominant_topic']\n",
    "    df = df.drop(columns=[col for col in existing_topic_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    topic_cols = {}\n",
    "    # For GSDMM, topic probability is binary (1 or 0)\n",
    "    for topic_idx in range(n_topics):\n",
    "        topic_cols[f'topic_{topic_idx+1}'] = doc_topic_dist[:, topic_idx]\n",
    "\n",
    "    # Dominant topic is the assigned cluster\n",
    "    topic_cols['dominant_topic'] = doc_topic_assignment + 1\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(topic_cols, index=df.index)], axis=1)\n",
    "\n",
    "    # Analyze topics - get top words per cluster\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TOP FEATURES BY TOPIC\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    topic_features = []\n",
    "    n_top_words = 15\n",
    "\n",
    "    for topic_idx in range(n_topics):\n",
    "        # Get top words for this cluster from GSDMM model\n",
    "        top_words = mgp.top_words(topic_idx, n_top_words)\n",
    "\n",
    "        # Get word counts\n",
    "        topic_word_counts = []\n",
    "        total_words_in_topic = sum(mgp.cluster_word_count[topic_idx].values())\n",
    "\n",
    "        for word in top_words:\n",
    "            count = mgp.cluster_word_count[topic_idx][word]\n",
    "            weight = count / total_words_in_topic if total_words_in_topic > 0 else 0\n",
    "            topic_word_counts.append((word, weight))\n",
    "\n",
    "        print(f\"\\nTopic {topic_idx + 1} ({mgp.cluster_doc_count[topic_idx]} properties):\")\n",
    "        for word, weight in topic_word_counts:\n",
    "            print(f\"  {word:40s} {weight:.4f}\")\n",
    "\n",
    "        topic_features.append({\n",
    "            'topic': topic_idx + 1,\n",
    "            'top_features': [w for w, _ in topic_word_counts],\n",
    "            'weights': [w for _, w in topic_word_counts]\n",
    "        })\n",
    "\n",
    "    # Create topic summary dataframe\n",
    "    topic_summary_data = []\n",
    "    for t in topic_features:\n",
    "        topic_num = t['topic']\n",
    "        topic_idx = topic_num - 1\n",
    "        num_props = int((df['dominant_topic'] == topic_num).sum())\n",
    "\n",
    "        topic_summary_data.append({\n",
    "            'topic': topic_num,\n",
    "            'top_5_features': ', '.join(t['top_features'][:5]),\n",
    "            'num_properties': num_props,\n",
    "            'cluster_size': mgp.cluster_doc_count[topic_idx],\n",
    "            'avg_topic_strength': float(df[f\"topic_{topic_num}\"].mean())\n",
    "        })\n",
    "\n",
    "    topic_summary = pd.DataFrame(topic_summary_data)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TOPIC SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(topic_summary.to_string(index=False))\n",
    "\n",
    "    # Calculate coherence score using gensim\n",
    "    try:\n",
    "        print(\"\\nCalculating topic coherence...\")\n",
    "        # Get topic words for coherence calculation\n",
    "        topic_words = [[word for word in mgp.top_words(k, 10)] for k in range(n_topics)]\n",
    "\n",
    "        # Calculate C_V coherence\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=docs,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_v'\n",
    "        )\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "        print(f\"Topic Coherence (C_V): {coherence_score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate coherence: {e}\")\n",
    "        coherence_score = None\n",
    "\n",
    "    # ========== CONTINUE WITH REGULAR FEATURE EXTRACTION ==========\n",
    "\n",
    "    # Create singles columns (OPTIMIZED - no fragmentation, no duplicates)\n",
    "    print(\"\\nCreating single feature columns...\")\n",
    "    feat_idx = {f: i for i, f in enumerate(top_singles)}\n",
    "    singles_data = np.zeros((len(df), len(top_singles)), dtype=np.int8)\n",
    "\n",
    "    for row, feats in enumerate(prop_feats):\n",
    "        for feat in feats:\n",
    "            if feat in feat_idx:\n",
    "                singles_data[row, feat_idx[feat]] = 1\n",
    "\n",
    "    # Build all single columns at once\n",
    "    single_cols = {}\n",
    "    for i, feat in enumerate(top_singles):\n",
    "        col_name = feat.replace(' ', '_')[:50]\n",
    "        # Ensure unique column names\n",
    "        if col_name in df.columns:\n",
    "            col_name = f\"{col_name}_{i}\"\n",
    "        single_cols[col_name] = singles_data[:, i]\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(single_cols, index=df.index)], axis=1)\n",
    "\n",
    "    # Pairs (OPTIMIZED - no fragmentation, no duplicates)\n",
    "    print(\"Counting pairs...\")\n",
    "    top_set = set(top_singles)\n",
    "    filtered_feats = [[f for f in feats if f in top_set] for feats in prop_feats]\n",
    "\n",
    "    pairs = Counter()\n",
    "    for feats in filtered_feats:\n",
    "        if len(feats) >= 2:\n",
    "            pairs.update(combinations(sorted(feats), 2))\n",
    "\n",
    "    top_pairs = [p for p, _ in pairs.most_common(n2)]\n",
    "\n",
    "    # Drop existing pair columns\n",
    "    existing_pair_cols = [f'pair_{i}' for i in range(1, n2+1)]\n",
    "    df = df.drop(columns=[col for col in existing_pair_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    pair_definitions = []\n",
    "    pair_cols = {}\n",
    "\n",
    "    for i, (f1, f2) in enumerate(top_pairs, 1):\n",
    "        i1, i2 = feat_idx[f1], feat_idx[f2]\n",
    "        pair_cols[f\"pair_{i}\"] = (singles_data[:, i1] & singles_data[:, i2]).astype(np.int8)\n",
    "        pair_definitions.append({\n",
    "            'column_name': f'pair_{i}',\n",
    "            'feature_1': f1,\n",
    "            'feature_2': f2,\n",
    "            'count': pairs[(f1, f2)]\n",
    "        })\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(pair_cols, index=df.index)], axis=1)\n",
    "\n",
    "    # Triples (OPTIMIZED - no fragmentation, no duplicates)\n",
    "    print(\"Counting triples...\")\n",
    "    triples = Counter()\n",
    "    for feats in filtered_feats:\n",
    "        if len(feats) >= 3:\n",
    "            triples.update(combinations(sorted(feats), 3))\n",
    "\n",
    "    top_triples = [t for t, _ in triples.most_common(n3)]\n",
    "\n",
    "    # Drop existing triple columns\n",
    "    existing_triple_cols = [f'trip_{i}' for i in range(1, n3+1)]\n",
    "    df = df.drop(columns=[col for col in existing_triple_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    triple_definitions = []\n",
    "    triple_cols = {}\n",
    "\n",
    "    for i, (f1, f2, f3) in enumerate(top_triples, 1):\n",
    "        i1, i2, i3 = feat_idx[f1], feat_idx[f2], feat_idx[f3]\n",
    "        triple_cols[f\"trip_{i}\"] = (singles_data[:, i1] & singles_data[:, i2] & singles_data[:, i3]).astype(np.int8)\n",
    "        triple_definitions.append({\n",
    "            'column_name': f'trip_{i}',\n",
    "            'feature_1': f1,\n",
    "            'feature_2': f2,\n",
    "            'feature_3': f3,\n",
    "            'count': triples[(f1, f2, f3)]\n",
    "        })\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(triple_cols, index=df.index)], axis=1)\n",
    "\n",
    "    # Conditions (OPTIMIZED - no fragmentation, no duplicates)\n",
    "    print(\"Adding conditions...\")\n",
    "    parsed = [ast.literal_eval(s) if pd.notna(s) else {} for s in df['PARSED'].values]\n",
    "\n",
    "    # Drop existing condition columns\n",
    "    condition_col_names = []\n",
    "    for pre in ['gran', 'high']:\n",
    "        for suf in ['_in', '_ex', '']:\n",
    "            condition_col_names.append(f'{pre}_c{suf}')\n",
    "    df = df.drop(columns=[col for col in condition_col_names if col in df.columns], errors='ignore')\n",
    "\n",
    "    condition_cols = {}\n",
    "    for pre, key in [('gran', 'granular_condition_num'), ('high', 'high_condition_num')]:\n",
    "        for suf, typ in [('_in', 'Indoor'), ('_ex', 'Exterior'), ('', None)]:\n",
    "            col_name = f'{pre}_c{suf}'\n",
    "            condition_cols[col_name] = [\n",
    "                np.mean([img[key] for img in d.values()\n",
    "                        if key in img and (not typ or img.get('image_type') == typ)])\n",
    "                if d else np.nan\n",
    "                for d in parsed\n",
    "            ]\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(condition_cols, index=df.index)], axis=1)\n",
    "\n",
    "    print(f\"\\n✓ Done: {df.shape}\")\n",
    "    print(f\"✓ Added {n_topics} topic columns: topic_1 to topic_{n_topics}\")\n",
    "    print(f\"✓ Added dominant_topic column (GSDMM hard clustering)\")\n",
    "    print(f\"✓ Added {len(single_cols)} single features\")\n",
    "    print(f\"✓ Added {len(pair_cols)} pair features\")\n",
    "    print(f\"✓ Added {len(triple_cols)} triple features\")\n",
    "    print(f\"✓ Added {len(condition_cols)} condition features\")\n",
    "    if coherence_score:\n",
    "        print(f\"✓ Topic coherence (C_V): {coherence_score:.4f}\")\n",
    "    print(f\"✓ No fragmentation warnings!\")\n",
    "    print(f\"✓ No duplicate columns!\")\n",
    "\n",
    "    pairs_df = pd.DataFrame(pair_definitions)\n",
    "    triples_df = pd.DataFrame(triple_definitions)\n",
    "\n",
    "    return df, pairs_df, triples_df, topic_summary, mgp, coherence_score\n",
    "\n",
    "\n",
    "# ========== USAGE ==========\n",
    "\n",
    "# Run with GSDMM (10 topics, using top 500 features)\n",
    "df, pairs, triples, topics, gsdmm_model, coherence = extract_features_with_gsdmm(\n",
    "    df,\n",
    "    n1=300,           # top single features\n",
    "    n2=20,            # top pairs\n",
    "    n3=20,            # top triples\n",
    "    dedupe_threshold=3,\n",
    "    dedupe_top_n=2000,\n",
    "    n_topics=10,      # number of clusters\n",
    "    gsdmm_features=500,  # features to use for GSDMM\n",
    "    alpha=0.1,        # Dirichlet prior for doc-topic\n",
    "    beta=0.1,         # Dirichlet prior for topic-word\n",
    "    n_iterations=30   # Gibbs sampling iterations\n",
    ")\n",
    "\n",
    "# Save results\n",
    "pairs.to_csv('pair_definitions.csv', index=False)\n",
    "triples.to_csv('triple_definitions.csv', index=False)\n",
    "topics.to_csv('topic_summary_gsdmm.csv', index=False)\n",
    "\n",
    "# Explore topics (GSDMM assigns each property to ONE topic)\n",
    "print(\"\\nProperties in Topic 1:\")\n",
    "print(df[df['dominant_topic'] == 1][['dominant_topic']].head())\n",
    "\n",
    "# Count properties per topic\n",
    "print(\"\\nProperties per topic:\")\n",
    "print(df['dominant_topic'].value_counts().sort_index())\n",
    "\n",
    "# See top features for a specific cluster\n",
    "print(\"\\nTop features for Topic 3:\")\n",
    "print(gsdmm_model.top_words(2, 10))  # Note: 0-indexed, so topic 3 is index"
   ],
   "id": "e433eec73c07a119",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gsdmm'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 354\u001B[39m\n\u001B[32m    348\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m df, pairs_df, triples_df, topic_summary, mgp, coherence_score\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# ========== USAGE ==========\u001B[39;00m\n\u001B[32m    352\u001B[39m \n\u001B[32m    353\u001B[39m \u001B[38;5;66;03m# Run with GSDMM (10 topics, using top 500 features)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m df, pairs, triples, topics, gsdmm_model, coherence = \u001B[43mextract_features_with_gsdmm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn1\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m300\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m           \u001B[49m\u001B[38;5;66;43;03m# top single features\u001B[39;49;00m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn2\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# top pairs\u001B[39;49;00m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn3\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# top triples\u001B[39;49;00m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdedupe_threshold\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdedupe_top_n\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn_topics\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m      \u001B[49m\u001B[38;5;66;43;03m# number of clusters\u001B[39;49;00m\n\u001B[32m    362\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgsdmm_features\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m500\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# features to use for GSDMM\u001B[39;49;00m\n\u001B[32m    363\u001B[39m \u001B[43m    \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Dirichlet prior for doc-topic\u001B[39;49;00m\n\u001B[32m    364\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbeta\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m         \u001B[49m\u001B[38;5;66;43;03m# Dirichlet prior for topic-word\u001B[39;49;00m\n\u001B[32m    365\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn_iterations\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m30\u001B[39;49m\u001B[43m   \u001B[49m\u001B[38;5;66;43;03m# Gibbs sampling iterations\u001B[39;49;00m\n\u001B[32m    366\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m    368\u001B[39m \u001B[38;5;66;03m# Save results\u001B[39;00m\n\u001B[32m    369\u001B[39m pairs.to_csv(\u001B[33m'\u001B[39m\u001B[33mpair_definitions.csv\u001B[39m\u001B[33m'\u001B[39m, index=\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 32\u001B[39m, in \u001B[36mextract_features_with_gsdmm\u001B[39m\u001B[34m(df, n1, n2, n3, dedupe_threshold, dedupe_top_n, n_topics, gsdmm_features, alpha, beta, n_iterations)\u001B[39m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mrapidfuzz\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m process, distance\n\u001B[32m     31\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpd\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mgsdmm\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m MovieGroupProcess\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mgensim\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m corpora\n\u001B[32m     34\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mgensim\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodels\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CoherenceModel\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'gsdmm'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T07:07:51.461595Z",
     "start_time": "2025-12-19T07:07:50.746128Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install gsdmm",
   "id": "b1850230b47dace8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: Could not find a version that satisfies the requirement gsdmm (from versions: none)\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "\u001B[31mERROR: No matching distribution found for gsdmm\u001B[0m\u001B[31m\r\n",
      "\u001B[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T07:20:34.904711Z",
     "start_time": "2025-12-19T07:20:06.400349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_features_with_gsdmm(df, n1=300, n2=20, n3=20,\n",
    "                                 dedupe_threshold=3, dedupe_top_n=2000,\n",
    "                                 n_topics=10, gsdmm_features=500,\n",
    "                                 alpha=0.1, beta=0.1, n_iterations=30):\n",
    "    \"\"\"\n",
    "    Extract features with GSDMM topic modeling - pure implementation\n",
    "    GSDMM (Movie Group Process) assigns each document to ONE dominant topic\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_topics : int\n",
    "        Number of clusters/topics to discover (e.g., 10 = Modern, Traditional, Luxury, etc.)\n",
    "    gsdmm_features : int\n",
    "        Number of top features to use for GSDMM (recommend 300-1000)\n",
    "    alpha : float\n",
    "        Dirichlet parameter for document-cluster distribution (default 0.1)\n",
    "    beta : float\n",
    "        Dirichlet parameter for cluster-word distribution (default 0.1)\n",
    "    n_iterations : int\n",
    "        Number of Gibbs sampling iterations (default 30)\n",
    "\n",
    "    No special dependencies - pure numpy/python implementation\n",
    "    \"\"\"\n",
    "    from collections import Counter, defaultdict\n",
    "    from itertools import combinations\n",
    "    import ast\n",
    "    import numpy as np\n",
    "    from rapidfuzz import process, distance\n",
    "    import pandas as pd\n",
    "\n",
    "    # ========== GSDMM Implementation (Movie Group Process) ==========\n",
    "    class MovieGroupProcess:\n",
    "        \"\"\"\n",
    "        GSDMM - Gibbs Sampling Dirichlet Multinomial Mixture\n",
    "        Pure numpy/python implementation - no external dependencies\n",
    "        \"\"\"\n",
    "        def __init__(self, K=10, alpha=0.1, beta=0.1, n_iters=30):\n",
    "            self.K = K  # number of clusters\n",
    "            self.alpha = alpha  # doc-cluster prior\n",
    "            self.beta = beta  # word-cluster prior\n",
    "            self.n_iters = n_iters\n",
    "\n",
    "            self.doc_cluster = []  # cluster assignment for each doc\n",
    "            self.cluster_doc_count = np.zeros(K)  # number of docs in each cluster\n",
    "            self.cluster_word_count = [defaultdict(int) for _ in range(K)]  # word counts per cluster\n",
    "            self.cluster_word_total = np.zeros(K)  # total words per cluster\n",
    "\n",
    "        def fit(self, docs, vocab_size):\n",
    "            \"\"\"\n",
    "            docs: list of lists of words (strings)\n",
    "            vocab_size: number of unique words in vocabulary\n",
    "            \"\"\"\n",
    "            n_docs = len(docs)\n",
    "\n",
    "            # Random initialization\n",
    "            print(\"  Initializing clusters randomly...\")\n",
    "            self.doc_cluster = np.random.randint(0, self.K, n_docs)\n",
    "\n",
    "            # Populate initial counts\n",
    "            for doc_id, doc in enumerate(docs):\n",
    "                cluster = self.doc_cluster[doc_id]\n",
    "                self.cluster_doc_count[cluster] += 1\n",
    "                for word in doc:\n",
    "                    self.cluster_word_count[cluster][word] += 1\n",
    "                    self.cluster_word_total[cluster] += 1\n",
    "\n",
    "            # Gibbs sampling\n",
    "            print(f\"  Running {self.n_iters} Gibbs sampling iterations...\")\n",
    "            for iteration in range(self.n_iters):\n",
    "                if iteration % 5 == 0:\n",
    "                    active_clusters = len([c for c in range(self.K) if self.cluster_doc_count[c] > 0])\n",
    "                    print(f\"    Iteration {iteration}/{self.n_iters} - Active clusters: {active_clusters}\")\n",
    "\n",
    "                for doc_id, doc in enumerate(docs):\n",
    "                    if len(doc) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Remove doc from current cluster\n",
    "                    old_cluster = self.doc_cluster[doc_id]\n",
    "                    self.cluster_doc_count[old_cluster] -= 1\n",
    "                    for word in doc:\n",
    "                        self.cluster_word_count[old_cluster][word] -= 1\n",
    "                        self.cluster_word_total[old_cluster] -= 1\n",
    "\n",
    "                    # Calculate probability for each cluster\n",
    "                    probs = np.zeros(self.K)\n",
    "                    for k in range(self.K):\n",
    "                        # P(z|d) ∝ P(z) * P(d|z)\n",
    "                        # P(z) = (n_z + alpha) / (D - 1 + K*alpha)\n",
    "                        p_z = (self.cluster_doc_count[k] + self.alpha) / (n_docs - 1 + self.K * self.alpha)\n",
    "\n",
    "                        # P(d|z) = product of P(w|z) for all words in d\n",
    "                        # P(w|z) = (n_z_w + beta) / (n_z + V*beta)\n",
    "                        p_d_given_z = 1.0\n",
    "                        for word in doc:\n",
    "                            n_z_w = self.cluster_word_count[k][word]\n",
    "                            n_z = self.cluster_word_total[k]\n",
    "                            p_w_given_z = (n_z_w + self.beta) / (n_z + vocab_size * self.beta)\n",
    "                            p_d_given_z *= p_w_given_z\n",
    "\n",
    "                        probs[k] = p_z * p_d_given_z\n",
    "\n",
    "                    # Normalize and sample new cluster\n",
    "                    if probs.sum() == 0:\n",
    "                        probs = np.ones(self.K) / self.K\n",
    "                    else:\n",
    "                        probs /= probs.sum()\n",
    "\n",
    "                    new_cluster = np.random.choice(self.K, p=probs)\n",
    "\n",
    "                    # Add doc to new cluster\n",
    "                    self.doc_cluster[doc_id] = new_cluster\n",
    "                    self.cluster_doc_count[new_cluster] += 1\n",
    "                    for word in doc:\n",
    "                        self.cluster_word_count[new_cluster][word] += 1\n",
    "                        self.cluster_word_total[new_cluster] += 1\n",
    "\n",
    "            # Final summary\n",
    "            active_clusters = len([c for c in range(self.K) if self.cluster_doc_count[c] > 0])\n",
    "            print(f\"  ✓ Converged. Active clusters: {active_clusters}/{self.K}\")\n",
    "\n",
    "            return self.doc_cluster\n",
    "\n",
    "        def top_words(self, cluster_id, n_words=10):\n",
    "            \"\"\"Get top n words for a cluster\"\"\"\n",
    "            word_counts = self.cluster_word_count[cluster_id]\n",
    "            sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            return [word for word, count in sorted_words[:n_words]]\n",
    "\n",
    "        def choose_best_label(self, cluster_id):\n",
    "            \"\"\"Choose best descriptive label for a cluster\"\"\"\n",
    "            if self.cluster_doc_count[cluster_id] == 0:\n",
    "                return \"Empty Cluster\"\n",
    "\n",
    "            top_words = self.top_words(cluster_id, 3)\n",
    "            return \" + \".join(top_words) if top_words else \"Empty\"\n",
    "\n",
    "    print(\"Parsing...\")\n",
    "    prop_feats = []\n",
    "    for s in df['PARSED'].values:\n",
    "        try:\n",
    "            d = ast.literal_eval(s) if pd.notna(s) else {}\n",
    "            feats = set()\n",
    "            for img in d.values():\n",
    "                feats.update(img.get('prominent_features', []))\n",
    "            prop_feats.append(frozenset(feats))\n",
    "        except:\n",
    "            prop_feats.append(frozenset())\n",
    "\n",
    "    # Count ALL features first\n",
    "    print(\"Counting all features...\")\n",
    "    all_singles = Counter()\n",
    "    for feats in prop_feats:\n",
    "        all_singles.update(feats)\n",
    "\n",
    "    # Dedupe top N features\n",
    "    if dedupe_top_n and dedupe_threshold > 0:\n",
    "        top_for_dedupe = [f for f, _ in all_singles.most_common(dedupe_top_n)]\n",
    "        print(f\"Deduplicating top {len(top_for_dedupe)} features...\")\n",
    "\n",
    "        canonical_map = {}\n",
    "        canonical_list = []\n",
    "\n",
    "        for feat in sorted(top_for_dedupe):\n",
    "            if not canonical_list:\n",
    "                canonical_map[feat] = feat\n",
    "                canonical_list.append(feat)\n",
    "                continue\n",
    "\n",
    "            match = process.extractOne(\n",
    "                feat.lower(),\n",
    "                [c.lower() for c in canonical_list],\n",
    "                scorer=distance.Levenshtein.distance,\n",
    "                score_cutoff=dedupe_threshold\n",
    "            )\n",
    "\n",
    "            if match:\n",
    "                canonical_map[feat] = canonical_list[match[2]]\n",
    "            else:\n",
    "                canonical_map[feat] = feat\n",
    "                canonical_list.append(feat)\n",
    "\n",
    "        for feat in all_singles:\n",
    "            if feat not in canonical_map:\n",
    "                canonical_map[feat] = feat\n",
    "    else:\n",
    "        canonical_map = {f: f for f in all_singles}\n",
    "\n",
    "    # Apply canonicalization\n",
    "    prop_feats = [frozenset(canonical_map[f] for f in feats) for feats in prop_feats]\n",
    "\n",
    "    # Count singles\n",
    "    print(\"Counting singles...\")\n",
    "    singles = Counter()\n",
    "    for feats in prop_feats:\n",
    "        singles.update(feats)\n",
    "    top_singles = [f for f, _ in singles.most_common(n1)]\n",
    "\n",
    "    # ========== GSDMM TOPIC MODELING ==========\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"RUNNING GSDMM TOPIC MODELING (Movie Group Process)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Get top features for GSDMM\n",
    "    gsdmm_top_features = [f for f, _ in singles.most_common(gsdmm_features)]\n",
    "    gsdmm_feat_set = set(gsdmm_top_features)\n",
    "    vocab_size = len(gsdmm_top_features)\n",
    "\n",
    "    # Create document representations (list of word strings)\n",
    "    print(f\"Building document representations with {vocab_size} features...\")\n",
    "    docs = []\n",
    "    for feats in prop_feats:\n",
    "        doc = [feat for feat in feats if feat in gsdmm_feat_set]\n",
    "        docs.append(doc)\n",
    "\n",
    "    # Fit GSDMM (Movie Group Process)\n",
    "    print(f\"Fitting GSDMM with {n_topics} topics...\")\n",
    "    print(f\"Parameters: alpha={alpha}, beta={beta}, iterations={n_iterations}\")\n",
    "\n",
    "    mgp = MovieGroupProcess(K=n_topics, alpha=alpha, beta=beta, n_iters=n_iterations)\n",
    "    doc_topic_assignment = mgp.fit(docs, vocab_size)\n",
    "\n",
    "    # Create topic distribution matrix (one-hot encoding for GSDMM)\n",
    "    # GSDMM is a hard clustering method - each doc belongs to ONE topic\n",
    "    doc_topic_dist = np.zeros((len(docs), n_topics))\n",
    "    for i, topic in enumerate(doc_topic_assignment):\n",
    "        doc_topic_dist[i, topic] = 1.0\n",
    "\n",
    "    # Add topic columns to dataframe (OPTIMIZED - no fragmentation, no duplicates)\n",
    "    print(\"\\nAdding topic distributions to dataframe...\")\n",
    "\n",
    "    # Drop any existing topic columns first to avoid duplicates\n",
    "    existing_topic_cols = [f'topic_{i+1}' for i in range(n_topics)] + ['dominant_topic']\n",
    "    df = df.drop(columns=[col for col in existing_topic_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    topic_cols = {}\n",
    "    # For GSDMM, topic probability is binary (1 or 0)\n",
    "    for topic_idx in range(n_topics):\n",
    "        topic_cols[f'topic_{topic_idx+1}'] = doc_topic_dist[:, topic_idx]\n",
    "\n",
    "    # Dominant topic is the assigned cluster\n",
    "    topic_cols['dominant_topic'] = doc_topic_assignment + 1\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(topic_cols, index=df.index)], axis=1)\n",
    "\n",
    "    # Analyze topics - get top words per cluster\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TOP FEATURES BY TOPIC\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    topic_features = []\n",
    "    n_top_words = 15\n",
    "\n",
    "    for topic_idx in range(n_topics):\n",
    "        # Get top words for this cluster\n",
    "        top_words = mgp.top_words(topic_idx, n_top_words)\n",
    "\n",
    "        # Get word counts and weights\n",
    "        topic_word_data = []\n",
    "        total_words_in_topic = mgp.cluster_word_total[topic_idx]\n",
    "\n",
    "        for word in top_words:\n",
    "            count = mgp.cluster_word_count[topic_idx][word]\n",
    "            weight = count / total_words_in_topic if total_words_in_topic > 0 else 0\n",
    "            topic_word_data.append((word, weight))\n",
    "\n",
    "        num_docs = int(mgp.cluster_doc_count[topic_idx])\n",
    "        print(f\"\\nTopic {topic_idx + 1} ({num_docs} properties):\")\n",
    "        for word, weight in topic_word_data:\n",
    "            print(f\"  {word:40s} {weight:.4f}\")\n",
    "\n",
    "        topic_features.append({\n",
    "            'topic': topic_idx + 1,\n",
    "            'top_features': [w for w, _ in topic_word_data],\n",
    "            'weights': [w for _, w in topic_word_data]\n",
    "        })\n",
    "\n",
    "    # Create topic summary dataframe\n",
    "    topic_summary_data = []\n",
    "    for t in topic_features:\n",
    "        topic_num = t['topic']\n",
    "        topic_idx = topic_num - 1\n",
    "        num_props = int((df['dominant_topic'] == topic_num).sum())\n",
    "\n",
    "        topic_summary_data.append({\n",
    "            'topic': topic_num,\n",
    "            'top_5_features': ', '.join(t['top_features'][:5]),\n",
    "            'num_properties': num_props,\n",
    "            'cluster_doc_count': int(mgp.cluster_doc_count[topic_idx]),\n",
    "            'avg_topic_strength': float(df[f\"topic_{topic_num}\"].mean()),\n",
    "            'cluster_label': mgp.choose_best_label(topic_idx)\n",
    "        })\n",
    "\n",
    "    topic_summary = pd.DataFrame(topic_summary_data)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TOPIC SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(topic_summary.to_string(index=False))\n",
    "\n",
    "    # ========== CONTINUE WITH REGULAR FEATURE EXTRACTION ==========\n",
    "\n",
    "    # Create singles columns (OPTIMIZED - no fragmentation, no duplicates)\n",
    "    print(\"\\nCreating single feature columns...\")\n",
    "    feat_idx = {f: i for i, f in enumerate(top_singles)}\n",
    "    singles_data = np.zeros((len(df), len(top_singles)), dtype=np.int8)\n",
    "\n",
    "    for row, feats in enumerate(prop_feats):\n",
    "        for feat in feats:\n",
    "            if feat in feat_idx:\n",
    "                singles_data[row, feat_idx[feat]] = 1\n",
    "\n",
    "    # Build all single columns at once\n",
    "    single_cols = {}\n",
    "    for i, feat in enumerate(top_singles):\n",
    "        col_name = feat.replace(' ', '_')[:50]\n",
    "        # Ensure unique column names\n",
    "        if col_name in df.columns:\n",
    "            col_name = f\"{col_name}_{i}\"\n",
    "        single_cols[col_name] = singles_data[:, i]\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(single_cols, index=df.index)], axis=1)\n",
    "\n",
    "    # Pairs (OPTIMIZED - no fragmentation, no duplicates)\n",
    "    print(\"Counting pairs...\")\n",
    "    top_set = set(top_singles)\n",
    "    filtered_feats = [[f for f in feats if f in top_set] for feats in prop_feats]\n",
    "\n",
    "    pairs = Counter()\n",
    "    for feats in filtered_feats:\n",
    "        if len(feats) >= 2:\n",
    "            pairs.update(combinations(sorted(feats), 2))\n",
    "\n",
    "    top_pairs = [p for p, _ in pairs.most_common(n2)]\n",
    "\n",
    "    # Drop existing pair columns\n",
    "    existing_pair_cols = [f'pair_{i}' for i in range(1, n2+1)]\n",
    "    df = df.drop(columns=[col for col in existing_pair_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    pair_definitions = []\n",
    "    pair_cols = {}\n",
    "\n",
    "    for i, (f1, f2) in enumerate(top_pairs, 1):\n",
    "        i1, i2 = feat_idx[f1], feat_idx[f2]\n",
    "        pair_cols[f\"pair_{i}\"] = (singles_data[:, i1] & singles_data[:, i2]).astype(np.int8)\n",
    "        pair_definitions.append({\n",
    "            'column_name': f'pair_{i}',\n",
    "            'feature_1': f1,\n",
    "            'feature_2': f2,\n",
    "            'count': pairs[(f1, f2)]\n",
    "        })\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(pair_cols, index=df.index)], axis=1)\n",
    "\n",
    "    # Triples (OPTIMIZED - no fragmentation, no duplicates)\n",
    "    print(\"Counting triples...\")\n",
    "    triples = Counter()\n",
    "    for feats in filtered_feats:\n",
    "        if len(feats) >= 3:\n",
    "            triples.update(combinations(sorted(feats), 3))\n",
    "\n",
    "    top_triples = [t for t, _ in triples.most_common(n3)]\n",
    "\n",
    "    # Drop existing triple columns\n",
    "    existing_triple_cols = [f'trip_{i}' for i in range(1, n3+1)]\n",
    "    df = df.drop(columns=[col for col in existing_triple_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    triple_definitions = []\n",
    "    triple_cols = {}\n",
    "\n",
    "    for i, (f1, f2, f3) in enumerate(top_triples, 1):\n",
    "        i1, i2, i3 = feat_idx[f1], feat_idx[f2], feat_idx[f3]\n",
    "        triple_cols[f\"trip_{i}\"] = (singles_data[:, i1] & singles_data[:, i2] & singles_data[:, i3]).astype(np.int8)\n",
    "        triple_definitions.append({\n",
    "            'column_name': f'trip_{i}',\n",
    "            'feature_1': f1,\n",
    "            'feature_2': f2,\n",
    "            'feature_3': f3,\n",
    "            'count': triples[(f1, f2, f3)]\n",
    "        })\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(triple_cols, index=df.index)], axis=1)\n",
    "\n",
    "    # Conditions (OPTIMIZED - no fragmentation, no duplicates)\n",
    "    print(\"Adding conditions...\")\n",
    "    parsed = [ast.literal_eval(s) if pd.notna(s) else {} for s in df['PARSED'].values]\n",
    "\n",
    "    # Drop existing condition columns\n",
    "    condition_col_names = []\n",
    "    for pre in ['gran', 'high']:\n",
    "        for suf in ['_in', '_ex', '']:\n",
    "            condition_col_names.append(f'{pre}_c{suf}')\n",
    "    df = df.drop(columns=[col for col in condition_col_names if col in df.columns], errors='ignore')\n",
    "\n",
    "    condition_cols = {}\n",
    "    for pre, key in [('gran', 'granular_condition_num'), ('high', 'high_condition_num')]:\n",
    "        for suf, typ in [('_in', 'Indoor'), ('_ex', 'Exterior'), ('', None)]:\n",
    "            col_name = f'{pre}_c{suf}'\n",
    "            condition_cols[col_name] = [\n",
    "                np.mean([img[key] for img in d.values()\n",
    "                        if key in img and (not typ or img.get('image_type') == typ)])\n",
    "                if d else np.nan\n",
    "                for d in parsed\n",
    "            ]\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(condition_cols, index=df.index)], axis=1)\n",
    "\n",
    "    print(f\"\\n✓ Done: {df.shape}\")\n",
    "    print(f\"✓ Added {n_topics} topic columns: topic_1 to topic_{n_topics}\")\n",
    "    print(f\"✓ Added dominant_topic column (GSDMM hard clustering - each property assigned to ONE topic)\")\n",
    "    print(f\"✓ Added {len(single_cols)} single features\")\n",
    "    print(f\"✓ Added {len(pair_cols)} pair features\")\n",
    "    print(f\"✓ Added {len(triple_cols)} triple features\")\n",
    "    print(f\"✓ Added {len(condition_cols)} condition features\")\n",
    "    print(f\"✓ No fragmentation warnings!\")\n",
    "    print(f\"✓ No duplicate columns!\")\n",
    "\n",
    "    pairs_df = pd.DataFrame(pair_definitions)\n",
    "    triples_df = pd.DataFrame(triple_definitions)\n",
    "\n",
    "    return df, pairs_df, triples_df, topic_summary, mgp\n",
    "\n",
    "\n",
    "# ========== USAGE ==========\n",
    "\n",
    "# Run with GSDMM (10 topics, using top 500 features)\n",
    "df, pairs, triples, topics, gsdmm_model = extract_features_with_gsdmm(\n",
    "    df,\n",
    "    n1=300,           # top single features\n",
    "    n2=20,            # top pairs\n",
    "    n3=20,            # top triples\n",
    "    dedupe_threshold=3,\n",
    "    dedupe_top_n=2000,\n",
    "    n_topics=10,      # number of clusters\n",
    "    gsdmm_features=500,  # features to use for GSDMM\n",
    "    alpha=0.1,        # Dirichlet prior for doc-topic (lower = fewer topics)\n",
    "    beta=0.1,         # Dirichlet prior for topic-word (lower = focused topics)\n",
    "    n_iterations=30   # Gibbs sampling iterations (more = better convergence)\n",
    ")\n",
    "\n",
    "# Save results\n",
    "pairs.to_csv('pair_definitions.csv', index=False)\n",
    "triples.to_csv('triple_definitions.csv', index=False)\n",
    "topics.to_csv('topic_summary_gsdmm.csv', index=False)\n",
    "\n",
    "# Explore topics (GSDMM assigns each property to ONE topic)\n",
    "print(\"\\nProperties in Topic 1:\")\n",
    "print(df[df['dominant_topic'] == 1][['dominant_topic']].head(10))\n",
    "\n",
    "# Count properties per topic\n",
    "print(\"\\nProperties per topic:\")\n",
    "topic_counts = df['dominant_topic'].value_counts().sort_index()\n",
    "for topic_num, count in topic_counts.items():\n",
    "    label = gsdmm_model.choose_best_label(int(topic_num) - 1)\n",
    "    print(f\"  Topic {topic_num}: {count} properties - [{label}]\")\n",
    "\n",
    "# See top features for ALL clusters\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 10 FEATURES FOR EACH TOPIC\")\n",
    "print(\"=\"*60)\n",
    "for topic_idx in range(n_topics):\n",
    "    topic_num = topic_idx + 1\n",
    "    num_props = int((df['dominant_topic'] == topic_num).sum())\n",
    "    label = gsdmm_model.choose_best_label(topic_idx)\n",
    "\n",
    "    print(f\"\\nTopic {topic_num} - {num_props} properties - [{label}]:\")\n",
    "    for i, word in enumerate(gsdmm_model.top_words(topic_idx, 10), 1):\n",
    "        count = gsdmm_model.cluster_word_count[topic_idx][word]\n",
    "        print(f\"  {i:2d}. {word:40s} (count: {count})\")"
   ],
   "id": "3d87e233b799ed1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing...\n",
      "Counting all features...\n",
      "Deduplicating top 2000 features...\n",
      "Counting singles...\n",
      "\n",
      "============================================================\n",
      "RUNNING GSDMM TOPIC MODELING (Movie Group Process)\n",
      "============================================================\n",
      "Building document representations with 500 features...\n",
      "Fitting GSDMM with 10 topics...\n",
      "Parameters: alpha=0.1, beta=0.1, iterations=30\n",
      "  Initializing clusters randomly...\n",
      "  Running 30 Gibbs sampling iterations...\n",
      "    Iteration 0/30 - Active clusters: 10\n",
      "    Iteration 5/30 - Active clusters: 10\n",
      "    Iteration 10/30 - Active clusters: 10\n",
      "    Iteration 15/30 - Active clusters: 10\n",
      "    Iteration 20/30 - Active clusters: 10\n",
      "    Iteration 25/30 - Active clusters: 10\n",
      "  ✓ Converged. Active clusters: 10/10\n",
      "\n",
      "Adding topic distributions to dataframe...\n",
      "\n",
      "============================================================\n",
      "TOP FEATURES BY TOPIC\n",
      "============================================================\n",
      "\n",
      "Topic 1 (836 properties):\n",
      "  Hardwood flooring                        0.0326\n",
      "  Ceiling fan                              0.0239\n",
      "  Neutral paint                            0.0225\n",
      "  stainless steel appliances               0.0222\n",
      "  Neutral finishes                         0.0199\n",
      "  granite counters                         0.0195\n",
      "  tile floor                               0.0188\n",
      "  Good natural light                       0.0158\n",
      "  modern fixtures                          0.0155\n",
      "  mature tree                              0.0140\n",
      "  ample natural light                      0.0137\n",
      "  natural light from window                0.0124\n",
      "  natural light                            0.0121\n",
      "  Carpeted floor                           0.0117\n",
      "  recessed lighting                        0.0115\n",
      "\n",
      "Topic 2 (1246 properties):\n",
      "  Neutral paint                            0.0344\n",
      "  Ceiling fan                              0.0278\n",
      "  Carpeted floor                           0.0273\n",
      "  Neutral finishes                         0.0250\n",
      "  Hardwood flooring                        0.0168\n",
      "  tile floor                               0.0160\n",
      "  tub-shower combo                         0.0152\n",
      "  fireplace                                0.0146\n",
      "  wood cabinetry                           0.0129\n",
      "  carpet flooring                          0.0124\n",
      "  large window                             0.0124\n",
      "  Good natural light                       0.0121\n",
      "  single sink vanity                       0.0105\n",
      "  natural light                            0.0099\n",
      "  standard fixtures                        0.0095\n",
      "\n",
      "Topic 3 (1097 properties):\n",
      "  Neutral paint                            0.0275\n",
      "  Hardwood flooring                        0.0272\n",
      "  Ceiling fan                              0.0266\n",
      "  Carpeted floor                           0.0264\n",
      "  laminate counters                        0.0176\n",
      "  natural light from window                0.0159\n",
      "  Good natural light                       0.0158\n",
      "  tile floor                               0.0151\n",
      "  mature tree                              0.0138\n",
      "  compact layout                           0.0127\n",
      "  single window                            0.0107\n",
      "  small footprint                          0.0103\n",
      "  Neutral finishes                         0.0102\n",
      "  brick exterior                           0.0102\n",
      "  natural light                            0.0088\n",
      "\n",
      "Topic 4 (961 properties):\n",
      "  Carpeted floor                           0.0351\n",
      "  Neutral paint                            0.0301\n",
      "  Ceiling fan                              0.0257\n",
      "  carpet flooring                          0.0241\n",
      "  Neutral finishes                         0.0227\n",
      "  laminate counters                        0.0206\n",
      "  Hardwood flooring                        0.0172\n",
      "  natural light from window                0.0166\n",
      "  Good natural light                       0.0166\n",
      "  neutral wall color                       0.0161\n",
      "  neutral paint and trim                   0.0139\n",
      "  tile floor                               0.0128\n",
      "  single sink vanity                       0.0119\n",
      "  vinyl siding exterior                    0.0114\n",
      "  wood cabinetry                           0.0111\n",
      "\n",
      "Topic 5 (591 properties):\n",
      "  Hardwood flooring                        0.0387\n",
      "  Ceiling fan                              0.0281\n",
      "  tile floor                               0.0223\n",
      "  Good natural light                       0.0203\n",
      "  Neutral paint                            0.0182\n",
      "  natural light from window                0.0180\n",
      "  mature tree                              0.0179\n",
      "  Carpeted floor                           0.0159\n",
      "  ample natural light                      0.0155\n",
      "  Neutral finishes                         0.0147\n",
      "  white cabinetry                          0.0131\n",
      "  mature landscaping                       0.0123\n",
      "  stainless steel appliances               0.0115\n",
      "  formal dining area                       0.0110\n",
      "  chandelier lighting                      0.0101\n",
      "\n",
      "Topic 6 (1156 properties):\n",
      "  Hardwood flooring                        0.0326\n",
      "  Ceiling fan                              0.0251\n",
      "  Carpeted floor                           0.0212\n",
      "  tile floor                               0.0205\n",
      "  ample natural light                      0.0205\n",
      "  granite counters                         0.0190\n",
      "  Neutral finishes                         0.0168\n",
      "  stainless steel appliances               0.0154\n",
      "  Good natural light                       0.0148\n",
      "  natural light from window                0.0139\n",
      "  recessed lighting                        0.0138\n",
      "  mature landscaping                       0.0123\n",
      "  large window                             0.0121\n",
      "  double vanities                          0.0116\n",
      "  formal dining area                       0.0108\n",
      "\n",
      "Topic 7 (1547 properties):\n",
      "  Hardwood flooring                        0.0311\n",
      "  Ceiling fan                              0.0256\n",
      "  Carpeted floor                           0.0252\n",
      "  Neutral finishes                         0.0222\n",
      "  tile floor                               0.0202\n",
      "  ample natural light                      0.0186\n",
      "  Good natural light                       0.0175\n",
      "  stainless steel appliances               0.0171\n",
      "  Neutral paint                            0.0163\n",
      "  carpet flooring                          0.0152\n",
      "  natural light from window                0.0146\n",
      "  granite counters                         0.0132\n",
      "  chandelier lighting                      0.0116\n",
      "  attached 2-car garage                    0.0115\n",
      "  formal dining area                       0.0111\n",
      "\n",
      "Topic 8 (1491 properties):\n",
      "  Hardwood flooring                        0.0248\n",
      "  Ceiling fan                              0.0221\n",
      "  Carpeted floor                           0.0203\n",
      "  Neutral paint                            0.0196\n",
      "  tile floor                               0.0191\n",
      "  Neutral finishes                         0.0172\n",
      "  large window                             0.0168\n",
      "  natural light                            0.0145\n",
      "  granite counters                         0.0135\n",
      "  ample natural light                      0.0132\n",
      "  double vanities                          0.0132\n",
      "  fireplace                                0.0131\n",
      "  chandelier                               0.0130\n",
      "  stainless steel appliances               0.0115\n",
      "  Good natural light                       0.0112\n",
      "\n",
      "Topic 9 (1181 properties):\n",
      "  Carpeted floor                           0.0227\n",
      "  Ceiling fan                              0.0222\n",
      "  Neutral paint                            0.0215\n",
      "  Hardwood flooring                        0.0206\n",
      "  Neutral finishes                         0.0178\n",
      "  Good natural light                       0.0170\n",
      "  tile floor                               0.0166\n",
      "  natural light from window                0.0155\n",
      "  mature tree                              0.0139\n",
      "  ample natural light                      0.0112\n",
      "  natural light                            0.0111\n",
      "  neutral decor                            0.0101\n",
      "  compact layout                           0.0096\n",
      "  carpet flooring                          0.0093\n",
      "  wood cabinetry                           0.0088\n",
      "\n",
      "Topic 10 (1252 properties):\n",
      "  attached 2-car garage                    0.1373\n",
      "  covered front porch                      0.0842\n",
      "  concrete driveway                        0.0688\n",
      "  brick exterior                           0.0596\n",
      "  attached garage                          0.0535\n",
      "  two-story single-family home             0.0474\n",
      "  mature trees / landscaping               0.0367\n",
      "  well-maintained lawn and landscaping     0.0350\n",
      "  covered front porch/entry                0.0292\n",
      "  gravel driveway                          0.0257\n",
      "  driveway                                 0.0239\n",
      "  well-maintained landscaping              0.0218\n",
      "  attached single-car garage               0.0193\n",
      "  single-story ranch style                 0.0168\n",
      "  mature shade tree                        0.0160\n",
      "\n",
      "============================================================\n",
      "TOPIC SUMMARY\n",
      "============================================================\n",
      " topic                                                                                 top_5_features  num_properties  cluster_doc_count  avg_topic_strength                                                   cluster_label\n",
      "     1    Hardwood flooring, Ceiling fan, Neutral paint, stainless steel appliances, Neutral finishes             836                836            0.073605                 Hardwood flooring + Ceiling fan + Neutral paint\n",
      "     2                Neutral paint, Ceiling fan, Carpeted floor, Neutral finishes, Hardwood flooring            1246               1246            0.109702                    Neutral paint + Ceiling fan + Carpeted floor\n",
      "     3               Neutral paint, Hardwood flooring, Ceiling fan, Carpeted floor, laminate counters            1097               1097            0.096584                 Neutral paint + Hardwood flooring + Ceiling fan\n",
      "     4                  Carpeted floor, Neutral paint, Ceiling fan, carpet flooring, Neutral finishes             961                961            0.084610                    Carpeted floor + Neutral paint + Ceiling fan\n",
      "     5                  Hardwood flooring, Ceiling fan, tile floor, Good natural light, Neutral paint             591                591            0.052034                    Hardwood flooring + Ceiling fan + tile floor\n",
      "     6                Hardwood flooring, Ceiling fan, Carpeted floor, tile floor, ample natural light            1156               1156            0.101778                Hardwood flooring + Ceiling fan + Carpeted floor\n",
      "     7                   Hardwood flooring, Ceiling fan, Carpeted floor, Neutral finishes, tile floor            1547               1547            0.136204                Hardwood flooring + Ceiling fan + Carpeted floor\n",
      "     8                      Hardwood flooring, Ceiling fan, Carpeted floor, Neutral paint, tile floor            1491               1491            0.131273                Hardwood flooring + Ceiling fan + Carpeted floor\n",
      "     9                Carpeted floor, Ceiling fan, Neutral paint, Hardwood flooring, Neutral finishes            1181               1181            0.103980                    Carpeted floor + Ceiling fan + Neutral paint\n",
      "    10 attached 2-car garage, covered front porch, concrete driveway, brick exterior, attached garage            1252               1252            0.110231 attached 2-car garage + covered front porch + concrete driveway\n",
      "\n",
      "Creating single feature columns...\n",
      "Counting pairs...\n",
      "Counting triples...\n",
      "Adding conditions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenny.lin/House_IQ_Setup/.venv1/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/jenny.lin/House_IQ_Setup/.venv1/lib/python3.13/site-packages/numpy/_core/_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Done: (11358, 1538)\n",
      "✓ Added 10 topic columns: topic_1 to topic_10\n",
      "✓ Added dominant_topic column (GSDMM hard clustering - each property assigned to ONE topic)\n",
      "✓ Added 300 single features\n",
      "✓ Added 20 pair features\n",
      "✓ Added 20 triple features\n",
      "✓ Added 6 condition features\n",
      "✓ No fragmentation warnings!\n",
      "✓ No duplicate columns!\n",
      "\n",
      "Properties in Topic 1:\n",
      "     dominant_topic\n",
      "7                 1\n",
      "13                1\n",
      "35                1\n",
      "42                1\n",
      "58                1\n",
      "78                1\n",
      "80                1\n",
      "87                1\n",
      "95                1\n",
      "101               1\n",
      "\n",
      "Properties per topic:\n",
      "  Topic 1: 836 properties - [Hardwood flooring + Ceiling fan + Neutral paint]\n",
      "  Topic 2: 1246 properties - [Neutral paint + Ceiling fan + Carpeted floor]\n",
      "  Topic 3: 1097 properties - [Neutral paint + Hardwood flooring + Ceiling fan]\n",
      "  Topic 4: 961 properties - [Carpeted floor + Neutral paint + Ceiling fan]\n",
      "  Topic 5: 591 properties - [Hardwood flooring + Ceiling fan + tile floor]\n",
      "  Topic 6: 1156 properties - [Hardwood flooring + Ceiling fan + Carpeted floor]\n",
      "  Topic 7: 1547 properties - [Hardwood flooring + Ceiling fan + Carpeted floor]\n",
      "  Topic 8: 1491 properties - [Hardwood flooring + Ceiling fan + Carpeted floor]\n",
      "  Topic 9: 1181 properties - [Carpeted floor + Ceiling fan + Neutral paint]\n",
      "  Topic 10: 1252 properties - [attached 2-car garage + covered front porch + concrete driveway]\n",
      "\n",
      "============================================================\n",
      "TOP 10 FEATURES FOR EACH TOPIC\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'n_topics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 461\u001B[39m\n\u001B[32m    459\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mTOP 10 FEATURES FOR EACH TOPIC\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    460\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m*\u001B[32m60\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m461\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m topic_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[43mn_topics\u001B[49m):\n\u001B[32m    462\u001B[39m     topic_num = topic_idx + \u001B[32m1\u001B[39m\n\u001B[32m    463\u001B[39m     num_props = \u001B[38;5;28mint\u001B[39m((df[\u001B[33m'\u001B[39m\u001B[33mdominant_topic\u001B[39m\u001B[33m'\u001B[39m] == topic_num).sum())\n",
      "\u001B[31mNameError\u001B[39m: name 'n_topics' is not defined"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4d8f2a7025346e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
