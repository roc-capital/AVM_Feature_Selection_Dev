{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-23T17:51:01.733965Z",
     "start_time": "2025-12-23T17:45:38.100543Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "import time, os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "# Config\n",
    "INCLUDE_MLS, INCLUDE_CENSUS, INCLUDE_NEIGHBORHOOD, INCLUDE_IMAGE = True, True, True, False\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS = 20000, 0.3, 42, -1, 8\n",
    "N_EST, QUANTILES = 500, [0.1, 0.5, 0.9]\n",
    "\n",
    "INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "BASE_FEATS = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"effective_year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\"fireplace_code\",\"latitude\",\"longitude\",\"geo_cluster\"]\n",
    "ENG_FEATS = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\"]\n",
    "PRIOR_FEATS = [\"prior_sale_price\",\"prior_price_per_sqft\",\"sqft_per_prior_dollar\",\"years_since_last_sale\",\"expected_appreciation\",\"has_prior_sale\",\"recently_sold\"]\n",
    "CLUSTER_FEATS = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "CENSUS_FEATS = [\"total_population_25plus\",\"male_bachelors_degree\",\"female_bachelors_degree\",\"pct_bachelors_degree\",\"total_population\",\"median_earnings_total\",\"median_earnings_male\",\"median_earnings_female\",\"median_household_income\",\"median_home_value\",\"median_gross_rent\",\"owner_occupied_units\",\"renter_occupied_units\",\"pct_owner_occupied\",\"occupied_units\",\"vacant_units\",\"median_age\",\"civilian_employed\",\"civilian_unemployed\",\"unemployment_rate\",\"income_education_score\"]\n",
    "ELECT_FEATS = [\"votes_gop\",\"votes_dem\",\"total_votes\",\"per_gop\",\"per_dem\",\"per_point_diff\",\"dem_margin\",\"rep_margin\"]\n",
    "IMG_FEATS = [\"topic_1\",\"topic_2\",\"topic_3\",\"topic_4\",\"topic_5\",\"topic_6\",\"topic_7\",\"topic_8\",\"topic_9\",\"topic_10\",\"gran_c_in\",\"gran_c_ex\",\"gran_c\",\"high_c_in\",\"high_c_ex\",\"high_c\"]\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8') if set(df[c].dropna().unique()).issubset({0,1}) else df[c].astype('int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return optimize_dtypes(df), y_col, id_col, state_col\n",
    "\n",
    "def assign_segment(row, df_q):\n",
    "    \"\"\"Robust segmentation that handles missing columns gracefully\"\"\"\n",
    "    # Size tier - use available size metrics\n",
    "    size = 'medium'\n",
    "    if 'living_sqft' in df_q.columns and 'living_sqft' in row.index:\n",
    "        sqft = row.get('living_sqft',0)\n",
    "        if sqft > 0:\n",
    "            q25,q50,q75 = df_q['living_sqft'].quantile([.25,.5,.75])\n",
    "            size = 'compact' if sqft<q25 else 'medium' if sqft<q50 else 'large' if sqft<q75 else 'xlarge'\n",
    "\n",
    "    # Age tier\n",
    "    age_class = 'mature'\n",
    "    if 'property_age' in row.index:\n",
    "        age = row.get('property_age',50)\n",
    "        age_class = 'new' if age<=5 else 'recent' if age<=15 else 'mature' if age<=30 else 'aging' if age<=50 else 'old'\n",
    "    elif 'year_built' in row.index:\n",
    "        yb = row.get('year_built',1980)\n",
    "        age = 2024 - yb\n",
    "        age_class = 'new' if age<=5 else 'recent' if age<=15 else 'mature' if age<=30 else 'aging' if age<=50 else 'old'\n",
    "\n",
    "    # Location quality\n",
    "    loc = 'mid'\n",
    "    if INCLUDE_CENSUS and 'median_household_income' in df_q.columns and 'median_household_income' in row.index:\n",
    "        inc,edu = row.get('median_household_income',0), row.get('pct_bachelors_degree',0)\n",
    "        iq33,iq67 = df_q['median_household_income'].quantile([.33,.67])\n",
    "        eq33,eq67 = df_q['pct_bachelors_degree'].quantile([.33,.67]) if 'pct_bachelors_degree' in df_q.columns else (0,0)\n",
    "        loc = 'prime' if inc>iq67 or edu>eq67 else 'basic' if inc<iq33 and edu<eq33 else 'mid'\n",
    "\n",
    "    # Luxury tier\n",
    "    lux_tier = 'mid'\n",
    "    if 'luxury_score' in df_q.columns and 'luxury_score' in row.index:\n",
    "        lux = row.get('luxury_score',0)\n",
    "        lq33,lq67 = df_q['luxury_score'].quantile([.33,.67])\n",
    "        lux_tier = 'upscale' if lux>lq67 else 'standard' if lux<lq33 else 'mid'\n",
    "\n",
    "    return f\"{size}_{age_class}_{loc}_{lux_tier}\"\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    if 'living_sqft' in df.columns and 'bedrooms' in df.columns: df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns: df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns: df['lot_acres'] = df['lot_sqft']/43560\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'] = 2024-df['year_built']\n",
    "        df['is_new'] = (df['property_age']<=5).astype('int8')\n",
    "        df['age_squared'] = df['property_age']**2\n",
    "    if 'garage_spaces' in df.columns: df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "    if 'living_sqft' in df.columns: df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "    if INCLUDE_CENSUS and all(c in df.columns for c in ['median_household_income','pct_bachelors_degree']):\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "    if 'prior_sale_price' in df.columns and 'living_sqft' in df.columns:\n",
    "        df['prior_price_per_sqft'] = df['prior_sale_price']/(df['living_sqft']+1)\n",
    "        df['sqft_per_prior_dollar'] = df['living_sqft']/(df['prior_sale_price']+1)\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        if 'prior_sale_price' in df.columns:\n",
    "            df['expected_appreciation'] = df['prior_sale_price']*(1.04)**df['years_since_last_sale']\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df.loc[missing,'median_home_value'] if 'median_home_value' in df.columns and INCLUDE_CENSUS else df['prior_sale_price'].median()\n",
    "    if 'years_since_last_sale' in df.columns: df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "    if with_price and y_col in df.columns and 'living_sqft' in df.columns:\n",
    "        df['price_per_sqft'] = df[y_col]/(df['living_sqft']+1)\n",
    "        df['sqft_per_dollar'] = df['living_sqft']/(df[y_col]+1)\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df):\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0; return df\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum()<N_CLUSTERS: df['geo_cluster'] = 0; return df\n",
    "    df['geo_cluster'] = 0\n",
    "    kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "    df.loc[valid,'geo_cluster'] = kmeans.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    return df\n",
    "\n",
    "def add_cluster_feats(train, test, y_col):\n",
    "    if 'geo_cluster' not in train.columns or y_col not in train.columns:\n",
    "        for d in [train,test]: d['cluster_avg_price'] = d['cluster_med_price'] = train[y_col].median() if y_col in train.columns else 0\n",
    "        return train, test\n",
    "    stats = train.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "    stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "    train = train.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    test = test.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    return train, test\n",
    "\n",
    "def filter_outliers(df, name, y_col):\n",
    "    orig = len(df)\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)]\n",
    "    if 'sqft_per_dollar' in df.columns: df = df[df['sqft_per_dollar']<=df['sqft_per_dollar'].quantile(.95)]\n",
    "    if 'price_per_sqft' in df.columns: df = df.drop(columns=['price_per_sqft','sqft_per_dollar'])\n",
    "    if 'lot_sqft' in df.columns: df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "    if 'year_built' in df.columns: df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "    if len(df)>=100:\n",
    "        try:\n",
    "            feats = [c for c in ['living_sqft','lot_sqft',y_col,'year_built'] if c in df.columns]\n",
    "            if len(feats)>=3:\n",
    "                X = df[feats].fillna(df[feats].median())\n",
    "                df = df[IsolationForest(contamination=.05,random_state=RAND_STATE,n_jobs=N_JOBS).fit_predict(X)==1]\n",
    "        except: pass\n",
    "    pct_filt = (orig-len(df))/orig*100 if orig>0 else 0\n",
    "    if pct_filt > 0: print(f\"  {name}: {orig:,}→{len(df):,} ({pct_filt:.1f}% filtered)\")\n",
    "    return df\n",
    "\n",
    "def train_model(X, y, q):\n",
    "    return XGBRegressor(objective='reg:quantileerror',quantile_alpha=q,n_estimators=N_EST,learning_rate=.05,max_depth=6,min_child_weight=3,subsample=.8,colsample_bytree=.8,random_state=RAND_STATE,n_jobs=N_JOBS,tree_method='hist').fit(X,y,verbose=False)\n",
    "\n",
    "def feature_importance(models, feat_names, metrics):\n",
    "    rows = []\n",
    "    for seg, mdl in models.items():\n",
    "        scores = mdl['q50'].get_booster().get_score(importance_type=\"gain\")\n",
    "        w = metrics[seg][\"n_test\"]\n",
    "        for k,v in scores.items():\n",
    "            idx = int(k[1:])\n",
    "            if idx<len(feat_names): rows.append((feat_names[idx],v,w))\n",
    "    if not rows: return pd.DataFrame(columns=[\"feature\",\"importance\"])\n",
    "    df = pd.DataFrame(rows, columns=[\"feature\",\"gain\",\"weight\"])\n",
    "    out = df.assign(wg=df[\"gain\"]*df[\"weight\"]).groupby(\"feature\",as_index=False).agg(tg=(\"wg\",\"sum\")).sort_values(\"tg\",ascending=False)\n",
    "    out[\"importance\"] = out[\"tg\"]/out[\"tg\"].sum()\n",
    "    return out[[\"feature\",\"importance\"]].head(100)\n",
    "\n",
    "def prepare_data(df, y_col, id_col, state_col):\n",
    "    print(f\"\\nPreparing data...\")\n",
    "    df = df[df[y_col]>=MIN_PRICE]\n",
    "    print(f\"{len(df):,} records after price filter\")\n",
    "    df = engineer(geo_cluster(df), y_col)\n",
    "    feat_groups = []\n",
    "    if INCLUDE_MLS: feat_groups.extend([BASE_FEATS,ENG_FEATS,PRIOR_FEATS,CLUSTER_FEATS])\n",
    "    if INCLUDE_CENSUS: feat_groups.append(CENSUS_FEATS)\n",
    "    if INCLUDE_NEIGHBORHOOD: feat_groups.append(ELECT_FEATS)\n",
    "    if INCLUDE_IMAGE: feat_groups.append(IMG_FEATS)\n",
    "    all_feats = [f for g in feat_groups for f in g]\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "    return df.dropna(subset=[y_col]), feats\n",
    "\n",
    "def train_segments(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining segmented models on {len(df):,} properties\")\n",
    "    df['seg'] = df.apply(lambda r: assign_segment(r,df), axis=1)\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    print(f\"{len(seg_cnts)} segments created\")\n",
    "    for seg, cnt in seg_cnts.head(10).items(): print(f\"  {seg}: {cnt:,}\")\n",
    "    small = seg_cnts[seg_cnts<50].index\n",
    "    if len(small)>0:\n",
    "        df.loc[df['seg'].isin(small),'seg'] = 'other'\n",
    "        print(f\"Merged {len(small)} small segments into 'other'\")\n",
    "    models, metrics, preds_list = {}, {}, []\n",
    "    for seg in df['seg'].unique():\n",
    "        seg_df = df[df['seg']==seg].copy()\n",
    "        if len(seg_df)<50: continue\n",
    "        seg_df = filter_outliers(seg_df, seg, y_col)\n",
    "        if len(seg_df)<50: continue\n",
    "        train_idx = seg_df.sample(frac=1-TEST_SIZE,random_state=RAND_STATE).index\n",
    "        train_df, test_df = seg_df.loc[train_idx].copy(), seg_df.loc[seg_df.index.difference(train_idx)].copy()\n",
    "        train_df, test_df = add_cluster_feats(train_df, test_df, y_col)\n",
    "        X_tr,y_tr = train_df[feats].values, train_df[y_col].values\n",
    "        X_te,y_te = test_df[feats].values, test_df[y_col].values\n",
    "        ids_te = test_df[id_col].values\n",
    "        states_te = test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "        seg_models, seg_preds = {}, []\n",
    "        for q in QUANTILES:\n",
    "            m = train_model(X_tr,y_tr,q)\n",
    "            seg_models[f\"q{int(q*100)}\"] = m\n",
    "            seg_preds.append(m.predict(X_te))\n",
    "        models[seg] = seg_models\n",
    "        y_pred = seg_preds[1]\n",
    "        mae,mape = mean_absolute_error(y_te,y_pred), np.mean(np.abs((y_te-y_pred)/y_te))*100\n",
    "        r2,cov = r2_score(y_te,y_pred), np.mean((y_te>=seg_preds[0])&(y_te<=seg_preds[2]))*100\n",
    "        metrics[seg] = {'n_train':len(X_tr),'n_test':len(X_te),'mae':mae,'mape':mape,'r2':r2,'cov':cov,'p_min':seg_df[y_col].min(),'p_max':seg_df[y_col].max(),'p_med':seg_df[y_col].median()}\n",
    "        print(f\"  {seg}: {len(test_df):,} test | MAE:${mae:,.0f} R²:{r2:.3f}\")\n",
    "        preds_list.append(pd.DataFrame({'property_id':ids_te,'state':states_te,'actual':y_te,'predicted':y_pred,'pred_lower':seg_preds[0],'pred_upper':seg_preds[2],'segment':seg}))\n",
    "    return {'models':models,'metrics':metrics,'predictions':pd.concat(preds_list),'feature_importance':feature_importance(models,feats,metrics),'feature_names':feats}\n",
    "\n",
    "def save_results(results, out_dir):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi = results['predictions'], results['metrics'], results['feature_importance']\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    # Summary\n",
    "    ws = wb.create_sheet(\"Summary\",0)\n",
    "    ws['A1'], ws['A1'].font = 'GRANULAR SEGMENTED AVM', Font(bold=True,size=14)\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    data = [['Metric','Value'],['Properties',len(preds)],['Segments',len(metrics)],['R²',f'{r2:.4f}'],['MAE',f'${mae:,.0f}'],['MAPE%',f'{mape:.2f}']]\n",
    "    for i,(k,v) in enumerate(data,5): ws[f'A{i}'],ws[f'A{i}'].font,ws[f'B{i}'] = k,Font(bold=True),v\n",
    "\n",
    "    # Segments\n",
    "    ws = wb.create_sheet(\"Segments\")\n",
    "    seg_df = pd.DataFrame([{**{'Segment':s},**m} for s,m in metrics.items()])\n",
    "    for i,h in enumerate(seg_df.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(seg_df.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    # Predictions\n",
    "    ws = wb.create_sheet(\"Predictions\")\n",
    "    for i,h in enumerate(preds.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(preds.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/granular_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "    preds.to_csv(f\"{out_dir}/predictions_{ts}.csv\",index=False)\n",
    "    seg_df.to_csv(f\"{out_dir}/segments_{ts}.csv\",index=False)\n",
    "    fi.to_csv(f\"{out_dir}/importance_{ts}.csv\",index=False)\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved with timestamp: {ts}\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60)\n",
    "    print(\"GRANULAR SEGMENTED AVM\")\n",
    "    print(\"=\"*60)\n",
    "    df, y_col, id_col, state_col = load_data(INPUT_PATH)\n",
    "    df, feats = prepare_data(df, y_col, id_col, state_col)\n",
    "    results = train_segments(df, feats, y_col, id_col, state_col)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR)\n",
    "    preds = results['predictions']\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ COMPLETE in {time.time()-t0:.1f}s\")\n",
    "    print(f\"  {len(preds):,} properties | {preds['segment'].nunique()} segments\")\n",
    "    print(f\"  R²: {r2:.4f} | MAE: ${mae:,.0f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "if __name__ == \"__main__\": main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRANULAR SEGMENTED AVM\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 129.6MB | Price:sale_price ID:property_id\n",
      "\n",
      "Preparing data...\n",
      "127,258 records after price filter\n",
      "51/60 features available\n",
      "\n",
      "Training segmented models on 127,258 properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3d76e3b52ad4a3db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T18:19:20.606654Z",
     "start_time": "2025-12-23T18:19:12.228724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import time, os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Config - FIXED PARAMETERS\n",
    "INCLUDE_MLS, INCLUDE_CENSUS, INCLUDE_NEIGHBORHOOD, INCLUDE_IMAGE = True, True, True, False\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS = 20000, 0.3, 42, -1, 8\n",
    "N_EST, QUANTILES, MAX_SEGMENTS = 100, [0.1, 0.5, 0.9], 7  # MAX 7 SEGMENTS\n",
    "\n",
    "INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "BASE_FEATS = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"effective_year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\"fireplace_code\",\"latitude\",\"longitude\",\"geo_cluster\"]\n",
    "ENG_FEATS = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\"]\n",
    "PRIOR_FEATS = [\"prior_sale_price\",\"prior_price_per_sqft\",\"sqft_per_prior_dollar\",\"years_since_last_sale\",\"expected_appreciation\",\"has_prior_sale\",\"recently_sold\"]\n",
    "CLUSTER_FEATS = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "CENSUS_FEATS = [\"total_population_25plus\",\"male_bachelors_degree\",\"female_bachelors_degree\",\"pct_bachelors_degree\",\"total_population\",\"median_earnings_total\",\"median_earnings_male\",\"median_earnings_female\",\"median_household_income\",\"median_home_value\",\"median_gross_rent\",\"owner_occupied_units\",\"renter_occupied_units\",\"pct_owner_occupied\",\"occupied_units\",\"vacant_units\",\"median_age\",\"civilian_employed\",\"civilian_unemployed\",\"unemployment_rate\",\"income_education_score\"]\n",
    "ELECT_FEATS = [\"votes_gop\",\"votes_dem\",\"total_votes\",\"per_gop\",\"per_dem\",\"per_point_diff\",\"dem_margin\",\"rep_margin\"]\n",
    "IMG_FEATS = [\"topic_1\",\"topic_2\",\"topic_3\",\"topic_4\",\"topic_5\",\"topic_6\",\"topic_7\",\"topic_8\",\"topic_9\",\"topic_10\",\"gran_c_in\",\"gran_c_ex\",\"gran_c\",\"high_c_in\",\"high_c_ex\",\"high_c\"]\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8') if set(df[c].dropna().unique()).issubset({0,1}) else df[c].astype('int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return optimize_dtypes(df), y_col, id_col, state_col\n",
    "\n",
    "def assign_segments_simple(df):\n",
    "    \"\"\"Segment based on PROPERTY CHARACTERISTICS only - NO PRICE\"\"\"\n",
    "\n",
    "    # Primary: SIZE (living sqft)\n",
    "    if 'living_sqft' in df.columns:\n",
    "        sqft = df['living_sqft'].fillna(df['living_sqft'].median())\n",
    "        q33, q67 = sqft.quantile([.33, .67])\n",
    "        size_tier = pd.Series(['medium'] * len(df), index=df.index)\n",
    "        size_tier[sqft < q33] = 'small'\n",
    "        size_tier[sqft > q67] = 'large'\n",
    "    else:\n",
    "        size_tier = pd.Series(['medium'] * len(df), index=df.index)\n",
    "\n",
    "    # Secondary: BEDROOMS (property complexity)\n",
    "    if 'bedrooms' in df.columns:\n",
    "        beds = df['bedrooms'].fillna(df['bedrooms'].median())\n",
    "        beds_median = beds.median()\n",
    "        room_tier = pd.Series(['standard'] * len(df), index=df.index)\n",
    "        room_tier[beds <= 2] = 'compact'\n",
    "        room_tier[beds >= 4] = 'family'\n",
    "    else:\n",
    "        room_tier = pd.Series(['standard'] * len(df), index=df.index)\n",
    "\n",
    "    # Tertiary (optional): AGE\n",
    "    if 'year_built' in df.columns:\n",
    "        age = 2024 - df['year_built'].fillna(1980)\n",
    "        age_tier = pd.Series(['mature'] * len(df), index=df.index)\n",
    "        age_tier[age <= 10] = 'new'\n",
    "        age_tier[age > 40] = 'older'\n",
    "    else:\n",
    "        age_tier = pd.Series(['mature'] * len(df), index=df.index)\n",
    "\n",
    "    # Combine into segments\n",
    "    # Format: size_rooms_age (e.g., \"small_compact_new\", \"large_family_mature\")\n",
    "    segments = size_tier + '_' + room_tier + '_' + age_tier\n",
    "\n",
    "    return segments\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    if 'living_sqft' in df.columns and 'bedrooms' in df.columns: df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns: df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns: df['lot_acres'] = df['lot_sqft']/43560\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'] = 2024-df['year_built']\n",
    "        df['is_new'] = (df['property_age']<=5).astype('int8')\n",
    "        df['age_squared'] = df['property_age']**2\n",
    "    if 'garage_spaces' in df.columns: df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "    if 'living_sqft' in df.columns: df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "    if INCLUDE_CENSUS and all(c in df.columns for c in ['median_household_income','pct_bachelors_degree']):\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "    if 'prior_sale_price' in df.columns and 'living_sqft' in df.columns:\n",
    "        df['prior_price_per_sqft'] = df['prior_sale_price']/(df['living_sqft']+1)\n",
    "        df['sqft_per_prior_dollar'] = df['living_sqft']/(df['prior_sale_price']+1)\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        if 'prior_sale_price' in df.columns:\n",
    "            df['expected_appreciation'] = df['prior_sale_price']*(1.04)**df['years_since_last_sale']\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df.loc[missing,'median_home_value'] if 'median_home_value' in df.columns and INCLUDE_CENSUS else df['prior_sale_price'].median()\n",
    "    if 'years_since_last_sale' in df.columns: df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "    if with_price and y_col in df.columns and 'living_sqft' in df.columns:\n",
    "        df['price_per_sqft'] = df[y_col]/(df['living_sqft']+1)\n",
    "        df['sqft_per_dollar'] = df['living_sqft']/(df[y_col]+1)\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df):\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0; return df\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum()<N_CLUSTERS: df['geo_cluster'] = 0; return df\n",
    "    df['geo_cluster'] = 0\n",
    "    kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "    df.loc[valid,'geo_cluster'] = kmeans.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    return df\n",
    "\n",
    "def add_cluster_feats(train, test, y_col):\n",
    "    if 'geo_cluster' not in train.columns or y_col not in train.columns:\n",
    "        for d in [train,test]: d['cluster_avg_price'] = d['cluster_med_price'] = train[y_col].median() if y_col in train.columns else 0\n",
    "        return train, test\n",
    "    stats = train.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "    stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "    train = train.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    test = test.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    return train, test\n",
    "\n",
    "def filter_outliers(df, name, y_col):\n",
    "    orig = len(df)\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)]\n",
    "    if 'sqft_per_dollar' in df.columns: df = df[df['sqft_per_dollar']<=df['sqft_per_dollar'].quantile(.95)]\n",
    "    if 'price_per_sqft' in df.columns: df = df.drop(columns=['price_per_sqft','sqft_per_dollar'])\n",
    "    if 'lot_sqft' in df.columns: df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "    if 'year_built' in df.columns: df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "    if len(df)>=100:\n",
    "        try:\n",
    "            feats = [c for c in ['living_sqft','lot_sqft',y_col,'year_built'] if c in df.columns]\n",
    "            if len(feats)>=3:\n",
    "                X = df[feats].fillna(df[feats].median())\n",
    "                df = df[IsolationForest(contamination=.05,random_state=RAND_STATE,n_jobs=N_JOBS).fit_predict(X)==1]\n",
    "        except: pass\n",
    "    pct_filt = (orig-len(df))/orig*100 if orig>0 else 0\n",
    "    if pct_filt > 0: print(f\"  {name}: {orig:,}→{len(df):,} ({pct_filt:.1f}% filtered)\")\n",
    "    return df\n",
    "\n",
    "def train_model(X, y, q):\n",
    "    return XGBRegressor(\n",
    "        objective='reg:quantileerror',\n",
    "        quantile_alpha=q,\n",
    "        n_estimators=N_EST,\n",
    "        learning_rate=.1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        subsample=.8,\n",
    "        colsample_bytree=.8,\n",
    "        random_state=RAND_STATE,\n",
    "        n_jobs=N_JOBS,\n",
    "        tree_method='hist'\n",
    "    ).fit(X, y, verbose=False)\n",
    "\n",
    "def get_feature_importance_per_segment(model, feat_names, top_n=20):\n",
    "    \"\"\"Extract feature importance for a single segment model\"\"\"\n",
    "    scores = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    importance_list = []\n",
    "    for k, v in scores.items():\n",
    "        idx = int(k[1:])\n",
    "        if idx < len(feat_names):\n",
    "            importance_list.append((feat_names[idx], v))\n",
    "\n",
    "    # Sort and normalize\n",
    "    importance_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    total_gain = sum(v for _, v in importance_list)\n",
    "\n",
    "    if total_gain > 0:\n",
    "        importance_df = pd.DataFrame([\n",
    "            {'feature': feat, 'gain': gain, 'importance': gain/total_gain}\n",
    "            for feat, gain in importance_list[:top_n]\n",
    "        ])\n",
    "    else:\n",
    "        importance_df = pd.DataFrame(columns=['feature', 'gain', 'importance'])\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "def feature_importance(models, feat_names, metrics):\n",
    "    \"\"\"Global feature importance weighted by segment size\"\"\"\n",
    "    rows = []\n",
    "    for seg, mdl in models.items():\n",
    "        scores = mdl['q50'].get_booster().get_score(importance_type=\"gain\")\n",
    "        w = metrics[seg][\"n_test\"]\n",
    "        for k,v in scores.items():\n",
    "            idx = int(k[1:])\n",
    "            if idx<len(feat_names): rows.append((feat_names[idx],v,w))\n",
    "    if not rows: return pd.DataFrame(columns=[\"feature\",\"importance\"])\n",
    "    df = pd.DataFrame(rows, columns=[\"feature\",\"gain\",\"weight\"])\n",
    "    out = df.assign(wg=df[\"gain\"]*df[\"weight\"]).groupby(\"feature\",as_index=False).agg(tg=(\"wg\",\"sum\")).sort_values(\"tg\",ascending=False)\n",
    "    out[\"importance\"] = out[\"tg\"]/out[\"tg\"].sum()\n",
    "    return out[[\"feature\",\"importance\"]].head(100)\n",
    "\n",
    "def prepare_data(df, y_col, id_col, state_col):\n",
    "    print(f\"\\nPreparing data...\")\n",
    "    df = df[df[y_col]>=MIN_PRICE]\n",
    "    print(f\"{len(df):,} records after price filter\")\n",
    "    df = engineer(geo_cluster(df), y_col)\n",
    "    feat_groups = []\n",
    "    if INCLUDE_MLS: feat_groups.extend([BASE_FEATS,ENG_FEATS,PRIOR_FEATS,CLUSTER_FEATS])\n",
    "    if INCLUDE_CENSUS: feat_groups.append(CENSUS_FEATS)\n",
    "    if INCLUDE_NEIGHBORHOOD: feat_groups.append(ELECT_FEATS)\n",
    "    if INCLUDE_IMAGE: feat_groups.append(IMG_FEATS)\n",
    "    all_feats = [f for g in feat_groups for f in g]\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "    return df.dropna(subset=[y_col]), feats\n",
    "\n",
    "def train_segments(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining segmented models on {len(df):,} properties\")\n",
    "\n",
    "    # Segment based on property characteristics (NO PRICE)\n",
    "    df['seg'] = assign_segments_simple(df)\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    print(f\"{len(seg_cnts)} segments created\")\n",
    "    for seg, cnt in seg_cnts.head(MAX_SEGMENTS).items(): print(f\"  {seg}: {cnt:,}\")\n",
    "\n",
    "    # Merge small segments\n",
    "    small = seg_cnts[seg_cnts<50].index\n",
    "    if len(small)>0:\n",
    "        df.loc[df['seg'].isin(small),'seg'] = 'other'\n",
    "        print(f\"Merged {len(small)} small segments into 'other'\")\n",
    "\n",
    "    # If still too many segments, merge smallest ones\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    if len(seg_cnts) > MAX_SEGMENTS:\n",
    "        keep_segs = seg_cnts.head(MAX_SEGMENTS-1).index\n",
    "        df.loc[~df['seg'].isin(keep_segs),'seg'] = 'other'\n",
    "        print(f\"Consolidated to {MAX_SEGMENTS} segments maximum\")\n",
    "\n",
    "    models, metrics, preds_list, segment_importances = {}, {}, [], {}\n",
    "\n",
    "    for seg in df['seg'].unique():\n",
    "        seg_df = df[df['seg']==seg].copy()\n",
    "        if len(seg_df)<50: continue\n",
    "        seg_df = filter_outliers(seg_df, seg, y_col)\n",
    "        if len(seg_df)<50: continue\n",
    "        train_idx = seg_df.sample(frac=1-TEST_SIZE,random_state=RAND_STATE).index\n",
    "        train_df, test_df = seg_df.loc[train_idx].copy(), seg_df.loc[seg_df.index.difference(train_idx)].copy()\n",
    "        train_df, test_df = add_cluster_feats(train_df, test_df, y_col)\n",
    "        X_tr,y_tr = train_df[feats].values, train_df[y_col].values\n",
    "        X_te,y_te = test_df[feats].values, test_df[y_col].values\n",
    "        ids_te = test_df[id_col].values\n",
    "        states_te = test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "        seg_models, seg_preds = {}, []\n",
    "        for q in QUANTILES:\n",
    "            m = train_model(X_tr,y_tr,q)\n",
    "            seg_models[f\"q{int(q*100)}\"] = m\n",
    "            seg_preds.append(m.predict(X_te))\n",
    "        models[seg] = seg_models\n",
    "\n",
    "        # Get feature importance for this segment\n",
    "        segment_importances[seg] = get_feature_importance_per_segment(seg_models['q50'], feats, top_n=20)\n",
    "\n",
    "        y_pred = seg_preds[1]\n",
    "        mae,mape = mean_absolute_error(y_te,y_pred), np.mean(np.abs((y_te-y_pred)/y_te))*100\n",
    "        r2,cov = r2_score(y_te,y_pred), np.mean((y_te>=seg_preds[0])&(y_te<=seg_preds[2]))*100\n",
    "        metrics[seg] = {'n_train':len(X_tr),'n_test':len(X_te),'mae':mae,'mape':mape,'r2':r2,'cov':cov,'p_min':seg_df[y_col].min(),'p_max':seg_df[y_col].max(),'p_med':seg_df[y_col].median()}\n",
    "        print(f\"  {seg}: {len(test_df):,} test | MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.3f}\")\n",
    "        preds_list.append(pd.DataFrame({'property_id':ids_te,'state':states_te,'actual':y_te,'predicted':y_pred,'pred_lower':seg_preds[0],'pred_upper':seg_preds[2],'segment':seg}))\n",
    "\n",
    "    return {\n",
    "        'models': models,\n",
    "        'metrics': metrics,\n",
    "        'predictions': pd.concat(preds_list),\n",
    "        'feature_importance': feature_importance(models, feats, metrics),\n",
    "        'segment_importances': segment_importances,\n",
    "        'feature_names': feats\n",
    "    }\n",
    "\n",
    "def save_results(results, out_dir):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi = results['predictions'], results['metrics'], results['feature_importance']\n",
    "    seg_importances = results['segment_importances']\n",
    "\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    # Summary\n",
    "    ws = wb.create_sheet(\"Summary\",0)\n",
    "    ws['A1'], ws['A1'].font = 'GRANULAR SEGMENTED AVM', Font(bold=True,size=14)\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    data = [['Metric','Value'],['Properties',len(preds)],['Segments',len(metrics)],['R²',f'{r2:.4f}'],['MAE',f'${mae:,.0f}'],['MAPE%',f'{mape:.2f}%']]\n",
    "    for i,(k,v) in enumerate(data,5): ws[f'A{i}'],ws[f'A{i}'].font,ws[f'B{i}'] = k,Font(bold=True),v\n",
    "\n",
    "    # Segments\n",
    "    ws = wb.create_sheet(\"Segments\")\n",
    "    seg_df = pd.DataFrame([{**{'Segment':s},**m} for s,m in metrics.items()])\n",
    "    for i,h in enumerate(seg_df.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(seg_df.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    # Global Feature Importance\n",
    "    ws = wb.create_sheet(\"Global_Feature_Importance\")\n",
    "    ws['A1'], ws['A1'].font = 'Global Feature Importance (Weighted)', Font(bold=True,size=12)\n",
    "    for r_idx, row in enumerate(dataframe_to_rows(fi, index=False, header=True), 2):\n",
    "        for c_idx, value in enumerate(row, 1):\n",
    "            cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "            if r_idx == 2:  # Header row\n",
    "                cell.font = Font(bold=True, color='FFFFFF')\n",
    "                cell.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "\n",
    "    # Per-Segment Feature Importance (one sheet per segment)\n",
    "    for seg_name, seg_fi in seg_importances.items():\n",
    "        sheet_name = f\"FI_{seg_name}\"[:31]  # Excel sheet name limit\n",
    "        ws = wb.create_sheet(sheet_name)\n",
    "        ws['A1'] = f'Feature Importance: {seg_name}'\n",
    "        ws['A1'].font = Font(bold=True, size=12)\n",
    "\n",
    "        for r_idx, row in enumerate(dataframe_to_rows(seg_fi, index=False, header=True), 2):\n",
    "            for c_idx, value in enumerate(row, 1):\n",
    "                cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "                if r_idx == 2:  # Header row\n",
    "                    cell.font = Font(bold=True, color='FFFFFF')\n",
    "                    cell.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "\n",
    "    # Predictions\n",
    "    ws = wb.create_sheet(\"Predictions\")\n",
    "    for i,h in enumerate(preds.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(preds.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/granular_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "    preds.to_csv(f\"{out_dir}/predictions_{ts}.csv\",index=False)\n",
    "    seg_df.to_csv(f\"{out_dir}/segments_{ts}.csv\",index=False)\n",
    "    fi.to_csv(f\"{out_dir}/global_importance_{ts}.csv\",index=False)\n",
    "\n",
    "    # Save per-segment feature importance CSVs\n",
    "    for seg_name, seg_fi in seg_importances.items():\n",
    "        seg_fi.to_csv(f\"{out_dir}/importance_{seg_name}_{ts}.csv\", index=False)\n",
    "\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved with timestamp: {ts}\")\n",
    "    print(f\"✓ Per-segment feature importance saved for {len(seg_importances)} segments\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60)\n",
    "    print(\"GRANULAR SEGMENTED AVM\")\n",
    "    print(\"=\"*60)\n",
    "    df, y_col, id_col, state_col = load_data(INPUT_PATH)\n",
    "    df, feats = prepare_data(df, y_col, id_col, state_col)\n",
    "    results = train_segments(df, feats, y_col, id_col, state_col)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR)\n",
    "    preds = results['predictions']\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ COMPLETE in {time.time()-t0:.1f}s\")\n",
    "    print(f\"  {len(preds):,} properties | {preds['segment'].nunique()} segments\")\n",
    "    print(f\"  R²: {r2:.4f} | MAE: ${mae:,.0f} | MAPE: {mape:.2f}%\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "if __name__ == \"__main__\": main()"
   ],
   "id": "9d5bd07f6df39c60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRANULAR SEGMENTED AVM\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 129.6MB | Price:sale_price ID:property_id\n",
      "\n",
      "Preparing data...\n",
      "127,258 records after price filter\n",
      "51/60 features available\n",
      "\n",
      "Training segmented models on 127,258 properties\n",
      "27 segments created\n",
      "  large_family_mature: 19,246\n",
      "  medium_family_older: 17,021\n",
      "  large_family_older: 12,118\n",
      "  small_standard_older: 11,606\n",
      "  medium_family_mature: 10,980\n",
      "  small_family_older: 7,384\n",
      "  small_compact_mature: 6,931\n",
      "Consolidated to 7 segments maximum\n",
      "  other: 48,903→38,254 (21.8% filtered)\n",
      "  other: 11,476 test | MAE:$620,127 | MAPE:26.91% | R²:0.212\n",
      "  large_family_older: 12,118→8,747 (27.8% filtered)\n",
      "  large_family_older: 2,624 test | MAE:$393,518 | MAPE:24.01% | R²:0.445\n",
      "  small_family_older: 7,384→5,453 (26.2% filtered)\n",
      "  small_family_older: 1,636 test | MAE:$471,420 | MAPE:21.50% | R²:0.104\n",
      "  small_standard_older: 11,606→8,712 (24.9% filtered)\n",
      "  small_standard_older: 2,614 test | MAE:$608,835 | MAPE:26.99% | R²:0.155\n",
      "  medium_family_mature: 10,980→8,740 (20.4% filtered)\n",
      "  medium_family_mature: 2,622 test | MAE:$234,602 | MAPE:17.07% | R²:0.215\n",
      "  large_family_mature: 19,246→15,317 (20.4% filtered)\n",
      "  large_family_mature: 4,595 test | MAE:$319,547 | MAPE:20.50% | R²:0.489\n",
      "  medium_family_older: 17,021→12,679 (25.5% filtered)\n",
      "  medium_family_older: 3,804 test | MAE:$212,064 | MAPE:16.49% | R²:0.300\n",
      "\n",
      "Saving results...\n",
      "✓ Excel: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/granular_20251223_131919.xlsx\n",
      "✓ CSVs saved with timestamp: 20251223_131919\n",
      "✓ Per-segment feature importance saved for 7 segments\n",
      "\n",
      "============================================================\n",
      "✓ COMPLETE in 8.4s\n",
      "  29,371 properties | 7 segments\n",
      "  R²: 0.2511 | MAE: $456,302 | MAPE: 23.13%\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T18:34:14.231819Z",
     "start_time": "2025-12-23T18:34:06.273469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import time, os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Config - FIXED PARAMETERS\n",
    "INCLUDE_MLS, INCLUDE_CENSUS, INCLUDE_NEIGHBORHOOD, INCLUDE_IMAGE = True, True, True, False\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS = 20000, 0.3, 42, -1, 8\n",
    "N_EST, QUANTILES, MAX_SEGMENTS = 100, [0.1, 0.5, 0.9], 7  # MAX 7 SEGMENTS\n",
    "\n",
    "INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "BASE_FEATS = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"effective_year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\"fireplace_code\",\"latitude\",\"longitude\",\"geo_cluster\"]\n",
    "ENG_FEATS = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\"]\n",
    "PRIOR_FEATS = [\"prior_sale_price\",\"prior_price_per_sqft\",\"sqft_per_prior_dollar\",\"years_since_last_sale\",\"expected_appreciation\",\"has_prior_sale\",\"recently_sold\"]\n",
    "CLUSTER_FEATS = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "CENSUS_FEATS = [\"total_population_25plus\",\"male_bachelors_degree\",\"female_bachelors_degree\",\"pct_bachelors_degree\",\"total_population\",\"median_earnings_total\",\"median_earnings_male\",\"median_earnings_female\",\"median_household_income\",\"median_home_value\",\"median_gross_rent\",\"owner_occupied_units\",\"renter_occupied_units\",\"pct_owner_occupied\",\"occupied_units\",\"vacant_units\",\"median_age\",\"civilian_employed\",\"civilian_unemployed\",\"unemployment_rate\",\"income_education_score\"]\n",
    "ELECT_FEATS = [\"votes_gop\",\"votes_dem\",\"total_votes\",\"per_gop\",\"per_dem\",\"per_point_diff\",\"dem_margin\",\"rep_margin\"]\n",
    "IMG_FEATS = [\"topic_1\",\"topic_2\",\"topic_3\",\"topic_4\",\"topic_5\",\"topic_6\",\"topic_7\",\"topic_8\",\"topic_9\",\"topic_10\",\"gran_c_in\",\"gran_c_ex\",\"gran_c\",\"high_c_in\",\"high_c_ex\",\"high_c\"]\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8') if set(df[c].dropna().unique()).issubset({0,1}) else df[c].astype('int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return optimize_dtypes(df), y_col, id_col, state_col\n",
    "\n",
    "def assign_segments_simple(df):\n",
    "    \"\"\"Segment based on PROPERTY CHARACTERISTICS only - NO PRICE\"\"\"\n",
    "\n",
    "    # Primary: SIZE (living sqft)\n",
    "    if 'living_sqft' in df.columns:\n",
    "        sqft = df['living_sqft'].fillna(df['living_sqft'].median())\n",
    "        q33, q67 = sqft.quantile([.33, .67])\n",
    "        size_tier = pd.Series(['medium'] * len(df), index=df.index)\n",
    "        size_tier[sqft < q33] = 'small'\n",
    "        size_tier[sqft > q67] = 'large'\n",
    "    else:\n",
    "        size_tier = pd.Series(['medium'] * len(df), index=df.index)\n",
    "\n",
    "    # Secondary: BEDROOMS (property complexity)\n",
    "    if 'bedrooms' in df.columns:\n",
    "        beds = df['bedrooms'].fillna(df['bedrooms'].median())\n",
    "        beds_median = beds.median()\n",
    "        room_tier = pd.Series(['standard'] * len(df), index=df.index)\n",
    "        room_tier[beds <= 2] = 'compact'\n",
    "        room_tier[beds >= 4] = 'family'\n",
    "    else:\n",
    "        room_tier = pd.Series(['standard'] * len(df), index=df.index)\n",
    "\n",
    "    # Tertiary (optional): AGE\n",
    "    if 'year_built' in df.columns:\n",
    "        age = 2024 - df['year_built'].fillna(1980)\n",
    "        age_tier = pd.Series(['mature'] * len(df), index=df.index)\n",
    "        age_tier[age <= 10] = 'new'\n",
    "        age_tier[age > 40] = 'older'\n",
    "    else:\n",
    "        age_tier = pd.Series(['mature'] * len(df), index=df.index)\n",
    "\n",
    "    # Combine into segments\n",
    "    # Format: size_rooms_age (e.g., \"small_compact_new\", \"large_family_mature\")\n",
    "    segments = size_tier + '_' + room_tier + '_' + age_tier\n",
    "\n",
    "    return segments\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    if 'living_sqft' in df.columns and 'bedrooms' in df.columns: df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns: df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns: df['lot_acres'] = df['lot_sqft']/43560\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'] = 2024-df['year_built']\n",
    "        df['is_new'] = (df['property_age']<=5).astype('int8')\n",
    "        df['age_squared'] = df['property_age']**2\n",
    "    if 'garage_spaces' in df.columns: df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "    if 'living_sqft' in df.columns: df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "    if INCLUDE_CENSUS and all(c in df.columns for c in ['median_household_income','pct_bachelors_degree']):\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "    if 'prior_sale_price' in df.columns and 'living_sqft' in df.columns:\n",
    "        df['prior_price_per_sqft'] = df['prior_sale_price']/(df['living_sqft']+1)\n",
    "        df['sqft_per_prior_dollar'] = df['living_sqft']/(df['prior_sale_price']+1)\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        if 'prior_sale_price' in df.columns:\n",
    "            df['expected_appreciation'] = df['prior_sale_price']*(1.04)**df['years_since_last_sale']\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df.loc[missing,'median_home_value'] if 'median_home_value' in df.columns and INCLUDE_CENSUS else df['prior_sale_price'].median()\n",
    "    if 'years_since_last_sale' in df.columns: df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "    if with_price and y_col in df.columns and 'living_sqft' in df.columns:\n",
    "        df['price_per_sqft'] = df[y_col]/(df['living_sqft']+1)\n",
    "        df['sqft_per_dollar'] = df['living_sqft']/(df[y_col]+1)\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df):\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0; return df\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum()<N_CLUSTERS: df['geo_cluster'] = 0; return df\n",
    "    df['geo_cluster'] = 0\n",
    "    kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "    df.loc[valid,'geo_cluster'] = kmeans.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    return df\n",
    "\n",
    "def add_cluster_feats(train, test, y_col):\n",
    "    if 'geo_cluster' not in train.columns or y_col not in train.columns:\n",
    "        for d in [train,test]: d['cluster_avg_price'] = d['cluster_med_price'] = train[y_col].median() if y_col in train.columns else 0\n",
    "        return train, test\n",
    "    stats = train.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "    stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "    train = train.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    test = test.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    return train, test\n",
    "\n",
    "def filter_outliers(df, name, y_col):\n",
    "    orig = len(df)\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)]\n",
    "    if 'sqft_per_dollar' in df.columns: df = df[df['sqft_per_dollar']<=df['sqft_per_dollar'].quantile(.95)]\n",
    "    if 'price_per_sqft' in df.columns: df = df.drop(columns=['price_per_sqft','sqft_per_dollar'])\n",
    "    if 'lot_sqft' in df.columns: df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "    if 'year_built' in df.columns: df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "    if len(df)>=100:\n",
    "        try:\n",
    "            feats = [c for c in ['living_sqft','lot_sqft',y_col,'year_built'] if c in df.columns]\n",
    "            if len(feats)>=3:\n",
    "                X = df[feats].fillna(df[feats].median())\n",
    "                df = df[IsolationForest(contamination=.05,random_state=RAND_STATE,n_jobs=N_JOBS).fit_predict(X)==1]\n",
    "        except: pass\n",
    "    pct_filt = (orig-len(df))/orig*100 if orig>0 else 0\n",
    "    if pct_filt > 0: print(f\"  {name}: {orig:,}→{len(df):,} ({pct_filt:.1f}% filtered)\")\n",
    "    return df\n",
    "\n",
    "def train_model(X, y, q):\n",
    "    return XGBRegressor(\n",
    "        objective='reg:quantileerror',\n",
    "        quantile_alpha=q,\n",
    "        n_estimators=N_EST,\n",
    "        learning_rate=.1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        subsample=.8,\n",
    "        colsample_bytree=.8,\n",
    "        random_state=RAND_STATE,\n",
    "        n_jobs=N_JOBS,\n",
    "        tree_method='hist'\n",
    "    ).fit(X, y, verbose=False)\n",
    "\n",
    "def get_feature_importance_per_segment(model, feat_names, top_n=20):\n",
    "    \"\"\"Extract feature importance for a single segment model\"\"\"\n",
    "    scores = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    importance_list = []\n",
    "    for k, v in scores.items():\n",
    "        idx = int(k[1:])\n",
    "        if idx < len(feat_names):\n",
    "            importance_list.append((feat_names[idx], v))\n",
    "\n",
    "    # Sort and normalize\n",
    "    importance_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    total_gain = sum(v for _, v in importance_list)\n",
    "\n",
    "    if total_gain > 0:\n",
    "        importance_df = pd.DataFrame([\n",
    "            {'feature': feat, 'gain': gain, 'importance': gain/total_gain}\n",
    "            for feat, gain in importance_list[:top_n]\n",
    "        ])\n",
    "    else:\n",
    "        importance_df = pd.DataFrame(columns=['feature', 'gain', 'importance'])\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "def feature_importance(models, feat_names, metrics):\n",
    "    \"\"\"Global feature importance weighted by segment size - NOW INCLUDES GAIN\"\"\"\n",
    "    rows = []\n",
    "    for seg, mdl in models.items():\n",
    "        scores = mdl['q50'].get_booster().get_score(importance_type=\"gain\")\n",
    "        w = metrics[seg][\"n_test\"]\n",
    "        for k,v in scores.items():\n",
    "            idx = int(k[1:])\n",
    "            if idx<len(feat_names): rows.append((feat_names[idx],v,w))\n",
    "    if not rows: return pd.DataFrame(columns=[\"feature\",\"gain\",\"importance\"])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"feature\",\"gain\",\"weight\"])\n",
    "\n",
    "    # Calculate total gain per feature (summing across segments)\n",
    "    total_gain_per_feature = df.groupby(\"feature\")[\"gain\"].sum().reset_index()\n",
    "    total_gain_per_feature.columns = [\"feature\", \"total_gain\"]\n",
    "\n",
    "    # Calculate weighted gain and importance\n",
    "    out = df.assign(wg=df[\"gain\"]*df[\"weight\"]).groupby(\"feature\",as_index=False).agg(\n",
    "        total_gain=(\"gain\", \"sum\"),\n",
    "        weighted_gain=(\"wg\",\"sum\")\n",
    "    ).sort_values(\"weighted_gain\",ascending=False)\n",
    "\n",
    "    out[\"importance\"] = out[\"weighted_gain\"]/out[\"weighted_gain\"].sum()\n",
    "\n",
    "    return out[[\"feature\",\"total_gain\",\"importance\"]].head(100)\n",
    "\n",
    "def prepare_data(df, y_col, id_col, state_col):\n",
    "    print(f\"\\nPreparing data...\")\n",
    "    df = df[df[y_col]>=MIN_PRICE]\n",
    "    print(f\"{len(df):,} records after price filter\")\n",
    "    df = engineer(geo_cluster(df), y_col)\n",
    "    feat_groups = []\n",
    "    if INCLUDE_MLS: feat_groups.extend([BASE_FEATS,ENG_FEATS,PRIOR_FEATS,CLUSTER_FEATS])\n",
    "    if INCLUDE_CENSUS: feat_groups.append(CENSUS_FEATS)\n",
    "    if INCLUDE_NEIGHBORHOOD: feat_groups.append(ELECT_FEATS)\n",
    "    if INCLUDE_IMAGE: feat_groups.append(IMG_FEATS)\n",
    "    all_feats = [f for g in feat_groups for f in g]\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "    return df.dropna(subset=[y_col]), feats\n",
    "\n",
    "def train_segments(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining segmented models on {len(df):,} properties\")\n",
    "\n",
    "    # Segment based on property characteristics (NO PRICE)\n",
    "    df['seg'] = assign_segments_simple(df)\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    print(f\"{len(seg_cnts)} segments created\")\n",
    "    for seg, cnt in seg_cnts.head(MAX_SEGMENTS).items(): print(f\"  {seg}: {cnt:,}\")\n",
    "\n",
    "    # Merge small segments\n",
    "    small = seg_cnts[seg_cnts<50].index\n",
    "    if len(small)>0:\n",
    "        df.loc[df['seg'].isin(small),'seg'] = 'other'\n",
    "        print(f\"Merged {len(small)} small segments into 'other'\")\n",
    "\n",
    "    # If still too many segments, merge smallest ones\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    if len(seg_cnts) > MAX_SEGMENTS:\n",
    "        keep_segs = seg_cnts.head(MAX_SEGMENTS-1).index\n",
    "        df.loc[~df['seg'].isin(keep_segs),'seg'] = 'other'\n",
    "        print(f\"Consolidated to {MAX_SEGMENTS} segments maximum\")\n",
    "\n",
    "    models, metrics, preds_list, segment_importances = {}, {}, [], {}\n",
    "\n",
    "    for seg in df['seg'].unique():\n",
    "        seg_df = df[df['seg']==seg].copy()\n",
    "        if len(seg_df)<50: continue\n",
    "        seg_df = filter_outliers(seg_df, seg, y_col)\n",
    "        if len(seg_df)<50: continue\n",
    "        train_idx = seg_df.sample(frac=1-TEST_SIZE,random_state=RAND_STATE).index\n",
    "        train_df, test_df = seg_df.loc[train_idx].copy(), seg_df.loc[seg_df.index.difference(train_idx)].copy()\n",
    "        train_df, test_df = add_cluster_feats(train_df, test_df, y_col)\n",
    "        X_tr,y_tr = train_df[feats].values, train_df[y_col].values\n",
    "        X_te,y_te = test_df[feats].values, test_df[y_col].values\n",
    "        ids_te = test_df[id_col].values\n",
    "        states_te = test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "        seg_models, seg_preds = {}, []\n",
    "        for q in QUANTILES:\n",
    "            m = train_model(X_tr,y_tr,q)\n",
    "            seg_models[f\"q{int(q*100)}\"] = m\n",
    "            seg_preds.append(m.predict(X_te))\n",
    "        models[seg] = seg_models\n",
    "\n",
    "        # Get feature importance for this segment\n",
    "        segment_importances[seg] = get_feature_importance_per_segment(seg_models['q50'], feats, top_n=20)\n",
    "\n",
    "        y_pred = seg_preds[1]\n",
    "        mae,mape = mean_absolute_error(y_te,y_pred), np.mean(np.abs((y_te-y_pred)/y_te))*100\n",
    "        r2,cov = r2_score(y_te,y_pred), np.mean((y_te>=seg_preds[0])&(y_te<=seg_preds[2]))*100\n",
    "        metrics[seg] = {'n_train':len(X_tr),'n_test':len(X_te),'mae':mae,'mape':mape,'r2':r2,'cov':cov,'p_min':seg_df[y_col].min(),'p_max':seg_df[y_col].max(),'p_med':seg_df[y_col].median()}\n",
    "        print(f\"  {seg}: {len(test_df):,} test | MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.3f}\")\n",
    "        preds_list.append(pd.DataFrame({'property_id':ids_te,'state':states_te,'actual':y_te,'predicted':y_pred,'pred_lower':seg_preds[0],'pred_upper':seg_preds[2],'segment':seg}))\n",
    "\n",
    "    return {\n",
    "        'models': models,\n",
    "        'metrics': metrics,\n",
    "        'predictions': pd.concat(preds_list),\n",
    "        'feature_importance': feature_importance(models, feats, metrics),\n",
    "        'segment_importances': segment_importances,\n",
    "        'feature_names': feats\n",
    "    }\n",
    "\n",
    "def save_results(results, out_dir):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi = results['predictions'], results['metrics'], results['feature_importance']\n",
    "    seg_importances = results['segment_importances']\n",
    "\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    # Summary\n",
    "    ws = wb.create_sheet(\"Summary\",0)\n",
    "    ws['A1'], ws['A1'].font = 'GRANULAR SEGMENTED AVM', Font(bold=True,size=14)\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    data = [['Metric','Value'],['Properties',len(preds)],['Segments',len(metrics)],['R²',f'{r2:.4f}'],['MAE',f'${mae:,.0f}'],['MAPE%',f'{mape:.2f}%']]\n",
    "    for i,(k,v) in enumerate(data,5): ws[f'A{i}'],ws[f'A{i}'].font,ws[f'B{i}'] = k,Font(bold=True),v\n",
    "\n",
    "    # Segments\n",
    "    ws = wb.create_sheet(\"Segments\")\n",
    "    seg_df = pd.DataFrame([{**{'Segment':s},**m} for s,m in metrics.items()])\n",
    "    for i,h in enumerate(seg_df.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(seg_df.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    # Global Feature Importance\n",
    "    ws = wb.create_sheet(\"Global_Feature_Importance\")\n",
    "    ws['A1'] = 'Global Feature Importance (Weighted)'\n",
    "    ws['A1'].font = Font(bold=True, size=12)\n",
    "    for r_idx, row in enumerate(dataframe_to_rows(fi, index=False, header=True), 2):\n",
    "        for c_idx, value in enumerate(row, 1):\n",
    "            cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "            if r_idx == 2:  # Header row\n",
    "                cell.font = Font(bold=True, color='FFFFFF')\n",
    "                cell.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "\n",
    "    # Per-Segment Feature Importance (one sheet per segment)\n",
    "    for seg_name, seg_fi in seg_importances.items():\n",
    "        sheet_name = f\"FI_{seg_name}\"[:31]  # Excel sheet name limit\n",
    "        ws = wb.create_sheet(sheet_name)\n",
    "        ws['A1'] = f'Feature Importance: {seg_name}'\n",
    "        ws['A1'].font = Font(bold=True, size=12)\n",
    "\n",
    "        for r_idx, row in enumerate(dataframe_to_rows(seg_fi, index=False, header=True), 2):\n",
    "            for c_idx, value in enumerate(row, 1):\n",
    "                cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "                if r_idx == 2:  # Header row\n",
    "                    cell.font = Font(bold=True, color='FFFFFF')\n",
    "                    cell.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "\n",
    "    # Predictions\n",
    "    ws = wb.create_sheet(\"Predictions\")\n",
    "    for i,h in enumerate(preds.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(preds.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/granular_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "    preds.to_csv(f\"{out_dir}/predictions_{ts}.csv\",index=False)\n",
    "    seg_df.to_csv(f\"{out_dir}/segments_{ts}.csv\",index=False)\n",
    "    fi.to_csv(f\"{out_dir}/global_importance_{ts}.csv\",index=False)\n",
    "\n",
    "    # Save per-segment feature importance CSVs\n",
    "    for seg_name, seg_fi in seg_importances.items():\n",
    "        seg_fi.to_csv(f\"{out_dir}/importance_{seg_name}_{ts}.csv\", index=False)\n",
    "\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved with timestamp: {ts}\")\n",
    "    print(f\"✓ Per-segment feature importance saved for {len(seg_importances)} segments\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60)\n",
    "    print(\"GRANULAR SEGMENTED AVM\")\n",
    "    print(\"=\"*60)\n",
    "    df, y_col, id_col, state_col = load_data(INPUT_PATH)\n",
    "    df, feats = prepare_data(df, y_col, id_col, state_col)\n",
    "    results = train_segments(df, feats, y_col, id_col, state_col)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR)\n",
    "    preds = results['predictions']\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ COMPLETE in {time.time()-t0:.1f}s\")\n",
    "    print(f\"  {len(preds):,} properties | {preds['segment'].nunique()} segments\")\n",
    "    print(f\"  R²: {r2:.4f} | MAE: ${mae:,.0f} | MAPE: {mape:.2f}%\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "if __name__ == \"__main__\": main()"
   ],
   "id": "d02bb9acda51699",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRANULAR SEGMENTED AVM\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 129.6MB | Price:sale_price ID:property_id\n",
      "\n",
      "Preparing data...\n",
      "127,258 records after price filter\n",
      "51/60 features available\n",
      "\n",
      "Training segmented models on 127,258 properties\n",
      "27 segments created\n",
      "  large_family_mature: 19,246\n",
      "  medium_family_older: 17,021\n",
      "  large_family_older: 12,118\n",
      "  small_standard_older: 11,606\n",
      "  medium_family_mature: 10,980\n",
      "  small_family_older: 7,384\n",
      "  small_compact_mature: 6,931\n",
      "Consolidated to 7 segments maximum\n",
      "  other: 48,903→38,254 (21.8% filtered)\n",
      "  other: 11,476 test | MAE:$620,127 | MAPE:26.91% | R²:0.212\n",
      "  large_family_older: 12,118→8,747 (27.8% filtered)\n",
      "  large_family_older: 2,624 test | MAE:$393,518 | MAPE:24.01% | R²:0.445\n",
      "  small_family_older: 7,384→5,453 (26.2% filtered)\n",
      "  small_family_older: 1,636 test | MAE:$471,420 | MAPE:21.50% | R²:0.104\n",
      "  small_standard_older: 11,606→8,712 (24.9% filtered)\n",
      "  small_standard_older: 2,614 test | MAE:$608,835 | MAPE:26.99% | R²:0.155\n",
      "  medium_family_mature: 10,980→8,740 (20.4% filtered)\n",
      "  medium_family_mature: 2,622 test | MAE:$234,602 | MAPE:17.07% | R²:0.215\n",
      "  large_family_mature: 19,246→15,317 (20.4% filtered)\n",
      "  large_family_mature: 4,595 test | MAE:$319,547 | MAPE:20.50% | R²:0.489\n",
      "  medium_family_older: 17,021→12,679 (25.5% filtered)\n",
      "  medium_family_older: 3,804 test | MAE:$212,064 | MAPE:16.49% | R²:0.300\n",
      "\n",
      "Saving results...\n",
      "✓ Excel: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/granular_20251223_133413.xlsx\n",
      "✓ CSVs saved with timestamp: 20251223_133413\n",
      "✓ Per-segment feature importance saved for 7 segments\n",
      "\n",
      "============================================================\n",
      "✓ COMPLETE in 7.9s\n",
      "  29,371 properties | 7 segments\n",
      "  R²: 0.2511 | MAE: $456,302 | MAPE: 23.13%\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T17:56:49.482061Z",
     "start_time": "2025-12-23T17:56:47.163283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "import time, os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Config - FIXED PARAMETERS\n",
    "INCLUDE_MLS, INCLUDE_CENSUS, INCLUDE_NEIGHBORHOOD, INCLUDE_IMAGE = True, True, True, False\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS = 20000, 0.3, 42, -1, 8\n",
    "N_EST, QUANTILES = 100, [0.1, 0.5, 0.9]  # REDUCED from 500 to 100\n",
    "\n",
    "INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "BASE_FEATS = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"effective_year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\"fireplace_code\",\"latitude\",\"longitude\",\"geo_cluster\"]\n",
    "ENG_FEATS = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\"]\n",
    "PRIOR_FEATS = [\"prior_sale_price\",\"prior_price_per_sqft\",\"sqft_per_prior_dollar\",\"years_since_last_sale\",\"expected_appreciation\",\"has_prior_sale\",\"recently_sold\"]\n",
    "CLUSTER_FEATS = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "CENSUS_FEATS = [\"total_population_25plus\",\"male_bachelors_degree\",\"female_bachelors_degree\",\"pct_bachelors_degree\",\"total_population\",\"median_earnings_total\",\"median_earnings_male\",\"median_earnings_female\",\"median_household_income\",\"median_home_value\",\"median_gross_rent\",\"owner_occupied_units\",\"renter_occupied_units\",\"pct_owner_occupied\",\"occupied_units\",\"vacant_units\",\"median_age\",\"civilian_employed\",\"civilian_unemployed\",\"unemployment_rate\",\"income_education_score\"]\n",
    "ELECT_FEATS = [\"votes_gop\",\"votes_dem\",\"total_votes\",\"per_gop\",\"per_dem\",\"per_point_diff\",\"dem_margin\",\"rep_margin\"]\n",
    "IMG_FEATS = [\"topic_1\",\"topic_2\",\"topic_3\",\"topic_4\",\"topic_5\",\"topic_6\",\"topic_7\",\"topic_8\",\"topic_9\",\"topic_10\",\"gran_c_in\",\"gran_c_ex\",\"gran_c\",\"high_c_in\",\"high_c_ex\",\"high_c\"]\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8') if set(df[c].dropna().unique()).issubset({0,1}) else df[c].astype('int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return optimize_dtypes(df), y_col, id_col, state_col\n",
    "\n",
    "def assign_segment(row, df_q):\n",
    "    \"\"\"Robust segmentation with tighter thresholds to create more segments\"\"\"\n",
    "    # Size tier - use tighter quantiles\n",
    "    size = 'medium'\n",
    "    if 'living_sqft' in df_q.columns and 'living_sqft' in row.index:\n",
    "        sqft = row.get('living_sqft',0)\n",
    "        if sqft > 0:\n",
    "            q20,q40,q60,q80 = df_q['living_sqft'].quantile([.2,.4,.6,.8])\n",
    "            if sqft < q20: size = 'tiny'\n",
    "            elif sqft < q40: size = 'small'\n",
    "            elif sqft < q60: size = 'medium'\n",
    "            elif sqft < q80: size = 'large'\n",
    "            else: size = 'xlarge'\n",
    "\n",
    "    # Age tier - more granular\n",
    "    age_class = 'mature'\n",
    "    if 'property_age' in row.index:\n",
    "        age = row.get('property_age',50)\n",
    "        if age <= 3: age_class = 'new'\n",
    "        elif age <= 10: age_class = 'recent'\n",
    "        elif age <= 25: age_class = 'mature'\n",
    "        elif age <= 45: age_class = 'aging'\n",
    "        else: age_class = 'old'\n",
    "    elif 'year_built' in row.index:\n",
    "        yb = row.get('year_built',1980)\n",
    "        age = 2024 - yb\n",
    "        if age <= 3: age_class = 'new'\n",
    "        elif age <= 10: age_class = 'recent'\n",
    "        elif age <= 25: age_class = 'mature'\n",
    "        elif age <= 45: age_class = 'aging'\n",
    "        else: age_class = 'old'\n",
    "\n",
    "    # Location quality - use tighter quantiles\n",
    "    loc = 'mid'\n",
    "    if INCLUDE_CENSUS and 'median_household_income' in df_q.columns and 'median_household_income' in row.index:\n",
    "        inc = row.get('median_household_income',0)\n",
    "        iq25,iq75 = df_q['median_household_income'].quantile([.25,.75])\n",
    "        if inc > iq75: loc = 'prime'\n",
    "        elif inc < iq25: loc = 'basic'\n",
    "        else: loc = 'mid'\n",
    "\n",
    "    # Luxury tier - use tighter quantiles\n",
    "    lux_tier = 'mid'\n",
    "    if 'luxury_score' in df_q.columns and 'luxury_score' in row.index:\n",
    "        lux = row.get('luxury_score',0)\n",
    "        lq25,lq75 = df_q['luxury_score'].quantile([.25,.75])\n",
    "        if lux > lq75: lux_tier = 'upscale'\n",
    "        elif lux < lq25: lux_tier = 'standard'\n",
    "        else: lux_tier = 'mid'\n",
    "\n",
    "    return f\"{size}_{age_class}_{loc}_{lux_tier}\"\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    if 'living_sqft' in df.columns and 'bedrooms' in df.columns: df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns: df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns: df['lot_acres'] = df['lot_sqft']/43560\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'] = 2024-df['year_built']\n",
    "        df['is_new'] = (df['property_age']<=5).astype('int8')\n",
    "        df['age_squared'] = df['property_age']**2\n",
    "    if 'garage_spaces' in df.columns: df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "    if 'living_sqft' in df.columns: df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "    if INCLUDE_CENSUS and all(c in df.columns for c in ['median_household_income','pct_bachelors_degree']):\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "    if 'prior_sale_price' in df.columns and 'living_sqft' in df.columns:\n",
    "        df['prior_price_per_sqft'] = df['prior_sale_price']/(df['living_sqft']+1)\n",
    "        df['sqft_per_prior_dollar'] = df['living_sqft']/(df['prior_sale_price']+1)\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        if 'prior_sale_price' in df.columns:\n",
    "            df['expected_appreciation'] = df['prior_sale_price']*(1.04)**df['years_since_last_sale']\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df.loc[missing,'median_home_value'] if 'median_home_value' in df.columns and INCLUDE_CENSUS else df['prior_sale_price'].median()\n",
    "    if 'years_since_last_sale' in df.columns: df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "    if with_price and y_col in df.columns and 'living_sqft' in df.columns:\n",
    "        df['price_per_sqft'] = df[y_col]/(df['living_sqft']+1)\n",
    "        df['sqft_per_dollar'] = df['living_sqft']/(df[y_col]+1)\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df):\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0; return df\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum()<N_CLUSTERS: df['geo_cluster'] = 0; return df\n",
    "    df['geo_cluster'] = 0\n",
    "    kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "    df.loc[valid,'geo_cluster'] = kmeans.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    return df\n",
    "\n",
    "def add_cluster_feats(train, test, y_col):\n",
    "    if 'geo_cluster' not in train.columns or y_col not in train.columns:\n",
    "        for d in [train,test]: d['cluster_avg_price'] = d['cluster_med_price'] = train[y_col].median() if y_col in train.columns else 0\n",
    "        return train, test\n",
    "    stats = train.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "    stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "    train = train.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    test = test.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    return train, test\n",
    "\n",
    "def filter_outliers(df, name, y_col):\n",
    "    orig = len(df)\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)]\n",
    "    if 'sqft_per_dollar' in df.columns: df = df[df['sqft_per_dollar']<=df['sqft_per_dollar'].quantile(.95)]\n",
    "    if 'price_per_sqft' in df.columns: df = df.drop(columns=['price_per_sqft','sqft_per_dollar'])\n",
    "    if 'lot_sqft' in df.columns: df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "    if 'year_built' in df.columns: df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "    if len(df)>=100:\n",
    "        try:\n",
    "            feats = [c for c in ['living_sqft','lot_sqft',y_col,'year_built'] if c in df.columns]\n",
    "            if len(feats)>=3:\n",
    "                X = df[feats].fillna(df[feats].median())\n",
    "                df = df[IsolationForest(contamination=.05,random_state=RAND_STATE,n_jobs=N_JOBS).fit_predict(X)==1]\n",
    "        except: pass\n",
    "    pct_filt = (orig-len(df))/orig*100 if orig>0 else 0\n",
    "    if pct_filt > 0: print(f\"  {name}: {orig:,}→{len(df):,} ({pct_filt:.1f}% filtered)\")\n",
    "    return df\n",
    "\n",
    "def train_model(X, y, q):\n",
    "    # FIXED: Optimized parameters for speed\n",
    "    return XGBRegressor(\n",
    "        objective='reg:quantileerror',\n",
    "        quantile_alpha=q,\n",
    "        n_estimators=N_EST,  # Now 100 instead of 500\n",
    "        learning_rate=.1,     # INCREASED from .05 to .1 for faster convergence\n",
    "        max_depth=5,          # REDUCED from 6 to 5\n",
    "        min_child_weight=3,\n",
    "        subsample=.8,\n",
    "        colsample_bytree=.8,\n",
    "        random_state=RAND_STATE,\n",
    "        n_jobs=N_JOBS,\n",
    "        tree_method='hist'\n",
    "    ).fit(X, y, verbose=False)\n",
    "\n",
    "def feature_importance(models, feat_names, metrics):\n",
    "    rows = []\n",
    "    for seg, mdl in models.items():\n",
    "        scores = mdl['q50'].get_booster().get_score(importance_type=\"gain\")\n",
    "        w = metrics[seg][\"n_test\"]\n",
    "        for k,v in scores.items():\n",
    "            idx = int(k[1:])\n",
    "            if idx<len(feat_names): rows.append((feat_names[idx],v,w))\n",
    "    if not rows: return pd.DataFrame(columns=[\"feature\",\"importance\"])\n",
    "    df = pd.DataFrame(rows, columns=[\"feature\",\"gain\",\"weight\"])\n",
    "    out = df.assign(wg=df[\"gain\"]*df[\"weight\"]).groupby(\"feature\",as_index=False).agg(tg=(\"wg\",\"sum\")).sort_values(\"tg\",ascending=False)\n",
    "    out[\"importance\"] = out[\"tg\"]/out[\"tg\"].sum()\n",
    "    return out[[\"feature\",\"importance\"]].head(100)\n",
    "\n",
    "def prepare_data(df, y_col, id_col, state_col):\n",
    "    print(f\"\\nPreparing data...\")\n",
    "    df = df[df[y_col]>=MIN_PRICE]\n",
    "    print(f\"{len(df):,} records after price filter\")\n",
    "    df = engineer(geo_cluster(df), y_col)\n",
    "    feat_groups = []\n",
    "    if INCLUDE_MLS: feat_groups.extend([BASE_FEATS,ENG_FEATS,PRIOR_FEATS,CLUSTER_FEATS])\n",
    "    if INCLUDE_CENSUS: feat_groups.append(CENSUS_FEATS)\n",
    "    if INCLUDE_NEIGHBORHOOD: feat_groups.append(ELECT_FEATS)\n",
    "    if INCLUDE_IMAGE: feat_groups.append(IMG_FEATS)\n",
    "    all_feats = [f for g in feat_groups for f in g]\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "    return df.dropna(subset=[y_col]), feats\n",
    "\n",
    "def train_segments(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining segmented models on {len(df):,} properties\")\n",
    "    df['seg'] = df.apply(lambda r: assign_segment(r,df), axis=1)\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    print(f\"{len(seg_cnts)} segments created\")\n",
    "    for seg, cnt in seg_cnts.head(10).items(): print(f\"  {seg}: {cnt:,}\")\n",
    "    small = seg_cnts[seg_cnts<50].index\n",
    "    if len(small)>0:\n",
    "        df.loc[df['seg'].isin(small),'seg'] = 'other'\n",
    "        print(f\"Merged {len(small)} small segments into 'other'\")\n",
    "    models, metrics, preds_list = {}, {}, []\n",
    "    for seg in df['seg'].unique():\n",
    "        seg_df = df[df['seg']==seg].copy()\n",
    "        if len(seg_df)<50: continue\n",
    "        seg_df = filter_outliers(seg_df, seg, y_col)\n",
    "        if len(seg_df)<50: continue\n",
    "        train_idx = seg_df.sample(frac=1-TEST_SIZE,random_state=RAND_STATE).index\n",
    "        train_df, test_df = seg_df.loc[train_idx].copy(), seg_df.loc[seg_df.index.difference(train_idx)].copy()\n",
    "        train_df, test_df = add_cluster_feats(train_df, test_df, y_col)\n",
    "        X_tr,y_tr = train_df[feats].values, train_df[y_col].values\n",
    "        X_te,y_te = test_df[feats].values, test_df[y_col].values\n",
    "        ids_te = test_df[id_col].values\n",
    "        states_te = test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "        seg_models, seg_preds = {}, []\n",
    "        for q in QUANTILES:\n",
    "            m = train_model(X_tr,y_tr,q)\n",
    "            seg_models[f\"q{int(q*100)}\"] = m\n",
    "            seg_preds.append(m.predict(X_te))\n",
    "        models[seg] = seg_models\n",
    "        y_pred = seg_preds[1]\n",
    "        mae,mape = mean_absolute_error(y_te,y_pred), np.mean(np.abs((y_te-y_pred)/y_te))*100\n",
    "        r2,cov = r2_score(y_te,y_pred), np.mean((y_te>=seg_preds[0])&(y_te<=seg_preds[2]))*100\n",
    "        metrics[seg] = {'n_train':len(X_tr),'n_test':len(X_te),'mae':mae,'mape':mape,'r2':r2,'cov':cov,'p_min':seg_df[y_col].min(),'p_max':seg_df[y_col].max(),'p_med':seg_df[y_col].median()}\n",
    "        print(f\"  {seg}: {len(test_df):,} test | MAE:${mae:,.0f} R²:{r2:.3f}\")\n",
    "        preds_list.append(pd.DataFrame({'property_id':ids_te,'state':states_te,'actual':y_te,'predicted':y_pred,'pred_lower':seg_preds[0],'pred_upper':seg_preds[2],'segment':seg}))\n",
    "    return {'models':models,'metrics':metrics,'predictions':pd.concat(preds_list),'feature_importance':feature_importance(models,feats,metrics),'feature_names':feats}\n",
    "\n",
    "def save_results(results, out_dir):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi = results['predictions'], results['metrics'], results['feature_importance']\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    # Summary\n",
    "    ws = wb.create_sheet(\"Summary\",0)\n",
    "    ws['A1'], ws['A1'].font = 'GRANULAR SEGMENTED AVM', Font(bold=True,size=14)\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    data = [['Metric','Value'],['Properties',len(preds)],['Segments',len(metrics)],['R²',f'{r2:.4f}'],['MAE',f'${mae:,.0f}'],['MAPE%',f'{mape:.2f}']]\n",
    "    for i,(k,v) in enumerate(data,5): ws[f'A{i}'],ws[f'A{i}'].font,ws[f'B{i}'] = k,Font(bold=True),v\n",
    "\n",
    "    # Segments\n",
    "    ws = wb.create_sheet(\"Segments\")\n",
    "    seg_df = pd.DataFrame([{**{'Segment':s},**m} for s,m in metrics.items()])\n",
    "    for i,h in enumerate(seg_df.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(seg_df.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    # Predictions\n",
    "    ws = wb.create_sheet(\"Predictions\")\n",
    "    for i,h in enumerate(preds.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(preds.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/granular_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "    preds.to_csv(f\"{out_dir}/predictions_{ts}.csv\",index=False)\n",
    "    seg_df.to_csv(f\"{out_dir}/segments_{ts}.csv\",index=False)\n",
    "    fi.to_csv(f\"{out_dir}/importance_{ts}.csv\",index=False)\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved with timestamp: {ts}\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60)\n",
    "    print(\"GRANULAR SEGMENTED AVM\")\n",
    "    print(\"=\"*60)\n",
    "    df, y_col, id_col, state_col = load_data(INPUT_PATH)\n",
    "    df, feats = prepare_data(df, y_col, id_col, state_col)\n",
    "    results = train_segments(df, feats, y_col, id_col, state_col)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR)\n",
    "    preds = results['predictions']\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ COMPLETE in {time.time()-t0:.1f}s\")\n",
    "    print(f\"  {len(preds):,} properties | {preds['segment'].nunique()} segments\")\n",
    "    print(f\"  R²: {r2:.4f} | MAE: ${mae:,.0f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "if __name__ == \"__main__\": main()"
   ],
   "id": "39081394c41a8108",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRANULAR SEGMENTED AVM\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 129.6MB | Price:sale_price ID:property_id\n",
      "\n",
      "Preparing data...\n",
      "127,258 records after price filter\n",
      "51/60 features available\n",
      "\n",
      "Training segmented models on 127,258 properties\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 320\u001B[39m\n\u001B[32m    317\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m  R²: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mr2\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | MAE: $\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmae\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m,.0f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    318\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m=\u001B[39m\u001B[33m'\u001B[39m*\u001B[32m60\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m320\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m: main()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 308\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    306\u001B[39m df, y_col, id_col, state_col = load_data(INPUT_PATH)\n\u001B[32m    307\u001B[39m df, feats = prepare_data(df, y_col, id_col, state_col)\n\u001B[32m--> \u001B[39m\u001B[32m308\u001B[39m results = \u001B[43mtrain_segments\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeats\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_col\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mid_col\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_col\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    309\u001B[39m os.makedirs(OUTPUT_DIR, exist_ok=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    310\u001B[39m save_results(results, OUTPUT_DIR)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 225\u001B[39m, in \u001B[36mtrain_segments\u001B[39m\u001B[34m(df, feats, y_col, id_col, state_col)\u001B[39m\n\u001B[32m    223\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtrain_segments\u001B[39m(df, feats, y_col, id_col, state_col):\n\u001B[32m    224\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mTraining segmented models on \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(df)\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m properties\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m225\u001B[39m     df[\u001B[33m'\u001B[39m\u001B[33mseg\u001B[39m\u001B[33m'\u001B[39m] = \u001B[43mdf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mr\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43massign_segment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mr\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    226\u001B[39m     seg_cnts = df[\u001B[33m'\u001B[39m\u001B[33mseg\u001B[39m\u001B[33m'\u001B[39m].value_counts()\n\u001B[32m    227\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(seg_cnts)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m segments created\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/House_IQ_Setup/.venv1/lib/python3.13/site-packages/pandas/core/frame.py:10401\u001B[39m, in \u001B[36mDataFrame.apply\u001B[39m\u001B[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001B[39m\n\u001B[32m  10387\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcore\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mapply\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m frame_apply\n\u001B[32m  10389\u001B[39m op = frame_apply(\n\u001B[32m  10390\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m  10391\u001B[39m     func=func,\n\u001B[32m   (...)\u001B[39m\u001B[32m  10399\u001B[39m     kwargs=kwargs,\n\u001B[32m  10400\u001B[39m )\n\u001B[32m> \u001B[39m\u001B[32m10401\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m.__finalize__(\u001B[38;5;28mself\u001B[39m, method=\u001B[33m\"\u001B[39m\u001B[33mapply\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/House_IQ_Setup/.venv1/lib/python3.13/site-packages/pandas/core/apply.py:916\u001B[39m, in \u001B[36mFrameApply.apply\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    913\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.raw:\n\u001B[32m    914\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.apply_raw(engine=\u001B[38;5;28mself\u001B[39m.engine, engine_kwargs=\u001B[38;5;28mself\u001B[39m.engine_kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m916\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/House_IQ_Setup/.venv1/lib/python3.13/site-packages/pandas/core/apply.py:1063\u001B[39m, in \u001B[36mFrameApply.apply_standard\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1061\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mapply_standard\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m   1062\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.engine == \u001B[33m\"\u001B[39m\u001B[33mpython\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m1063\u001B[39m         results, res_index = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapply_series_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1064\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1065\u001B[39m         results, res_index = \u001B[38;5;28mself\u001B[39m.apply_series_numba()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/House_IQ_Setup/.venv1/lib/python3.13/site-packages/pandas/core/apply.py:1081\u001B[39m, in \u001B[36mFrameApply.apply_series_generator\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1078\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m option_context(\u001B[33m\"\u001B[39m\u001B[33mmode.chained_assignment\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m   1079\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m i, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(series_gen):\n\u001B[32m   1080\u001B[39m         \u001B[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1081\u001B[39m         results[i] = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1082\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(results[i], ABCSeries):\n\u001B[32m   1083\u001B[39m             \u001B[38;5;66;03m# If we have a view on v, we need to make a copy because\u001B[39;00m\n\u001B[32m   1084\u001B[39m             \u001B[38;5;66;03m#  series_generator will swap out the underlying data\u001B[39;00m\n\u001B[32m   1085\u001B[39m             results[i] = results[i].copy(deep=\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 225\u001B[39m, in \u001B[36mtrain_segments.<locals>.<lambda>\u001B[39m\u001B[34m(r)\u001B[39m\n\u001B[32m    223\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtrain_segments\u001B[39m(df, feats, y_col, id_col, state_col):\n\u001B[32m    224\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mTraining segmented models on \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(df)\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m properties\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m225\u001B[39m     df[\u001B[33m'\u001B[39m\u001B[33mseg\u001B[39m\u001B[33m'\u001B[39m] = df.apply(\u001B[38;5;28;01mlambda\u001B[39;00m r: \u001B[43massign_segment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mr\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m, axis=\u001B[32m1\u001B[39m)\n\u001B[32m    226\u001B[39m     seg_cnts = df[\u001B[33m'\u001B[39m\u001B[33mseg\u001B[39m\u001B[33m'\u001B[39m].value_counts()\n\u001B[32m    227\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(seg_cnts)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m segments created\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 93\u001B[39m, in \u001B[36massign_segment\u001B[39m\u001B[34m(row, df_q)\u001B[39m\n\u001B[32m     91\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m'\u001B[39m\u001B[33mluxury_score\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m df_q.columns \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m'\u001B[39m\u001B[33mluxury_score\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m row.index:\n\u001B[32m     92\u001B[39m     lux = row.get(\u001B[33m'\u001B[39m\u001B[33mluxury_score\u001B[39m\u001B[33m'\u001B[39m,\u001B[32m0\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m93\u001B[39m     lq25,lq75 = \u001B[43mdf_q\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mluxury_score\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquantile\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m.25\u001B[39;49m\u001B[43m,\u001B[49m\u001B[32;43m.75\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     94\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m lux > lq75: lux_tier = \u001B[33m'\u001B[39m\u001B[33mupscale\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m     95\u001B[39m     \u001B[38;5;28;01melif\u001B[39;00m lux < lq25: lux_tier = \u001B[33m'\u001B[39m\u001B[33mstandard\u001B[39m\u001B[33m'\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/House_IQ_Setup/.venv1/lib/python3.13/site-packages/pandas/core/series.py:2901\u001B[39m, in \u001B[36mSeries.quantile\u001B[39m\u001B[34m(self, q, interpolation)\u001B[39m\n\u001B[32m   2897\u001B[39m \u001B[38;5;66;03m# We dispatch to DataFrame so that core.internals only has to worry\u001B[39;00m\n\u001B[32m   2898\u001B[39m \u001B[38;5;66;03m#  about 2D cases.\u001B[39;00m\n\u001B[32m   2899\u001B[39m df = \u001B[38;5;28mself\u001B[39m.to_frame()\n\u001B[32m-> \u001B[39m\u001B[32m2901\u001B[39m result = \u001B[43mdf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquantile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m=\u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m=\u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumeric_only\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m   2902\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result.ndim == \u001B[32m2\u001B[39m:\n\u001B[32m   2903\u001B[39m     result = result.iloc[:, \u001B[32m0\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/House_IQ_Setup/.venv1/lib/python3.13/site-packages/pandas/core/frame.py:12218\u001B[39m, in \u001B[36mDataFrame.quantile\u001B[39m\u001B[34m(self, q, axis, numeric_only, interpolation, method)\u001B[39m\n\u001B[32m  12214\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m  12215\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mInvalid method: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmethod\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m. Method must be in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvalid_method\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m  12216\u001B[39m     )\n\u001B[32m  12217\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m method == \u001B[33m\"\u001B[39m\u001B[33msingle\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m> \u001B[39m\u001B[32m12218\u001B[39m     res = \u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_mgr\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquantile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mqs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m=\u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m  12219\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m method == \u001B[33m\"\u001B[39m\u001B[33mtable\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m  12220\u001B[39m     valid_interpolation = {\u001B[33m\"\u001B[39m\u001B[33mnearest\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mlower\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mhigher\u001B[39m\u001B[33m\"\u001B[39m}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/House_IQ_Setup/.venv1/lib/python3.13/site-packages/pandas/core/internals/managers.py:1568\u001B[39m, in \u001B[36mBlockManager.quantile\u001B[39m\u001B[34m(self, qs, interpolation)\u001B[39m\n\u001B[32m   1564\u001B[39m new_axes = \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m.axes)\n\u001B[32m   1565\u001B[39m new_axes[\u001B[32m1\u001B[39m] = Index(qs, dtype=np.float64)\n\u001B[32m   1567\u001B[39m blocks = [\n\u001B[32m-> \u001B[39m\u001B[32m1568\u001B[39m     \u001B[43mblk\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquantile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mqs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mqs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m=\u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m blk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.blocks\n\u001B[32m   1569\u001B[39m ]\n\u001B[32m   1571\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)(blocks, new_axes)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/House_IQ_Setup/.venv1/lib/python3.13/site-packages/pandas/core/internals/blocks.py:1957\u001B[39m, in \u001B[36mBlock.quantile\u001B[39m\u001B[34m(self, qs, interpolation)\u001B[39m\n\u001B[32m   1954\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.ndim == \u001B[32m2\u001B[39m\n\u001B[32m   1955\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m is_list_like(qs)  \u001B[38;5;66;03m# caller is responsible for this\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1957\u001B[39m result = \u001B[43mquantile_compat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mqs\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_values\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1958\u001B[39m \u001B[38;5;66;03m# ensure_block_shape needed for cases where we start with EA and result\u001B[39;00m\n\u001B[32m   1959\u001B[39m \u001B[38;5;66;03m#  is ndarray, e.g. IntegerArray, SparseArray\u001B[39;00m\n\u001B[32m   1960\u001B[39m result = ensure_block_shape(result, ndim=\u001B[32m2\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/House_IQ_Setup/.venv1/lib/python3.13/site-packages/pandas/core/array_algos/quantile.py:39\u001B[39m, in \u001B[36mquantile_compat\u001B[39m\u001B[34m(values, qs, interpolation)\u001B[39m\n\u001B[32m     37\u001B[39m     fill_value = na_value_for_dtype(values.dtype, compat=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m     38\u001B[39m     mask = isna(values)\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mquantile_with_mask\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mqs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     40\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     41\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m values._quantile(qs, interpolation)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/House_IQ_Setup/.venv1/lib/python3.13/site-packages/pandas/core/array_algos/quantile.py:97\u001B[39m, in \u001B[36mquantile_with_mask\u001B[39m\u001B[34m(values, mask, fill_value, qs, interpolation)\u001B[39m\n\u001B[32m     95\u001B[39m     result = np.repeat(flat, \u001B[38;5;28mlen\u001B[39m(values)).reshape(\u001B[38;5;28mlen\u001B[39m(values), \u001B[38;5;28mlen\u001B[39m(qs))\n\u001B[32m     96\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m97\u001B[39m     result = \u001B[43m_nanpercentile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     98\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     99\u001B[39m \u001B[43m        \u001B[49m\u001B[43mqs\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m100.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    100\u001B[39m \u001B[43m        \u001B[49m\u001B[43mna_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfill_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    101\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    102\u001B[39m \u001B[43m        \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m=\u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    103\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    105\u001B[39m     result = np.asarray(result)\n\u001B[32m    106\u001B[39m     result = result.T\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/House_IQ_Setup/.venv1/lib/python3.13/site-packages/pandas/core/array_algos/quantile.py:218\u001B[39m, in \u001B[36m_nanpercentile\u001B[39m\u001B[34m(values, qs, na_value, mask, interpolation)\u001B[39m\n\u001B[32m    216\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[32m    217\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m218\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpercentile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    220\u001B[39m \u001B[43m        \u001B[49m\u001B[43mqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    221\u001B[39m \u001B[43m        \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    222\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# error: No overload variant of \"percentile\" matches argument types\u001B[39;49;00m\n\u001B[32m    223\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# \"ndarray[Any, Any]\", \"ndarray[Any, dtype[floating[_64Bit]]]\",\u001B[39;49;00m\n\u001B[32m    224\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# \"int\", \"Dict[str, str]\"  [call-overload]\u001B[39;49;00m\n\u001B[32m    225\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[call-overload]\u001B[39;49;00m\n\u001B[32m    226\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/House_IQ_Setup/.venv1/lib/python3.13/site-packages/numpy/lib/_function_base_impl.py:4292\u001B[39m, in \u001B[36mpercentile\u001B[39m\u001B[34m(a, q, axis, out, overwrite_input, method, keepdims, weights, interpolation)\u001B[39m\n\u001B[32m   4289\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m np.any(weights < \u001B[32m0\u001B[39m):\n\u001B[32m   4290\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mWeights must be non-negative.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m4292\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_quantile_unchecked\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4293\u001B[39m \u001B[43m    \u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moverwrite_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/House_IQ_Setup/.venv1/lib/python3.13/site-packages/numpy/lib/_function_base_impl.py:4569\u001B[39m, in \u001B[36m_quantile_unchecked\u001B[39m\u001B[34m(a, q, axis, out, overwrite_input, method, keepdims, weights)\u001B[39m\n\u001B[32m   4560\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_quantile_unchecked\u001B[39m(a,\n\u001B[32m   4561\u001B[39m                         q,\n\u001B[32m   4562\u001B[39m                         axis=\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   4566\u001B[39m                         keepdims=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m   4567\u001B[39m                         weights=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m   4568\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Assumes that q is in [0, 1], and is an ndarray\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m4569\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_ureduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4570\u001B[39m \u001B[43m                    \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_quantile_ureduce_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4571\u001B[39m \u001B[43m                    \u001B[49m\u001B[43mq\u001B[49m\u001B[43m=\u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4572\u001B[39m \u001B[43m                    \u001B[49m\u001B[43mweights\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4573\u001B[39m \u001B[43m                    \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4574\u001B[39m \u001B[43m                    \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4575\u001B[39m \u001B[43m                    \u001B[49m\u001B[43mout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4576\u001B[39m \u001B[43m                    \u001B[49m\u001B[43moverwrite_input\u001B[49m\u001B[43m=\u001B[49m\u001B[43moverwrite_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4577\u001B[39m \u001B[43m                    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/House_IQ_Setup/.venv1/lib/python3.13/site-packages/numpy/lib/_function_base_impl.py:3914\u001B[39m, in \u001B[36m_ureduce\u001B[39m\u001B[34m(a, func, keepdims, **kwargs)\u001B[39m\n\u001B[32m   3911\u001B[39m     index_out = (\u001B[32m0\u001B[39m, ) * nd\n\u001B[32m   3912\u001B[39m     kwargs[\u001B[33m'\u001B[39m\u001B[33mout\u001B[39m\u001B[33m'\u001B[39m] = out[(\u001B[38;5;28mEllipsis\u001B[39m, ) + index_out]\n\u001B[32m-> \u001B[39m\u001B[32m3914\u001B[39m r = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3916\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m out \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   3917\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/House_IQ_Setup/.venv1/lib/python3.13/site-packages/numpy/lib/_function_base_impl.py:4744\u001B[39m, in \u001B[36m_quantile_ureduce_func\u001B[39m\u001B[34m(a, q, weights, axis, out, overwrite_input, method)\u001B[39m\n\u001B[32m   4742\u001B[39m     arr = a.copy()\n\u001B[32m   4743\u001B[39m     wgt = weights\n\u001B[32m-> \u001B[39m\u001B[32m4744\u001B[39m result = \u001B[43m_quantile\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4745\u001B[39m \u001B[43m                   \u001B[49m\u001B[43mquantiles\u001B[49m\u001B[43m=\u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4746\u001B[39m \u001B[43m                   \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4747\u001B[39m \u001B[43m                   \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4748\u001B[39m \u001B[43m                   \u001B[49m\u001B[43mout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4749\u001B[39m \u001B[43m                   \u001B[49m\u001B[43mweights\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwgt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4750\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/House_IQ_Setup/.venv1/lib/python3.13/site-packages/numpy/lib/_function_base_impl.py:4859\u001B[39m, in \u001B[36m_quantile\u001B[39m\u001B[34m(arr, quantiles, axis, method, out, weights)\u001B[39m\n\u001B[32m   4855\u001B[39m previous_indexes, next_indexes = _get_indexes(arr,\n\u001B[32m   4856\u001B[39m                                               virtual_indexes,\n\u001B[32m   4857\u001B[39m                                               values_count)\n\u001B[32m   4858\u001B[39m \u001B[38;5;66;03m# --- Sorting\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m4859\u001B[39m \u001B[43marr\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpartition\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4860\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43munique\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconcatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4861\u001B[39m \u001B[43m                              \u001B[49m\u001B[43mprevious_indexes\u001B[49m\u001B[43m.\u001B[49m\u001B[43mravel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4862\u001B[39m \u001B[43m                              \u001B[49m\u001B[43mnext_indexes\u001B[49m\u001B[43m.\u001B[49m\u001B[43mravel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4863\u001B[39m \u001B[43m                              \u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4864\u001B[39m \u001B[43m    \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   4865\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m supports_nans:\n\u001B[32m   4866\u001B[39m     slices_having_nans = np.isnan(arr[-\u001B[32m1\u001B[39m, ...])\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T17:57:22.598123Z",
     "start_time": "2025-12-23T17:57:22.097704Z"
    }
   },
   "cell_type": "code",
   "source": "pd.read_csv('/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv')",
   "id": "6812ba5f39856987",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        PROPERTY_ID  SALE_PRICE   SALE_DATE   LATITUDE  LONGITUDE STATE  \\\n",
       "0         173029632   100000000  2024-07-22  39.321025 -84.639359    oh   \n",
       "1          91476555   100000000  2021-04-19  40.744511 -73.637589    ny   \n",
       "2          94792198   100000000  2023-06-21  40.930592 -73.893787    ny   \n",
       "3         107312767    99000000  2021-10-05  39.332883 -84.222479    oh   \n",
       "4         101158806    98750000  2023-07-19  40.747580 -82.212181    oh   \n",
       "...             ...         ...         ...        ...        ...   ...   \n",
       "127321     93412006       10000  2022-09-08  44.181778 -75.069692    ny   \n",
       "127322    106448552       10000  2024-08-13  38.745783 -82.975813    oh   \n",
       "127323    105735114       10000  2016-07-22  39.747802 -84.266273    oh   \n",
       "127324     93659705       10000  2014-08-25  42.468624 -76.761039    ny   \n",
       "127325     90042973       10000  2014-01-31  42.941134 -78.807581    ny   \n",
       "\n",
       "                CITY      ZIP  CENSUS_TRACT  YEAR_BUILT  ...  VOTES_GOP  \\\n",
       "0          fairfield  45014.0       10800.0      1990.0  ...     114392   \n",
       "1            mineola  11501.0      303600.0      2017.0  ...     326716   \n",
       "2            yonkers  10701.0         300.0      1965.0  ...     144713   \n",
       "3         maineville  45039.0       32203.0      2005.0  ...      87988   \n",
       "4       jeromesville  44840.0      970900.0      2005.0  ...      19407   \n",
       "...              ...      ...           ...         ...  ...        ...   \n",
       "127321   oswegatchie  13670.0      492501.0      1890.0  ...      24608   \n",
       "127322    portsmouth  45662.0        3400.0      1919.0  ...      22609   \n",
       "127323        dayton  45417.0        4400.0      1932.0  ...     129034   \n",
       "127324   trumansburg  14886.0      950100.0      2017.0  ...       5621   \n",
       "127325       buffalo  14215.0        4300.0      1920.0  ...     197527   \n",
       "\n",
       "        VOTES_DEM  TOTAL_VOTES   PER_GOP   PER_DEM  PER_POINT_DIFF  \\\n",
       "0           69613       186737  0.612583  0.372786        0.239797   \n",
       "1          396504       732756  0.445873  0.541113       -0.095240   \n",
       "2          312371       462122  0.313149  0.675949       -0.362800   \n",
       "3           46069       136100  0.646495  0.338494        0.308001   \n",
       "4            6541        26405  0.734974  0.247718        0.487256   \n",
       "...           ...          ...       ...       ...             ...   \n",
       "127321      19361        44907  0.547977  0.431135        0.116841   \n",
       "127322       9080        32047  0.705495  0.283334        0.422161   \n",
       "127323     135064       268505  0.480565  0.503022       -0.022458   \n",
       "127324       3903         9766  0.575568  0.399652        0.175916   \n",
       "127325     267174       473021  0.417586  0.564825       -0.147239   \n",
       "\n",
       "        DEM_MARGIN  REP_MARGIN  POLITICAL_LEAN_STRENGTH  STATE_FIPS  \n",
       "0        -0.127214    0.112583                 0.239797          39  \n",
       "1         0.041113   -0.054127                 0.095240          36  \n",
       "2         0.175949   -0.186851                 0.362800          36  \n",
       "3        -0.161506    0.146495                 0.308001          39  \n",
       "4        -0.252282    0.234974                 0.487256          39  \n",
       "...            ...         ...                      ...         ...  \n",
       "127321   -0.068865    0.047977                 0.116841          36  \n",
       "127322   -0.216666    0.205495                 0.422161          39  \n",
       "127323    0.003022   -0.019435                 0.022458          39  \n",
       "127324   -0.100348    0.075568                 0.175916          36  \n",
       "127325    0.064825   -0.082414                 0.147239          36  \n",
       "\n",
       "[127326 rows x 86 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROPERTY_ID</th>\n",
       "      <th>SALE_PRICE</th>\n",
       "      <th>SALE_DATE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>STATE</th>\n",
       "      <th>CITY</th>\n",
       "      <th>ZIP</th>\n",
       "      <th>CENSUS_TRACT</th>\n",
       "      <th>YEAR_BUILT</th>\n",
       "      <th>...</th>\n",
       "      <th>VOTES_GOP</th>\n",
       "      <th>VOTES_DEM</th>\n",
       "      <th>TOTAL_VOTES</th>\n",
       "      <th>PER_GOP</th>\n",
       "      <th>PER_DEM</th>\n",
       "      <th>PER_POINT_DIFF</th>\n",
       "      <th>DEM_MARGIN</th>\n",
       "      <th>REP_MARGIN</th>\n",
       "      <th>POLITICAL_LEAN_STRENGTH</th>\n",
       "      <th>STATE_FIPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173029632</td>\n",
       "      <td>100000000</td>\n",
       "      <td>2024-07-22</td>\n",
       "      <td>39.321025</td>\n",
       "      <td>-84.639359</td>\n",
       "      <td>oh</td>\n",
       "      <td>fairfield</td>\n",
       "      <td>45014.0</td>\n",
       "      <td>10800.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>...</td>\n",
       "      <td>114392</td>\n",
       "      <td>69613</td>\n",
       "      <td>186737</td>\n",
       "      <td>0.612583</td>\n",
       "      <td>0.372786</td>\n",
       "      <td>0.239797</td>\n",
       "      <td>-0.127214</td>\n",
       "      <td>0.112583</td>\n",
       "      <td>0.239797</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91476555</td>\n",
       "      <td>100000000</td>\n",
       "      <td>2021-04-19</td>\n",
       "      <td>40.744511</td>\n",
       "      <td>-73.637589</td>\n",
       "      <td>ny</td>\n",
       "      <td>mineola</td>\n",
       "      <td>11501.0</td>\n",
       "      <td>303600.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>...</td>\n",
       "      <td>326716</td>\n",
       "      <td>396504</td>\n",
       "      <td>732756</td>\n",
       "      <td>0.445873</td>\n",
       "      <td>0.541113</td>\n",
       "      <td>-0.095240</td>\n",
       "      <td>0.041113</td>\n",
       "      <td>-0.054127</td>\n",
       "      <td>0.095240</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94792198</td>\n",
       "      <td>100000000</td>\n",
       "      <td>2023-06-21</td>\n",
       "      <td>40.930592</td>\n",
       "      <td>-73.893787</td>\n",
       "      <td>ny</td>\n",
       "      <td>yonkers</td>\n",
       "      <td>10701.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>...</td>\n",
       "      <td>144713</td>\n",
       "      <td>312371</td>\n",
       "      <td>462122</td>\n",
       "      <td>0.313149</td>\n",
       "      <td>0.675949</td>\n",
       "      <td>-0.362800</td>\n",
       "      <td>0.175949</td>\n",
       "      <td>-0.186851</td>\n",
       "      <td>0.362800</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107312767</td>\n",
       "      <td>99000000</td>\n",
       "      <td>2021-10-05</td>\n",
       "      <td>39.332883</td>\n",
       "      <td>-84.222479</td>\n",
       "      <td>oh</td>\n",
       "      <td>maineville</td>\n",
       "      <td>45039.0</td>\n",
       "      <td>32203.0</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>...</td>\n",
       "      <td>87988</td>\n",
       "      <td>46069</td>\n",
       "      <td>136100</td>\n",
       "      <td>0.646495</td>\n",
       "      <td>0.338494</td>\n",
       "      <td>0.308001</td>\n",
       "      <td>-0.161506</td>\n",
       "      <td>0.146495</td>\n",
       "      <td>0.308001</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101158806</td>\n",
       "      <td>98750000</td>\n",
       "      <td>2023-07-19</td>\n",
       "      <td>40.747580</td>\n",
       "      <td>-82.212181</td>\n",
       "      <td>oh</td>\n",
       "      <td>jeromesville</td>\n",
       "      <td>44840.0</td>\n",
       "      <td>970900.0</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19407</td>\n",
       "      <td>6541</td>\n",
       "      <td>26405</td>\n",
       "      <td>0.734974</td>\n",
       "      <td>0.247718</td>\n",
       "      <td>0.487256</td>\n",
       "      <td>-0.252282</td>\n",
       "      <td>0.234974</td>\n",
       "      <td>0.487256</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127321</th>\n",
       "      <td>93412006</td>\n",
       "      <td>10000</td>\n",
       "      <td>2022-09-08</td>\n",
       "      <td>44.181778</td>\n",
       "      <td>-75.069692</td>\n",
       "      <td>ny</td>\n",
       "      <td>oswegatchie</td>\n",
       "      <td>13670.0</td>\n",
       "      <td>492501.0</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24608</td>\n",
       "      <td>19361</td>\n",
       "      <td>44907</td>\n",
       "      <td>0.547977</td>\n",
       "      <td>0.431135</td>\n",
       "      <td>0.116841</td>\n",
       "      <td>-0.068865</td>\n",
       "      <td>0.047977</td>\n",
       "      <td>0.116841</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127322</th>\n",
       "      <td>106448552</td>\n",
       "      <td>10000</td>\n",
       "      <td>2024-08-13</td>\n",
       "      <td>38.745783</td>\n",
       "      <td>-82.975813</td>\n",
       "      <td>oh</td>\n",
       "      <td>portsmouth</td>\n",
       "      <td>45662.0</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>1919.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22609</td>\n",
       "      <td>9080</td>\n",
       "      <td>32047</td>\n",
       "      <td>0.705495</td>\n",
       "      <td>0.283334</td>\n",
       "      <td>0.422161</td>\n",
       "      <td>-0.216666</td>\n",
       "      <td>0.205495</td>\n",
       "      <td>0.422161</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127323</th>\n",
       "      <td>105735114</td>\n",
       "      <td>10000</td>\n",
       "      <td>2016-07-22</td>\n",
       "      <td>39.747802</td>\n",
       "      <td>-84.266273</td>\n",
       "      <td>oh</td>\n",
       "      <td>dayton</td>\n",
       "      <td>45417.0</td>\n",
       "      <td>4400.0</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>...</td>\n",
       "      <td>129034</td>\n",
       "      <td>135064</td>\n",
       "      <td>268505</td>\n",
       "      <td>0.480565</td>\n",
       "      <td>0.503022</td>\n",
       "      <td>-0.022458</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>-0.019435</td>\n",
       "      <td>0.022458</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127324</th>\n",
       "      <td>93659705</td>\n",
       "      <td>10000</td>\n",
       "      <td>2014-08-25</td>\n",
       "      <td>42.468624</td>\n",
       "      <td>-76.761039</td>\n",
       "      <td>ny</td>\n",
       "      <td>trumansburg</td>\n",
       "      <td>14886.0</td>\n",
       "      <td>950100.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5621</td>\n",
       "      <td>3903</td>\n",
       "      <td>9766</td>\n",
       "      <td>0.575568</td>\n",
       "      <td>0.399652</td>\n",
       "      <td>0.175916</td>\n",
       "      <td>-0.100348</td>\n",
       "      <td>0.075568</td>\n",
       "      <td>0.175916</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127325</th>\n",
       "      <td>90042973</td>\n",
       "      <td>10000</td>\n",
       "      <td>2014-01-31</td>\n",
       "      <td>42.941134</td>\n",
       "      <td>-78.807581</td>\n",
       "      <td>ny</td>\n",
       "      <td>buffalo</td>\n",
       "      <td>14215.0</td>\n",
       "      <td>4300.0</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>...</td>\n",
       "      <td>197527</td>\n",
       "      <td>267174</td>\n",
       "      <td>473021</td>\n",
       "      <td>0.417586</td>\n",
       "      <td>0.564825</td>\n",
       "      <td>-0.147239</td>\n",
       "      <td>0.064825</td>\n",
       "      <td>-0.082414</td>\n",
       "      <td>0.147239</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127326 rows × 86 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T18:03:44.334245Z",
     "start_time": "2025-12-23T18:03:36.447944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "import time, os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Config - FIXED PARAMETERS\n",
    "INCLUDE_MLS, INCLUDE_CENSUS, INCLUDE_NEIGHBORHOOD, INCLUDE_IMAGE = True, True, True, False\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS = 20000, 0.3, 42, -1, 8\n",
    "N_EST, QUANTILES, MAX_SEGMENTS = 100, [0.1, 0.5, 0.9], 7  # MAX 7 SEGMENTS\n",
    "\n",
    "INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "BASE_FEATS = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"effective_year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\"fireplace_code\",\"latitude\",\"longitude\",\"geo_cluster\"]\n",
    "ENG_FEATS = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\"]\n",
    "PRIOR_FEATS = [\"prior_sale_price\",\"prior_price_per_sqft\",\"sqft_per_prior_dollar\",\"years_since_last_sale\",\"expected_appreciation\",\"has_prior_sale\",\"recently_sold\"]\n",
    "CLUSTER_FEATS = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "CENSUS_FEATS = [\"total_population_25plus\",\"male_bachelors_degree\",\"female_bachelors_degree\",\"pct_bachelors_degree\",\"total_population\",\"median_earnings_total\",\"median_earnings_male\",\"median_earnings_female\",\"median_household_income\",\"median_home_value\",\"median_gross_rent\",\"owner_occupied_units\",\"renter_occupied_units\",\"pct_owner_occupied\",\"occupied_units\",\"vacant_units\",\"median_age\",\"civilian_employed\",\"civilian_unemployed\",\"unemployment_rate\",\"income_education_score\"]\n",
    "ELECT_FEATS = [\"votes_gop\",\"votes_dem\",\"total_votes\",\"per_gop\",\"per_dem\",\"per_point_diff\",\"dem_margin\",\"rep_margin\"]\n",
    "IMG_FEATS = [\"topic_1\",\"topic_2\",\"topic_3\",\"topic_4\",\"topic_5\",\"topic_6\",\"topic_7\",\"topic_8\",\"topic_9\",\"topic_10\",\"gran_c_in\",\"gran_c_ex\",\"gran_c\",\"high_c_in\",\"high_c_ex\",\"high_c\"]\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8') if set(df[c].dropna().unique()).issubset({0,1}) else df[c].astype('int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return optimize_dtypes(df), y_col, id_col, state_col\n",
    "\n",
    "def assign_segments_simple(df):\n",
    "    \"\"\"Simple 5-7 segment strategy based on price and size\"\"\"\n",
    "    segments = pd.Series(['mid'] * len(df), index=df.index)\n",
    "\n",
    "    # Primary: Price tier (3 segments)\n",
    "    if 'currentsalesprice' in df.columns or 'sale_price' in df.columns:\n",
    "        price_col = 'currentsalesprice' if 'currentsalesprice' in df.columns else 'sale_price'\n",
    "        price = df[price_col].fillna(df[price_col].median())\n",
    "        q33, q67 = price.quantile([.33, .67])\n",
    "        price_tier = pd.Series(['mid'] * len(df), index=df.index)\n",
    "        price_tier[price < q33] = 'budget'\n",
    "        price_tier[price > q67] = 'premium'\n",
    "    else:\n",
    "        price_tier = pd.Series(['mid'] * len(df), index=df.index)\n",
    "\n",
    "    # Secondary: Size (2 segments: small vs large)\n",
    "    if 'living_sqft' in df.columns:\n",
    "        sqft = df['living_sqft'].fillna(df['living_sqft'].median())\n",
    "        sqft_median = sqft.median()\n",
    "        size_tier = pd.Series(['small'] * len(df), index=df.index)\n",
    "        size_tier[sqft > sqft_median] = 'large'\n",
    "    else:\n",
    "        size_tier = pd.Series(['small'] * len(df), index=df.index)\n",
    "\n",
    "    # Combine: price_size format (e.g., \"budget_small\", \"premium_large\")\n",
    "    segments = price_tier + '_' + size_tier\n",
    "\n",
    "    return segments\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    if 'living_sqft' in df.columns and 'bedrooms' in df.columns: df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns: df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns: df['lot_acres'] = df['lot_sqft']/43560\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'] = 2024-df['year_built']\n",
    "        df['is_new'] = (df['property_age']<=5).astype('int8')\n",
    "        df['age_squared'] = df['property_age']**2\n",
    "    if 'garage_spaces' in df.columns: df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "    if 'living_sqft' in df.columns: df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "    if INCLUDE_CENSUS and all(c in df.columns for c in ['median_household_income','pct_bachelors_degree']):\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "    if 'prior_sale_price' in df.columns and 'living_sqft' in df.columns:\n",
    "        df['prior_price_per_sqft'] = df['prior_sale_price']/(df['living_sqft']+1)\n",
    "        df['sqft_per_prior_dollar'] = df['living_sqft']/(df['prior_sale_price']+1)\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        if 'prior_sale_price' in df.columns:\n",
    "            df['expected_appreciation'] = df['prior_sale_price']*(1.04)**df['years_since_last_sale']\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df.loc[missing,'median_home_value'] if 'median_home_value' in df.columns and INCLUDE_CENSUS else df['prior_sale_price'].median()\n",
    "    if 'years_since_last_sale' in df.columns: df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "    if with_price and y_col in df.columns and 'living_sqft' in df.columns:\n",
    "        df['price_per_sqft'] = df[y_col]/(df['living_sqft']+1)\n",
    "        df['sqft_per_dollar'] = df['living_sqft']/(df[y_col]+1)\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df):\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0; return df\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum()<N_CLUSTERS: df['geo_cluster'] = 0; return df\n",
    "    df['geo_cluster'] = 0\n",
    "    kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "    df.loc[valid,'geo_cluster'] = kmeans.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    return df\n",
    "\n",
    "def add_cluster_feats(train, test, y_col):\n",
    "    if 'geo_cluster' not in train.columns or y_col not in train.columns:\n",
    "        for d in [train,test]: d['cluster_avg_price'] = d['cluster_med_price'] = train[y_col].median() if y_col in train.columns else 0\n",
    "        return train, test\n",
    "    stats = train.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "    stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "    train = train.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    test = test.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    return train, test\n",
    "\n",
    "def filter_outliers(df, name, y_col):\n",
    "    orig = len(df)\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)]\n",
    "    if 'sqft_per_dollar' in df.columns: df = df[df['sqft_per_dollar']<=df['sqft_per_dollar'].quantile(.95)]\n",
    "    if 'price_per_sqft' in df.columns: df = df.drop(columns=['price_per_sqft','sqft_per_dollar'])\n",
    "    if 'lot_sqft' in df.columns: df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "    if 'year_built' in df.columns: df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "    if len(df)>=100:\n",
    "        try:\n",
    "            feats = [c for c in ['living_sqft','lot_sqft',y_col,'year_built'] if c in df.columns]\n",
    "            if len(feats)>=3:\n",
    "                X = df[feats].fillna(df[feats].median())\n",
    "                df = df[IsolationForest(contamination=.05,random_state=RAND_STATE,n_jobs=N_JOBS).fit_predict(X)==1]\n",
    "        except: pass\n",
    "    pct_filt = (orig-len(df))/orig*100 if orig>0 else 0\n",
    "    if pct_filt > 0: print(f\"  {name}: {orig:,}→{len(df):,} ({pct_filt:.1f}% filtered)\")\n",
    "    return df\n",
    "\n",
    "def train_model(X, y, q):\n",
    "    return XGBRegressor(\n",
    "        objective='reg:quantileerror',\n",
    "        quantile_alpha=q,\n",
    "        n_estimators=N_EST,\n",
    "        learning_rate=.1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        subsample=.8,\n",
    "        colsample_bytree=.8,\n",
    "        random_state=RAND_STATE,\n",
    "        n_jobs=N_JOBS,\n",
    "        tree_method='hist'\n",
    "    ).fit(X, y, verbose=False)\n",
    "\n",
    "def feature_importance(models, feat_names, metrics):\n",
    "    rows = []\n",
    "    for seg, mdl in models.items():\n",
    "        scores = mdl['q50'].get_booster().get_score(importance_type=\"gain\")\n",
    "        w = metrics[seg][\"n_test\"]\n",
    "        for k,v in scores.items():\n",
    "            idx = int(k[1:])\n",
    "            if idx<len(feat_names): rows.append((feat_names[idx],v,w))\n",
    "    if not rows: return pd.DataFrame(columns=[\"feature\",\"importance\"])\n",
    "    df = pd.DataFrame(rows, columns=[\"feature\",\"gain\",\"weight\"])\n",
    "    out = df.assign(wg=df[\"gain\"]*df[\"weight\"]).groupby(\"feature\",as_index=False).agg(tg=(\"wg\",\"sum\")).sort_values(\"tg\",ascending=False)\n",
    "    out[\"importance\"] = out[\"tg\"]/out[\"tg\"].sum()\n",
    "    return out[[\"feature\",\"importance\"]].head(100)\n",
    "\n",
    "def prepare_data(df, y_col, id_col, state_col):\n",
    "    print(f\"\\nPreparing data...\")\n",
    "    df = df[df[y_col]>=MIN_PRICE]\n",
    "    print(f\"{len(df):,} records after price filter\")\n",
    "    df = engineer(geo_cluster(df), y_col)\n",
    "    feat_groups = []\n",
    "    if INCLUDE_MLS: feat_groups.extend([BASE_FEATS,ENG_FEATS,PRIOR_FEATS,CLUSTER_FEATS])\n",
    "    if INCLUDE_CENSUS: feat_groups.append(CENSUS_FEATS)\n",
    "    if INCLUDE_NEIGHBORHOOD: feat_groups.append(ELECT_FEATS)\n",
    "    if INCLUDE_IMAGE: feat_groups.append(IMG_FEATS)\n",
    "    all_feats = [f for g in feat_groups for f in g]\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "    return df.dropna(subset=[y_col]), feats\n",
    "\n",
    "def train_segments(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining segmented models on {len(df):,} properties\")\n",
    "\n",
    "    # Simple 5-7 segment strategy\n",
    "    df['seg'] = assign_segments_simple(df)\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    print(f\"{len(seg_cnts)} segments created\")\n",
    "    for seg, cnt in seg_cnts.head(MAX_SEGMENTS).items(): print(f\"  {seg}: {cnt:,}\")\n",
    "\n",
    "    # Merge small segments\n",
    "    small = seg_cnts[seg_cnts<50].index\n",
    "    if len(small)>0:\n",
    "        df.loc[df['seg'].isin(small),'seg'] = 'other'\n",
    "        print(f\"Merged {len(small)} small segments into 'other'\")\n",
    "\n",
    "    # If still too many segments, merge smallest ones\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    if len(seg_cnts) > MAX_SEGMENTS:\n",
    "        keep_segs = seg_cnts.head(MAX_SEGMENTS-1).index\n",
    "        df.loc[~df['seg'].isin(keep_segs),'seg'] = 'other'\n",
    "        print(f\"Consolidated to {MAX_SEGMENTS} segments maximum\")\n",
    "\n",
    "    models, metrics, preds_list = {}, {}, []\n",
    "    for seg in df['seg'].unique():\n",
    "        seg_df = df[df['seg']==seg].copy()\n",
    "        if len(seg_df)<50: continue\n",
    "        seg_df = filter_outliers(seg_df, seg, y_col)\n",
    "        if len(seg_df)<50: continue\n",
    "        train_idx = seg_df.sample(frac=1-TEST_SIZE,random_state=RAND_STATE).index\n",
    "        train_df, test_df = seg_df.loc[train_idx].copy(), seg_df.loc[seg_df.index.difference(train_idx)].copy()\n",
    "        train_df, test_df = add_cluster_feats(train_df, test_df, y_col)\n",
    "        X_tr,y_tr = train_df[feats].values, train_df[y_col].values\n",
    "        X_te,y_te = test_df[feats].values, test_df[y_col].values\n",
    "        ids_te = test_df[id_col].values\n",
    "        states_te = test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "        seg_models, seg_preds = {}, []\n",
    "        for q in QUANTILES:\n",
    "            m = train_model(X_tr,y_tr,q)\n",
    "            seg_models[f\"q{int(q*100)}\"] = m\n",
    "            seg_preds.append(m.predict(X_te))\n",
    "        models[seg] = seg_models\n",
    "        y_pred = seg_preds[1]\n",
    "        mae,mape = mean_absolute_error(y_te,y_pred), np.mean(np.abs((y_te-y_pred)/y_te))*100\n",
    "        r2,cov = r2_score(y_te,y_pred), np.mean((y_te>=seg_preds[0])&(y_te<=seg_preds[2]))*100\n",
    "        metrics[seg] = {'n_train':len(X_tr),'n_test':len(X_te),'mae':mae,'mape':mape,'r2':r2,'cov':cov,'p_min':seg_df[y_col].min(),'p_max':seg_df[y_col].max(),'p_med':seg_df[y_col].median()}\n",
    "        print(f\"  {seg}: {len(test_df):,} test | MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.3f}\")\n",
    "        preds_list.append(pd.DataFrame({'property_id':ids_te,'state':states_te,'actual':y_te,'predicted':y_pred,'pred_lower':seg_preds[0],'pred_upper':seg_preds[2],'segment':seg}))\n",
    "    return {'models':models,'metrics':metrics,'predictions':pd.concat(preds_list),'feature_importance':feature_importance(models,feats,metrics),'feature_names':feats}\n",
    "\n",
    "def save_results(results, out_dir):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi = results['predictions'], results['metrics'], results['feature_importance']\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    # Summary\n",
    "    ws = wb.create_sheet(\"Summary\",0)\n",
    "    ws['A1'], ws['A1'].font = 'GRANULAR SEGMENTED AVM', Font(bold=True,size=14)\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    data = [['Metric','Value'],['Properties',len(preds)],['Segments',len(metrics)],['R²',f'{r2:.4f}'],['MAE',f'${mae:,.0f}'],['MAPE%',f'{mape:.2f}%']]\n",
    "    for i,(k,v) in enumerate(data,5): ws[f'A{i}'],ws[f'A{i}'].font,ws[f'B{i}'] = k,Font(bold=True),v\n",
    "\n",
    "    # Segments\n",
    "    ws = wb.create_sheet(\"Segments\")\n",
    "    seg_df = pd.DataFrame([{**{'Segment':s},**m} for s,m in metrics.items()])\n",
    "    for i,h in enumerate(seg_df.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(seg_df.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    # Predictions\n",
    "    ws = wb.create_sheet(\"Predictions\")\n",
    "    for i,h in enumerate(preds.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(preds.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/granular_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "    preds.to_csv(f\"{out_dir}/predictions_{ts}.csv\",index=False)\n",
    "    seg_df.to_csv(f\"{out_dir}/segments_{ts}.csv\",index=False)\n",
    "    fi.to_csv(f\"{out_dir}/importance_{ts}.csv\",index=False)\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved with timestamp: {ts}\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60)\n",
    "    print(\"GRANULAR SEGMENTED AVM\")\n",
    "    print(\"=\"*60)\n",
    "    df, y_col, id_col, state_col = load_data(INPUT_PATH)\n",
    "    df, feats = prepare_data(df, y_col, id_col, state_col)\n",
    "    results = train_segments(df, feats, y_col, id_col, state_col)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR)\n",
    "    preds = results['predictions']\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ COMPLETE in {time.time()-t0:.1f}s\")\n",
    "    print(f\"  {len(preds):,} properties | {preds['segment'].nunique()} segments\")\n",
    "    print(f\"  R²: {r2:.4f} | MAE: ${mae:,.0f} | MAPE: {mape:.2f}%\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "if __name__ == \"__main__\": main()"
   ],
   "id": "1a71f259a75a067",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRANULAR SEGMENTED AVM\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 129.6MB | Price:sale_price ID:property_id\n",
      "\n",
      "Preparing data...\n",
      "127,258 records after price filter\n",
      "51/60 features available\n",
      "\n",
      "Training segmented models on 127,258 properties\n",
      "6 segments created\n",
      "  budget_small: 23,028\n",
      "  mid_large: 22,880\n",
      "  premium_large: 21,935\n",
      "  mid_small: 20,638\n",
      "  premium_small: 19,967\n",
      "  budget_large: 18,810\n",
      "  premium_small: 19,967→15,479 (22.5% filtered)\n",
      "  premium_small: 4,644 test | MAE:$1,693,499 | MAPE:34.70% | R²:0.463\n",
      "  premium_large: 21,935→16,873 (23.1% filtered)\n",
      "  premium_large: 5,062 test | MAE:$635,936 | MAPE:21.32% | R²:0.395\n",
      "  mid_large: 22,880→17,671 (22.8% filtered)\n",
      "  mid_large: 5,301 test | MAE:$119,768 | MAPE:10.16% | R²:0.117\n",
      "  mid_small: 20,638→15,752 (23.7% filtered)\n",
      "  mid_small: 4,726 test | MAE:$113,631 | MAPE:9.51% | R²:0.169\n",
      "  budget_large: 18,810→14,526 (22.8% filtered)\n",
      "  budget_large: 4,358 test | MAE:$37,205 | MAPE:4.26% | R²:0.062\n",
      "  budget_small: 23,028→17,660 (23.3% filtered)\n",
      "  budget_small: 5,298 test | MAE:$39,278 | MAPE:4.56% | R²:0.028\n",
      "\n",
      "Saving results...\n",
      "✓ Excel: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/granular_20251223_130343.xlsx\n",
      "✓ CSVs saved with timestamp: 20251223_130343\n",
      "\n",
      "============================================================\n",
      "✓ COMPLETE in 7.9s\n",
      "  29,389 properties | 6 segments\n",
      "  R²: 0.6605 | MAE: $429,612 | MAPE: 13.97%\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T19:04:19.881565Z",
     "start_time": "2025-12-23T19:04:10.364256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import time, os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Config - FIXED PARAMETERS\n",
    "INCLUDE_MLS, INCLUDE_CENSUS, INCLUDE_NEIGHBORHOOD, INCLUDE_IMAGE = True, True, True, False\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS = 20000, 0.3, 42, -1, 8\n",
    "N_EST, QUANTILES, MAX_SEGMENTS = 100, [0.1, 0.5, 0.9], 7  # MAX 7 SEGMENTS\n",
    "\n",
    "INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "BASE_FEATS = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"effective_year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\"fireplace_code\",\"latitude\",\"longitude\",\"geo_cluster\"]\n",
    "ENG_FEATS = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\"]\n",
    "PRIOR_FEATS = [\"prior_sale_price\",\"prior_price_per_sqft\",\"sqft_per_prior_dollar\",\"years_since_last_sale\",\"expected_appreciation\",\"has_prior_sale\",\"recently_sold\"]\n",
    "CLUSTER_FEATS = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "CENSUS_FEATS = [\"total_population_25plus\",\"male_bachelors_degree\",\"female_bachelors_degree\",\"pct_bachelors_degree\",\"median_earnings_total\",\"median_earnings_male\",\"median_earnings_female\",\"median_household_income\",\"median_home_value\",\"median_gross_rent\",\"owner_occupied_units\",\"renter_occupied_units\",\"pct_owner_occupied\",\"occupied_units\",\"vacant_units\",\"median_age\",\"civilian_employed\",\"civilian_unemployed\",\"unemployment_rate\",\"income_education_score\"]\n",
    "ELECT_FEATS = [\"votes_gop\",\"votes_dem\",\"total_votes\",\"per_gop\",\"per_dem\",\"per_point_diff\",\"dem_margin\",\"rep_margin\"]\n",
    "IMG_FEATS = [\"topic_1\",\"topic_2\",\"topic_3\",\"topic_4\",\"topic_5\",\"topic_6\",\"topic_7\",\"topic_8\",\"topic_9\",\"topic_10\",\"gran_c_in\",\"gran_c_ex\",\"gran_c\",\"high_c_in\",\"high_c_ex\",\"high_c\"]\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8') if set(df[c].dropna().unique()).issubset({0,1}) else df[c].astype('int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return optimize_dtypes(df), y_col, id_col, state_col\n",
    "\n",
    "def assign_segments_simple(df):\n",
    "    \"\"\"Simple 5-7 segment strategy based on price and size\"\"\"\n",
    "    segments = pd.Series(['mid'] * len(df), index=df.index)\n",
    "\n",
    "    # Primary: Price tier (3 segments)\n",
    "    if 'currentsalesprice' in df.columns or 'sale_price' in df.columns:\n",
    "        price_col = 'currentsalesprice' if 'currentsalesprice' in df.columns else 'sale_price'\n",
    "        price = df[price_col].fillna(df[price_col].median())\n",
    "        q33, q67 = price.quantile([.33, .67])\n",
    "        price_tier = pd.Series(['mid'] * len(df), index=df.index)\n",
    "        price_tier[price < q33] = 'budget'\n",
    "        price_tier[price > q67] = 'premium'\n",
    "    else:\n",
    "        price_tier = pd.Series(['mid'] * len(df), index=df.index)\n",
    "\n",
    "    # Secondary: Size (2 segments: small vs large)\n",
    "    if 'living_sqft' in df.columns:\n",
    "        sqft = df['living_sqft'].fillna(df['living_sqft'].median())\n",
    "        sqft_median = sqft.median()\n",
    "        size_tier = pd.Series(['small'] * len(df), index=df.index)\n",
    "        size_tier[sqft > sqft_median] = 'large'\n",
    "    else:\n",
    "        size_tier = pd.Series(['small'] * len(df), index=df.index)\n",
    "\n",
    "    # Combine: price_size format (e.g., \"budget_small\", \"premium_large\")\n",
    "    segments = price_tier + '_' + size_tier\n",
    "\n",
    "    return segments\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    if 'living_sqft' in df.columns and 'bedrooms' in df.columns: df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns: df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns: df['lot_acres'] = df['lot_sqft']/43560\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'] = 2024-df['year_built']\n",
    "        df['is_new'] = (df['property_age']<=5).astype('int8')\n",
    "        df['age_squared'] = df['property_age']**2\n",
    "    if 'garage_spaces' in df.columns: df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "    if 'living_sqft' in df.columns: df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "    if INCLUDE_CENSUS and all(c in df.columns for c in ['median_household_income','pct_bachelors_degree']):\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "    if 'prior_sale_price' in df.columns and 'living_sqft' in df.columns:\n",
    "        df['prior_price_per_sqft'] = df['prior_sale_price']/(df['living_sqft']+1)\n",
    "        df['sqft_per_prior_dollar'] = df['living_sqft']/(df['prior_sale_price']+1)\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        if 'prior_sale_price' in df.columns:\n",
    "            df['expected_appreciation'] = df['prior_sale_price']*(1.04)**df['years_since_last_sale']\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df.loc[missing,'median_home_value'] if 'median_home_value' in df.columns and INCLUDE_CENSUS else df['prior_sale_price'].median()\n",
    "    if 'years_since_last_sale' in df.columns: df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "    if with_price and y_col in df.columns and 'living_sqft' in df.columns:\n",
    "        df['price_per_sqft'] = df[y_col]/(df['living_sqft']+1)\n",
    "        df['sqft_per_dollar'] = df['living_sqft']/(df[y_col]+1)\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df):\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0; return df\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum()<N_CLUSTERS: df['geo_cluster'] = 0; return df\n",
    "    df['geo_cluster'] = 0\n",
    "    kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "    df.loc[valid,'geo_cluster'] = kmeans.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    return df\n",
    "\n",
    "def add_cluster_feats(train, test, y_col):\n",
    "    if 'geo_cluster' not in train.columns or y_col not in train.columns:\n",
    "        for d in [train,test]: d['cluster_avg_price'] = d['cluster_med_price'] = train[y_col].median() if y_col in train.columns else 0\n",
    "        return train, test\n",
    "    stats = train.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "    stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "    train = train.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    test = test.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    return train, test\n",
    "\n",
    "def filter_outliers(df, name, y_col):\n",
    "    orig = len(df)\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)]\n",
    "    if 'sqft_per_dollar' in df.columns: df = df[df['sqft_per_dollar']<=df['sqft_per_dollar'].quantile(.95)]\n",
    "    if 'price_per_sqft' in df.columns: df = df.drop(columns=['price_per_sqft','sqft_per_dollar'])\n",
    "    if 'lot_sqft' in df.columns: df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "    if 'year_built' in df.columns: df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "    if len(df)>=100:\n",
    "        try:\n",
    "            feats = [c for c in ['living_sqft','lot_sqft',y_col,'year_built'] if c in df.columns]\n",
    "            if len(feats)>=3:\n",
    "                X = df[feats].fillna(df[feats].median())\n",
    "                df = df[IsolationForest(contamination=.05,random_state=RAND_STATE,n_jobs=N_JOBS).fit_predict(X)==1]\n",
    "        except: pass\n",
    "    pct_filt = (orig-len(df))/orig*100 if orig>0 else 0\n",
    "    if pct_filt > 0: print(f\"  {name}: {orig:,}→{len(df):,} ({pct_filt:.1f}% filtered)\")\n",
    "    return df\n",
    "\n",
    "def train_model(X, y, q):\n",
    "    return XGBRegressor(\n",
    "        objective='reg:quantileerror',\n",
    "        quantile_alpha=q,\n",
    "        n_estimators=N_EST,\n",
    "        learning_rate=.1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        subsample=.8,\n",
    "        colsample_bytree=.8,\n",
    "        random_state=RAND_STATE,\n",
    "        n_jobs=N_JOBS,\n",
    "        tree_method='hist'\n",
    "    ).fit(X, y, verbose=False)\n",
    "\n",
    "def get_feature_importance_per_segment(model, feat_names, top_n=20):\n",
    "    \"\"\"Extract feature importance for a single segment model\"\"\"\n",
    "    scores = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    importance_list = []\n",
    "    for k, v in scores.items():\n",
    "        idx = int(k[1:])\n",
    "        if idx < len(feat_names):\n",
    "            importance_list.append((feat_names[idx], v))\n",
    "\n",
    "    # Sort and normalize\n",
    "    importance_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    total_gain = sum(v for _, v in importance_list)\n",
    "\n",
    "    if total_gain > 0:\n",
    "        importance_df = pd.DataFrame([\n",
    "            {'feature': feat, 'gain': gain, 'importance': gain/total_gain}\n",
    "            for feat, gain in importance_list[:top_n]\n",
    "        ])\n",
    "    else:\n",
    "        importance_df = pd.DataFrame(columns=['feature', 'gain', 'importance'])\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "def feature_importance(models, feat_names, metrics):\n",
    "    \"\"\"Global feature importance weighted by segment size - NOW INCLUDES TOTAL_GAIN\"\"\"\n",
    "    rows = []\n",
    "    for seg, mdl in models.items():\n",
    "        scores = mdl['q50'].get_booster().get_score(importance_type=\"gain\")\n",
    "        w = metrics[seg][\"n_test\"]\n",
    "        for k,v in scores.items():\n",
    "            idx = int(k[1:])\n",
    "            if idx<len(feat_names): rows.append((feat_names[idx],v,w))\n",
    "    if not rows: return pd.DataFrame(columns=[\"feature\",\"total_gain\",\"importance\"])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"feature\",\"gain\",\"weight\"])\n",
    "\n",
    "    # Calculate both total_gain and weighted importance\n",
    "    out = df.assign(wg=df[\"gain\"]*df[\"weight\"]).groupby(\"feature\",as_index=False).agg(\n",
    "        total_gain=(\"gain\",\"sum\"),      # Sum of gain across all segments\n",
    "        weighted_gain=(\"wg\",\"sum\")      # Weighted by segment size\n",
    "    ).sort_values(\"weighted_gain\",ascending=False)\n",
    "\n",
    "    out[\"importance\"] = out[\"weighted_gain\"]/out[\"weighted_gain\"].sum()\n",
    "\n",
    "    return out[[\"feature\",\"total_gain\",\"importance\"]].head(100)\n",
    "\n",
    "def prepare_data(df, y_col, id_col, state_col):\n",
    "    print(f\"\\nPreparing data...\")\n",
    "    df = df[df[y_col]>=MIN_PRICE]\n",
    "    print(f\"{len(df):,} records after price filter\")\n",
    "    df = engineer(geo_cluster(df), y_col)\n",
    "    feat_groups = []\n",
    "    if INCLUDE_MLS: feat_groups.extend([BASE_FEATS,ENG_FEATS,PRIOR_FEATS,CLUSTER_FEATS])\n",
    "    if INCLUDE_CENSUS: feat_groups.append(CENSUS_FEATS)\n",
    "    if INCLUDE_NEIGHBORHOOD: feat_groups.append(ELECT_FEATS)\n",
    "    if INCLUDE_IMAGE: feat_groups.append(IMG_FEATS)\n",
    "    all_feats = [f for g in feat_groups for f in g]\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "    return df.dropna(subset=[y_col]), feats\n",
    "\n",
    "def train_segments(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining segmented models on {len(df):,} properties\")\n",
    "\n",
    "    # Simple 5-7 segment strategy\n",
    "    df['seg'] = assign_segments_simple(df)\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    print(f\"{len(seg_cnts)} segments created\")\n",
    "    for seg, cnt in seg_cnts.head(MAX_SEGMENTS).items(): print(f\"  {seg}: {cnt:,}\")\n",
    "\n",
    "    # Merge small segments\n",
    "    small = seg_cnts[seg_cnts<50].index\n",
    "    if len(small)>0:\n",
    "        df.loc[df['seg'].isin(small),'seg'] = 'other'\n",
    "        print(f\"Merged {len(small)} small segments into 'other'\")\n",
    "\n",
    "    # If still too many segments, merge smallest ones\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    if len(seg_cnts) > MAX_SEGMENTS:\n",
    "        keep_segs = seg_cnts.head(MAX_SEGMENTS-1).index\n",
    "        df.loc[~df['seg'].isin(keep_segs),'seg'] = 'other'\n",
    "        print(f\"Consolidated to {MAX_SEGMENTS} segments maximum\")\n",
    "\n",
    "    models, metrics, preds_list, segment_importances = {}, {}, [], {}\n",
    "\n",
    "    for seg in df['seg'].unique():\n",
    "        seg_df = df[df['seg']==seg].copy()\n",
    "        if len(seg_df)<50: continue\n",
    "        seg_df = filter_outliers(seg_df, seg, y_col)\n",
    "        if len(seg_df)<50: continue\n",
    "        train_idx = seg_df.sample(frac=1-TEST_SIZE,random_state=RAND_STATE).index\n",
    "        train_df, test_df = seg_df.loc[train_idx].copy(), seg_df.loc[seg_df.index.difference(train_idx)].copy()\n",
    "        train_df, test_df = add_cluster_feats(train_df, test_df, y_col)\n",
    "        X_tr,y_tr = train_df[feats].values, train_df[y_col].values\n",
    "        X_te,y_te = test_df[feats].values, test_df[y_col].values\n",
    "        ids_te = test_df[id_col].values\n",
    "        states_te = test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "        seg_models, seg_preds = {}, []\n",
    "        for q in QUANTILES:\n",
    "            m = train_model(X_tr,y_tr,q)\n",
    "            seg_models[f\"q{int(q*100)}\"] = m\n",
    "            seg_preds.append(m.predict(X_te))\n",
    "        models[seg] = seg_models\n",
    "\n",
    "        # Get feature importance for this segment\n",
    "        segment_importances[seg] = get_feature_importance_per_segment(seg_models['q50'], feats, top_n=20)\n",
    "\n",
    "        y_pred = seg_preds[1]\n",
    "        mae,mape = mean_absolute_error(y_te,y_pred), np.mean(np.abs((y_te-y_pred)/y_te))*100\n",
    "        r2,cov = r2_score(y_te,y_pred), np.mean((y_te>=seg_preds[0])&(y_te<=seg_preds[2]))*100\n",
    "        metrics[seg] = {'n_train':len(X_tr),'n_test':len(X_te),'mae':mae,'mape':mape,'r2':r2,'cov':cov,'p_min':seg_df[y_col].min(),'p_max':seg_df[y_col].max(),'p_med':seg_df[y_col].median()}\n",
    "        print(f\"  {seg}: {len(test_df):,} test | MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.3f}\")\n",
    "        preds_list.append(pd.DataFrame({'property_id':ids_te,'state':states_te,'actual':y_te,'predicted':y_pred,'pred_lower':seg_preds[0],'pred_upper':seg_preds[2],'segment':seg}))\n",
    "\n",
    "    return {\n",
    "        'models': models,\n",
    "        'metrics': metrics,\n",
    "        'predictions': pd.concat(preds_list),\n",
    "        'feature_importance': feature_importance(models, feats, metrics),\n",
    "        'segment_importances': segment_importances,\n",
    "        'feature_names': feats\n",
    "    }\n",
    "\n",
    "def save_results(results, out_dir):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi = results['predictions'], results['metrics'], results['feature_importance']\n",
    "    seg_importances = results['segment_importances']\n",
    "\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    # Summary\n",
    "    ws = wb.create_sheet(\"Summary\",0)\n",
    "    ws['A1'], ws['A1'].font = 'GRANULAR SEGMENTED AVM', Font(bold=True,size=14)\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    data = [['Metric','Value'],['Properties',len(preds)],['Segments',len(metrics)],['R²',f'{r2:.4f}'],['MAE',f'${mae:,.0f}'],['MAPE%',f'{mape:.2f}%']]\n",
    "    for i,(k,v) in enumerate(data,5): ws[f'A{i}'],ws[f'A{i}'].font,ws[f'B{i}'] = k,Font(bold=True),v\n",
    "\n",
    "    # Segments\n",
    "    ws = wb.create_sheet(\"Segments\")\n",
    "    seg_df = pd.DataFrame([{**{'Segment':s},**m} for s,m in metrics.items()])\n",
    "    for i,h in enumerate(seg_df.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(seg_df.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    # Global Feature Importance\n",
    "    ws = wb.create_sheet(\"Global_Feature_Importance\")\n",
    "    ws['A1'] = 'Global Feature Importance (Weighted)'\n",
    "    ws['A1'].font = Font(bold=True, size=12)\n",
    "    for r_idx, row in enumerate(dataframe_to_rows(fi, index=False, header=True), 2):\n",
    "        for c_idx, value in enumerate(row, 1):\n",
    "            cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "            if r_idx == 2:  # Header row\n",
    "                cell.font = Font(bold=True, color='FFFFFF')\n",
    "                cell.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "\n",
    "    # Per-Segment Feature Importance (one sheet per segment)\n",
    "    for seg_name, seg_fi in seg_importances.items():\n",
    "        sheet_name = f\"FI_{seg_name}\"[:31]  # Excel sheet name limit\n",
    "        ws = wb.create_sheet(sheet_name)\n",
    "        ws['A1'] = f'Feature Importance: {seg_name}'\n",
    "        ws['A1'].font = Font(bold=True, size=12)\n",
    "\n",
    "        for r_idx, row in enumerate(dataframe_to_rows(seg_fi, index=False, header=True), 2):\n",
    "            for c_idx, value in enumerate(row, 1):\n",
    "                cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "                if r_idx == 2:  # Header row\n",
    "                    cell.font = Font(bold=True, color='FFFFFF')\n",
    "                    cell.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "\n",
    "    # Predictions\n",
    "    ws = wb.create_sheet(\"Predictions\")\n",
    "    for i,h in enumerate(preds.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(preds.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/granular_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "    preds.to_csv(f\"{out_dir}/predictions_{ts}.csv\",index=False)\n",
    "    seg_df.to_csv(f\"{out_dir}/segments_{ts}.csv\",index=False)\n",
    "    fi.to_csv(f\"{out_dir}/global_importance_{ts}.csv\",index=False)\n",
    "\n",
    "    # Save per-segment feature importance CSVs\n",
    "    for seg_name, seg_fi in seg_importances.items():\n",
    "        seg_fi.to_csv(f\"{out_dir}/importance_{seg_name}_{ts}.csv\", index=False)\n",
    "\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved with timestamp: {ts}\")\n",
    "    print(f\"✓ Per-segment feature importance saved for {len(seg_importances)} segments\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60)\n",
    "    print(\"GRANULAR SEGMENTED AVM\")\n",
    "    print(\"=\"*60)\n",
    "    df, y_col, id_col, state_col = load_data(INPUT_PATH)\n",
    "    df, feats = prepare_data(df, y_col, id_col, state_col)\n",
    "    results = train_segments(df, feats, y_col, id_col, state_col)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR)\n",
    "    preds = results['predictions']\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ COMPLETE in {time.time()-t0:.1f}s\")\n",
    "    print(f\"  {len(preds):,} properties | {preds['segment'].nunique()} segments\")\n",
    "    print(f\"  R²: {r2:.4f} | MAE: ${mae:,.0f} | MAPE: {mape:.2f}%\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "if __name__ == \"__main__\": main()"
   ],
   "id": "6baab3dcb092f898",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRANULAR SEGMENTED AVM\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 129.6MB | Price:sale_price ID:property_id\n",
      "\n",
      "Preparing data...\n",
      "127,258 records after price filter\n",
      "48/57 features available\n",
      "\n",
      "Training segmented models on 127,258 properties\n",
      "6 segments created\n",
      "  budget_small: 23,028\n",
      "  mid_large: 22,880\n",
      "  premium_large: 21,935\n",
      "  mid_small: 20,638\n",
      "  premium_small: 19,967\n",
      "  budget_large: 18,810\n",
      "  premium_small: 19,967→15,479 (22.5% filtered)\n",
      "  premium_small: 4,644 test | MAE:$1,651,245 | MAPE:35.23% | R²:0.494\n",
      "  premium_large: 21,935→16,873 (23.1% filtered)\n",
      "  premium_large: 5,062 test | MAE:$632,201 | MAPE:21.25% | R²:0.404\n",
      "  mid_large: 22,880→17,671 (22.8% filtered)\n",
      "  mid_large: 5,301 test | MAE:$119,653 | MAPE:10.14% | R²:0.115\n",
      "  mid_small: 20,638→15,752 (23.7% filtered)\n",
      "  mid_small: 4,726 test | MAE:$113,877 | MAPE:9.52% | R²:0.165\n",
      "  budget_large: 18,810→14,526 (22.8% filtered)\n",
      "  budget_large: 4,358 test | MAE:$37,307 | MAPE:4.27% | R²:0.059\n",
      "  budget_small: 23,028→17,660 (23.3% filtered)\n",
      "  budget_small: 5,298 test | MAE:$39,311 | MAPE:4.56% | R²:0.027\n",
      "\n",
      "Saving results...\n",
      "✓ Excel: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/granular_20251223_140418.xlsx\n",
      "✓ CSVs saved with timestamp: 20251223_140418\n",
      "✓ Per-segment feature importance saved for 6 segments\n",
      "\n",
      "============================================================\n",
      "✓ COMPLETE in 9.5s\n",
      "  29,389 properties | 6 segments\n",
      "  R²: 0.6785 | MAE: $422,331 | MAPE: 14.04%\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T04:08:42.783323Z",
     "start_time": "2025-12-24T04:08:32.258504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import time, os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Config - FIXED PARAMETERS\n",
    "INCLUDE_MLS, INCLUDE_CENSUS, INCLUDE_NEIGHBORHOOD, INCLUDE_IMAGE = True, True, True, False\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS = 20000, 0.3, 42, -1, 8\n",
    "N_EST, QUANTILES, MAX_SEGMENTS = 100, [0.1, 0.5, 0.9], 7  # MAX 7 SEGMENTS\n",
    "\n",
    "# INPUT PATHS\n",
    "TRAINING_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "PREDICTION_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\"  # Set to None to skip\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "BASE_FEATS = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"effective_year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\"fireplace_code\",\"latitude\",\"longitude\",\"geo_cluster\"]\n",
    "ENG_FEATS = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\"]\n",
    "PRIOR_FEATS = [\"prior_sale_price\",\"prior_price_per_sqft\",\"sqft_per_prior_dollar\",\"years_since_last_sale\",\"expected_appreciation\",\"has_prior_sale\",\"recently_sold\"]\n",
    "CLUSTER_FEATS = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "CENSUS_FEATS = [\"total_population_25plus\",\"male_bachelors_degree\",\"female_bachelors_degree\",\"pct_bachelors_degree\",\"median_earnings_total\",\"median_earnings_male\",\"median_earnings_female\",\"median_household_income\",\"median_home_value\",\"median_gross_rent\",\"owner_occupied_units\",\"renter_occupied_units\",\"pct_owner_occupied\",\"occupied_units\",\"vacant_units\",\"median_age\",\"civilian_employed\",\"civilian_unemployed\",\"unemployment_rate\",\"income_education_score\"]\n",
    "ELECT_FEATS = [\"votes_gop\",\"votes_dem\",\"total_votes\",\"per_gop\",\"per_dem\",\"per_point_diff\",\"dem_margin\",\"rep_margin\"]\n",
    "IMG_FEATS = [\"topic_1\",\"topic_2\",\"topic_3\",\"topic_4\",\"topic_5\",\"topic_6\",\"topic_7\",\"topic_8\",\"topic_9\",\"topic_10\",\"gran_c_in\",\"gran_c_ex\",\"gran_c\",\"high_c_in\",\"high_c_ex\",\"high_c\"]\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8') if set(df[c].dropna().unique()).issubset({0,1}) else df[c].astype('int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return optimize_dtypes(df), y_col, id_col, state_col\n",
    "\n",
    "def assign_segments_simple(df):\n",
    "    \"\"\"Simple 5-7 segment strategy based on price and size\"\"\"\n",
    "    segments = pd.Series(['mid'] * len(df), index=df.index)\n",
    "\n",
    "    # Primary: Price tier (3 segments)\n",
    "    if 'currentsalesprice' in df.columns or 'sale_price' in df.columns:\n",
    "        price_col = 'currentsalesprice' if 'currentsalesprice' in df.columns else 'sale_price'\n",
    "        price = df[price_col].fillna(df[price_col].median())\n",
    "        q33, q67 = price.quantile([.33, .67])\n",
    "        price_tier = pd.Series(['mid'] * len(df), index=df.index)\n",
    "        price_tier[price < q33] = 'budget'\n",
    "        price_tier[price > q67] = 'premium'\n",
    "    else:\n",
    "        price_tier = pd.Series(['mid'] * len(df), index=df.index)\n",
    "\n",
    "    # Secondary: Size (2 segments: small vs large)\n",
    "    if 'living_sqft' in df.columns:\n",
    "        sqft = df['living_sqft'].fillna(df['living_sqft'].median())\n",
    "        sqft_median = sqft.median()\n",
    "        size_tier = pd.Series(['small'] * len(df), index=df.index)\n",
    "        size_tier[sqft > sqft_median] = 'large'\n",
    "    else:\n",
    "        size_tier = pd.Series(['small'] * len(df), index=df.index)\n",
    "\n",
    "    # Combine: price_size format (e.g., \"budget_small\", \"premium_large\")\n",
    "    segments = price_tier + '_' + size_tier\n",
    "\n",
    "    return segments\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    if 'living_sqft' in df.columns and 'bedrooms' in df.columns: df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns: df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns: df['lot_acres'] = df['lot_sqft']/43560\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'] = 2024-df['year_built']\n",
    "        df['is_new'] = (df['property_age']<=5).astype('int8')\n",
    "        df['age_squared'] = df['property_age']**2\n",
    "    if 'garage_spaces' in df.columns: df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "    if 'living_sqft' in df.columns: df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "    if INCLUDE_CENSUS and all(c in df.columns for c in ['median_household_income','pct_bachelors_degree']):\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "    if 'prior_sale_price' in df.columns and 'living_sqft' in df.columns:\n",
    "        df['prior_price_per_sqft'] = df['prior_sale_price']/(df['living_sqft']+1)\n",
    "        df['sqft_per_prior_dollar'] = df['living_sqft']/(df['prior_sale_price']+1)\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        if 'prior_sale_price' in df.columns:\n",
    "            df['expected_appreciation'] = df['prior_sale_price']*(1.04)**df['years_since_last_sale']\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df.loc[missing,'median_home_value'] if 'median_home_value' in df.columns and INCLUDE_CENSUS else df['prior_sale_price'].median()\n",
    "    if 'years_since_last_sale' in df.columns: df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "    if with_price and y_col in df.columns and 'living_sqft' in df.columns:\n",
    "        df['price_per_sqft'] = df[y_col]/(df['living_sqft']+1)\n",
    "        df['sqft_per_dollar'] = df['living_sqft']/(df[y_col]+1)\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df, kmeans_model=None):\n",
    "    \"\"\"Apply geo clustering - can use existing model for prediction\"\"\"\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans_model\n",
    "\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum() < N_CLUSTERS:\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans_model\n",
    "\n",
    "    df['geo_cluster'] = 0\n",
    "\n",
    "    if kmeans_model is None:\n",
    "        # Training: fit new model\n",
    "        kmeans_model = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "        df.loc[valid,'geo_cluster'] = kmeans_model.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    else:\n",
    "        # Prediction: use existing model\n",
    "        df.loc[valid,'geo_cluster'] = kmeans_model.predict(df.loc[valid,['latitude','longitude']])\n",
    "\n",
    "    return df, kmeans_model\n",
    "\n",
    "def add_cluster_feats(train, test, y_col):\n",
    "    if 'geo_cluster' not in train.columns or y_col not in train.columns:\n",
    "        for d in [train,test]: d['cluster_avg_price'] = d['cluster_med_price'] = train[y_col].median() if y_col in train.columns else 0\n",
    "        return train, test\n",
    "    stats = train.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "    stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "    train = train.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    test = test.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    return train, test\n",
    "\n",
    "def filter_outliers(df, name, y_col):\n",
    "    orig = len(df)\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)]\n",
    "    if 'sqft_per_dollar' in df.columns: df = df[df['sqft_per_dollar']<=df['sqft_per_dollar'].quantile(.95)]\n",
    "    if 'price_per_sqft' in df.columns: df = df.drop(columns=['price_per_sqft','sqft_per_dollar'])\n",
    "    if 'lot_sqft' in df.columns: df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "    if 'year_built' in df.columns: df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "    if len(df)>=100:\n",
    "        try:\n",
    "            feats = [c for c in ['living_sqft','lot_sqft',y_col,'year_built'] if c in df.columns]\n",
    "            if len(feats)>=3:\n",
    "                X = df[feats].fillna(df[feats].median())\n",
    "                df = df[IsolationForest(contamination=.05,random_state=RAND_STATE,n_jobs=N_JOBS).fit_predict(X)==1]\n",
    "        except: pass\n",
    "    pct_filt = (orig-len(df))/orig*100 if orig>0 else 0\n",
    "    if pct_filt > 0: print(f\"  {name}: {orig:,}→{len(df):,} ({pct_filt:.1f}% filtered)\")\n",
    "    return df\n",
    "\n",
    "def train_model(X, y, q):\n",
    "    return XGBRegressor(\n",
    "        objective='reg:quantileerror',\n",
    "        quantile_alpha=q,\n",
    "        n_estimators=N_EST,\n",
    "        learning_rate=.1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        subsample=.8,\n",
    "        colsample_bytree=.8,\n",
    "        random_state=RAND_STATE,\n",
    "        n_jobs=N_JOBS,\n",
    "        tree_method='hist'\n",
    "    ).fit(X, y, verbose=False)\n",
    "\n",
    "def get_feature_importance_per_segment(model, feat_names, top_n=20):\n",
    "    \"\"\"Extract feature importance for a single segment model\"\"\"\n",
    "    scores = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    importance_list = []\n",
    "    for k, v in scores.items():\n",
    "        idx = int(k[1:])\n",
    "        if idx < len(feat_names):\n",
    "            importance_list.append((feat_names[idx], v))\n",
    "\n",
    "    # Sort and normalize\n",
    "    importance_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    total_gain = sum(v for _, v in importance_list)\n",
    "\n",
    "    if total_gain > 0:\n",
    "        importance_df = pd.DataFrame([\n",
    "            {'feature': feat, 'gain': gain, 'importance': gain/total_gain}\n",
    "            for feat, gain in importance_list[:top_n]\n",
    "        ])\n",
    "    else:\n",
    "        importance_df = pd.DataFrame(columns=['feature', 'gain', 'importance'])\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "def feature_importance(models, feat_names, metrics):\n",
    "    \"\"\"Global feature importance weighted by segment size - NOW INCLUDES TOTAL_GAIN\"\"\"\n",
    "    rows = []\n",
    "    for seg, mdl in models.items():\n",
    "        scores = mdl['q50'].get_booster().get_score(importance_type=\"gain\")\n",
    "        w = metrics[seg][\"n_test\"]\n",
    "        for k,v in scores.items():\n",
    "            idx = int(k[1:])\n",
    "            if idx<len(feat_names): rows.append((feat_names[idx],v,w))\n",
    "    if not rows: return pd.DataFrame(columns=[\"feature\",\"total_gain\",\"importance\"])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"feature\",\"gain\",\"weight\"])\n",
    "\n",
    "    # Calculate both total_gain and weighted importance\n",
    "    out = df.assign(wg=df[\"gain\"]*df[\"weight\"]).groupby(\"feature\",as_index=False).agg(\n",
    "        total_gain=(\"gain\",\"sum\"),      # Sum of gain across all segments\n",
    "        weighted_gain=(\"wg\",\"sum\")      # Weighted by segment size\n",
    "    ).sort_values(\"weighted_gain\",ascending=False)\n",
    "\n",
    "    out[\"importance\"] = out[\"weighted_gain\"]/out[\"weighted_gain\"].sum()\n",
    "\n",
    "    return out[[\"feature\",\"total_gain\",\"importance\"]].head(100)\n",
    "\n",
    "def prepare_data(df, y_col, id_col, state_col, for_training=True):\n",
    "    print(f\"\\nPreparing data...\")\n",
    "    if for_training:\n",
    "        df = df[df[y_col]>=MIN_PRICE]\n",
    "        print(f\"{len(df):,} records after price filter\")\n",
    "    df, kmeans_model = geo_cluster(engineer(df, y_col))\n",
    "    feat_groups = []\n",
    "    if INCLUDE_MLS: feat_groups.extend([BASE_FEATS,ENG_FEATS,PRIOR_FEATS,CLUSTER_FEATS])\n",
    "    if INCLUDE_CENSUS: feat_groups.append(CENSUS_FEATS)\n",
    "    if INCLUDE_NEIGHBORHOOD: feat_groups.append(ELECT_FEATS)\n",
    "    if INCLUDE_IMAGE: feat_groups.append(IMG_FEATS)\n",
    "    all_feats = [f for g in feat_groups for f in g]\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "    if for_training:\n",
    "        return df.dropna(subset=[y_col]), feats, kmeans_model\n",
    "    else:\n",
    "        return df, feats, kmeans_model\n",
    "\n",
    "def train_segments(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining segmented models on {len(df):,} properties\")\n",
    "\n",
    "    # Simple 5-7 segment strategy\n",
    "    df['seg'] = assign_segments_simple(df)\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    print(f\"{len(seg_cnts)} segments created\")\n",
    "    for seg, cnt in seg_cnts.head(MAX_SEGMENTS).items(): print(f\"  {seg}: {cnt:,}\")\n",
    "\n",
    "    # Merge small segments\n",
    "    small = seg_cnts[seg_cnts<50].index\n",
    "    if len(small)>0:\n",
    "        df.loc[df['seg'].isin(small),'seg'] = 'other'\n",
    "        print(f\"Merged {len(small)} small segments into 'other'\")\n",
    "\n",
    "    # If still too many segments, merge smallest ones\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    if len(seg_cnts) > MAX_SEGMENTS:\n",
    "        keep_segs = seg_cnts.head(MAX_SEGMENTS-1).index\n",
    "        df.loc[~df['seg'].isin(keep_segs),'seg'] = 'other'\n",
    "        print(f\"Consolidated to {MAX_SEGMENTS} segments maximum\")\n",
    "\n",
    "    models, metrics, preds_list, segment_importances = {}, {}, [], {}\n",
    "\n",
    "    for seg in df['seg'].unique():\n",
    "        seg_df = df[df['seg']==seg].copy()\n",
    "        if len(seg_df)<50: continue\n",
    "        seg_df = filter_outliers(seg_df, seg, y_col)\n",
    "        if len(seg_df)<50: continue\n",
    "        train_idx = seg_df.sample(frac=1-TEST_SIZE,random_state=RAND_STATE).index\n",
    "        train_df, test_df = seg_df.loc[train_idx].copy(), seg_df.loc[seg_df.index.difference(train_idx)].copy()\n",
    "        train_df, test_df = add_cluster_feats(train_df, test_df, y_col)\n",
    "        X_tr,y_tr = train_df[feats].values, train_df[y_col].values\n",
    "        X_te,y_te = test_df[feats].values, test_df[y_col].values\n",
    "        ids_te = test_df[id_col].values\n",
    "        states_te = test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "        seg_models, seg_preds = {}, []\n",
    "        for q in QUANTILES:\n",
    "            m = train_model(X_tr,y_tr,q)\n",
    "            seg_models[f\"q{int(q*100)}\"] = m\n",
    "            seg_preds.append(m.predict(X_te))\n",
    "        models[seg] = seg_models\n",
    "\n",
    "        # Get feature importance for this segment\n",
    "        segment_importances[seg] = get_feature_importance_per_segment(seg_models['q50'], feats, top_n=20)\n",
    "\n",
    "        y_pred = seg_preds[1]\n",
    "        mae,mape = mean_absolute_error(y_te,y_pred), np.mean(np.abs((y_te-y_pred)/y_te))*100\n",
    "        r2,cov = r2_score(y_te,y_pred), np.mean((y_te>=seg_preds[0])&(y_te<=seg_preds[2]))*100\n",
    "        metrics[seg] = {'n_train':len(X_tr),'n_test':len(X_te),'mae':mae,'mape':mape,'r2':r2,'cov':cov,'p_min':seg_df[y_col].min(),'p_max':seg_df[y_col].max(),'p_med':seg_df[y_col].median()}\n",
    "        print(f\"  {seg}: {len(test_df):,} test | MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.3f}\")\n",
    "        preds_list.append(pd.DataFrame({'property_id':ids_te,'state':states_te,'actual':y_te,'predicted':y_pred,'pred_lower':seg_preds[0],'pred_upper':seg_preds[2],'segment':seg}))\n",
    "\n",
    "    return {\n",
    "        'models': models,\n",
    "        'metrics': metrics,\n",
    "        'predictions': pd.concat(preds_list),\n",
    "        'feature_importance': feature_importance(models, feats, metrics),\n",
    "        'segment_importances': segment_importances,\n",
    "        'feature_names': feats\n",
    "    }\n",
    "\n",
    "def predict_new_properties(pred_df, models, feats, y_col, id_col, state_col, kmeans_model, train_cluster_stats):\n",
    "    \"\"\"Generate predictions for new properties using trained models\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"GENERATING PREDICTIONS FOR NEW PROPERTIES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Input properties: {len(pred_df):,}\")\n",
    "\n",
    "    # Engineer features (no price filtering for prediction)\n",
    "    pred_df = engineer(pred_df, y_col)\n",
    "    pred_df, _ = geo_cluster(pred_df, kmeans_model)\n",
    "\n",
    "    # Add cluster features from training data\n",
    "    if 'geo_cluster' in pred_df.columns and train_cluster_stats is not None:\n",
    "        pred_df = pred_df.merge(train_cluster_stats, on='geo_cluster', how='left')\n",
    "        median_price = train_cluster_stats['cluster_avg_price'].median()\n",
    "        pred_df['cluster_avg_price'] = pred_df['cluster_avg_price'].fillna(median_price)\n",
    "        pred_df['cluster_med_price'] = pred_df['cluster_med_price'].fillna(median_price)\n",
    "\n",
    "    # Fill missing features with median or 0\n",
    "    for feat in feats:\n",
    "        if feat not in pred_df.columns:\n",
    "            pred_df[feat] = 0\n",
    "        else:\n",
    "            pred_df[feat] = pred_df[feat].fillna(pred_df[feat].median() if pred_df[feat].notna().sum() > 0 else 0)\n",
    "\n",
    "    # Assign segments\n",
    "    pred_df['seg'] = assign_segments_simple(pred_df)\n",
    "\n",
    "    # Generate predictions\n",
    "    preds_list = []\n",
    "    for seg in pred_df['seg'].unique():\n",
    "        seg_df = pred_df[pred_df['seg']==seg].copy()\n",
    "\n",
    "        if seg not in models:\n",
    "            print(f\"  Warning: Segment '{seg}' not in trained models, using 'mid_large' as fallback\")\n",
    "            seg = 'mid_large' if 'mid_large' in models else list(models.keys())[0]\n",
    "\n",
    "        X = seg_df[feats].values\n",
    "        ids = seg_df[id_col].values\n",
    "        states = seg_df[state_col].values if state_col and state_col in seg_df.columns else ['Unknown']*len(seg_df)\n",
    "\n",
    "        # Get predictions from quantile models\n",
    "        pred_lower = models[seg]['q10'].predict(X)\n",
    "        pred_mid = models[seg]['q50'].predict(X)\n",
    "        pred_upper = models[seg]['q90'].predict(X)\n",
    "\n",
    "        actual = seg_df[y_col].values if y_col in seg_df.columns else [np.nan]*len(seg_df)\n",
    "\n",
    "        preds_list.append(pd.DataFrame({\n",
    "            'property_id': ids,\n",
    "            'state': states,\n",
    "            'actual': actual,\n",
    "            'predicted': pred_mid,\n",
    "            'pred_lower': pred_lower,\n",
    "            'pred_upper': pred_upper,\n",
    "            'segment': seg,\n",
    "            'error': [actual[i] - pred_mid[i] if not np.isnan(actual[i]) else np.nan for i in range(len(actual))],\n",
    "            'pct_error': [100 * (actual[i] - pred_mid[i]) / actual[i] if not np.isnan(actual[i]) and actual[i] != 0 else np.nan for i in range(len(actual))]\n",
    "        }))\n",
    "\n",
    "        print(f\"  {seg}: {len(seg_df):,} properties predicted\")\n",
    "\n",
    "    result_df = pd.concat(preds_list, ignore_index=True)\n",
    "    print(f\"\\n✓ Generated {len(result_df):,} predictions\")\n",
    "\n",
    "    # Calculate metrics if actuals are available\n",
    "    valid_actuals = result_df['actual'].notna().sum()\n",
    "    if valid_actuals > 0:\n",
    "        valid_preds = result_df[result_df['actual'].notna()].copy()\n",
    "        mae = mean_absolute_error(valid_preds['actual'], valid_preds['predicted'])\n",
    "        mape = np.mean(np.abs((valid_preds['actual'] - valid_preds['predicted']) / valid_preds['actual'])) * 100\n",
    "        r2 = r2_score(valid_preds['actual'], valid_preds['predicted'])\n",
    "        print(f\"  Validation metrics ({valid_actuals} properties):\")\n",
    "        print(f\"  MAE: ${mae:,.0f} | MAPE: {mape:.2f}% | R²: {r2:.4f}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def save_results(results, out_dir, new_predictions=None):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi = results['predictions'], results['metrics'], results['feature_importance']\n",
    "    seg_importances = results['segment_importances']\n",
    "\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    # Summary\n",
    "    ws = wb.create_sheet(\"Summary\",0)\n",
    "    ws['A1'], ws['A1'].font = 'GRANULAR SEGMENTED AVM', Font(bold=True,size=14)\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    data = [['Metric','Value'],['Properties',len(preds)],['Segments',len(metrics)],['R²',f'{r2:.4f}'],['MAE',f'${mae:,.0f}'],['MAPE%',f'{mape:.2f}%']]\n",
    "    if new_predictions is not None:\n",
    "        data.append(['New Predictions', len(new_predictions)])\n",
    "    for i,(k,v) in enumerate(data,5): ws[f'A{i}'],ws[f'A{i}'].font,ws[f'B{i}'] = k,Font(bold=True),v\n",
    "\n",
    "    # Segments\n",
    "    ws = wb.create_sheet(\"Segments\")\n",
    "    seg_df = pd.DataFrame([{**{'Segment':s},**m} for s,m in metrics.items()])\n",
    "    for i,h in enumerate(seg_df.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(seg_df.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    # Global Feature Importance\n",
    "    ws = wb.create_sheet(\"Global_Feature_Importance\")\n",
    "    ws['A1'] = 'Global Feature Importance (Weighted)'\n",
    "    ws['A1'].font = Font(bold=True, size=12)\n",
    "    for r_idx, row in enumerate(dataframe_to_rows(fi, index=False, header=True), 2):\n",
    "        for c_idx, value in enumerate(row, 1):\n",
    "            cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "            if r_idx == 2:  # Header row\n",
    "                cell.font = Font(bold=True, color='FFFFFF')\n",
    "                cell.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "\n",
    "    # Per-Segment Feature Importance (one sheet per segment)\n",
    "    for seg_name, seg_fi in seg_importances.items():\n",
    "        sheet_name = f\"FI_{seg_name}\"[:31]  # Excel sheet name limit\n",
    "        ws = wb.create_sheet(sheet_name)\n",
    "        ws['A1'] = f'Feature Importance: {seg_name}'\n",
    "        ws['A1'].font = Font(bold=True, size=12)\n",
    "\n",
    "        for r_idx, row in enumerate(dataframe_to_rows(seg_fi, index=False, header=True), 2):\n",
    "            for c_idx, value in enumerate(row, 1):\n",
    "                cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "                if r_idx == 2:  # Header row\n",
    "                    cell.font = Font(bold=True, color='FFFFFF')\n",
    "                    cell.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "\n",
    "    # Test Set Predictions\n",
    "    ws = wb.create_sheet(\"Test_Predictions\")\n",
    "    for i,h in enumerate(preds.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(preds.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    # New Predictions (if provided)\n",
    "    if new_predictions is not None:\n",
    "        ws = wb.create_sheet(\"New_Predictions\")\n",
    "        for i,h in enumerate(new_predictions.columns,1):\n",
    "            c = ws.cell(1,i,h)\n",
    "            c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='4472C4',end_color='4472C4',fill_type='solid')\n",
    "        for i,row in enumerate(new_predictions.itertuples(index=False),2):\n",
    "            for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/granular_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "    preds.to_csv(f\"{out_dir}/test_predictions_{ts}.csv\",index=False)\n",
    "    seg_df.to_csv(f\"{out_dir}/segments_{ts}.csv\",index=False)\n",
    "    fi.to_csv(f\"{out_dir}/global_importance_{ts}.csv\",index=False)\n",
    "\n",
    "    # Save per-segment feature importance CSVs\n",
    "    for seg_name, seg_fi in seg_importances.items():\n",
    "        seg_fi.to_csv(f\"{out_dir}/importance_{seg_name}_{ts}.csv\", index=False)\n",
    "\n",
    "    # Save new predictions\n",
    "    if new_predictions is not None:\n",
    "        new_predictions.to_csv(f\"{out_dir}/new_predictions_{ts}.csv\", index=False)\n",
    "        print(f\"✓ New predictions CSV: {out_dir}/new_predictions_{ts}.csv\")\n",
    "\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved with timestamp: {ts}\")\n",
    "    print(f\"✓ Per-segment feature importance saved for {len(seg_importances)} segments\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60)\n",
    "    print(\"GRANULAR SEGMENTED AVM\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Load and prepare training data\n",
    "    df, y_col, id_col, state_col = load_data(TRAINING_INPUT_PATH)\n",
    "    df, feats, kmeans_model = prepare_data(df, y_col, id_col, state_col, for_training=True)\n",
    "\n",
    "    # Train models\n",
    "    results = train_segments(df, feats, y_col, id_col, state_col)\n",
    "\n",
    "    # Get cluster stats for prediction data\n",
    "    train_cluster_stats = None\n",
    "    if 'geo_cluster' in df.columns:\n",
    "        train_cluster_stats = df.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "        train_cluster_stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "\n",
    "    # Generate predictions for new properties if input file provided\n",
    "    new_predictions = None\n",
    "    if PREDICTION_INPUT_PATH:\n",
    "        pred_df, _, _, _ = load_data(PREDICTION_INPUT_PATH)\n",
    "        new_predictions = predict_new_properties(\n",
    "            pred_df, results['models'], feats, y_col, id_col, state_col,\n",
    "            kmeans_model, train_cluster_stats\n",
    "        )\n",
    "\n",
    "    # Save results\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR, new_predictions)\n",
    "\n",
    "    # Print summary\n",
    "    preds = results['predictions']\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ TRAINING COMPLETE in {time.time()-t0:.1f}s\")\n",
    "    print(f\"  Test set: {len(preds):,} properties | {preds['segment'].nunique()} segments\")\n",
    "    print(f\"  R²: {r2:.4f} | MAE: ${mae:,.0f} | MAPE: {mape:.2f}%\")\n",
    "    if new_predictions is not None:\n",
    "        print(f\"  New predictions: {len(new_predictions):,} properties\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "if __name__ == \"__main__\": main()"
   ],
   "id": "a4777571ea654808",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRANULAR SEGMENTED AVM\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 129.6MB | Price:sale_price ID:property_id\n",
      "\n",
      "Preparing data...\n",
      "127,258 records after price filter\n",
      "48/57 features available\n",
      "\n",
      "Training segmented models on 127,258 properties\n",
      "6 segments created\n",
      "  budget_small: 23,028\n",
      "  mid_large: 22,880\n",
      "  premium_large: 21,935\n",
      "  mid_small: 20,638\n",
      "  premium_small: 19,967\n",
      "  budget_large: 18,810\n",
      "  premium_small: 19,967→15,479 (22.5% filtered)\n",
      "  premium_small: 4,644 test | MAE:$1,651,245 | MAPE:35.23% | R²:0.494\n",
      "  premium_large: 21,935→16,873 (23.1% filtered)\n",
      "  premium_large: 5,062 test | MAE:$632,201 | MAPE:21.25% | R²:0.404\n",
      "  mid_large: 22,880→17,671 (22.8% filtered)\n",
      "  mid_large: 5,301 test | MAE:$119,653 | MAPE:10.14% | R²:0.115\n",
      "  mid_small: 20,638→15,752 (23.7% filtered)\n",
      "  mid_small: 4,726 test | MAE:$113,877 | MAPE:9.52% | R²:0.165\n",
      "  budget_large: 18,810→14,526 (22.8% filtered)\n",
      "  budget_large: 4,358 test | MAE:$37,307 | MAPE:4.27% | R²:0.059\n",
      "  budget_small: 23,028→17,660 (23.3% filtered)\n",
      "  budget_small: 5,298 test | MAE:$39,311 | MAPE:4.56% | R²:0.027\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\n",
      "1 records | 0.0MB | Price:sale_price ID:property_id\n",
      "\n",
      "============================================================\n",
      "GENERATING PREDICTIONS FOR NEW PROPERTIES\n",
      "============================================================\n",
      "Input properties: 1\n",
      "  mid_small: 1 properties predicted\n",
      "\n",
      "✓ Generated 1 predictions\n",
      "  Validation metrics (1 properties):\n",
      "  MAE: $616,927 | MAPE: 117.29% | R²: nan\n",
      "\n",
      "Saving results...\n",
      "✓ New predictions CSV: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/new_predictions_20251223_230841.csv\n",
      "✓ Excel: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/granular_20251223_230841.xlsx\n",
      "✓ CSVs saved with timestamp: 20251223_230841\n",
      "✓ Per-segment feature importance saved for 6 segments\n",
      "\n",
      "============================================================\n",
      "✓ TRAINING COMPLETE in 10.5s\n",
      "  Test set: 29,389 properties | 6 segments\n",
      "  R²: 0.6785 | MAE: $422,331 | MAPE: 14.04%\n",
      "  New predictions: 1 properties\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T04:15:30.808858Z",
     "start_time": "2025-12-24T04:15:20.343025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import time, os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Config - LOCATION-FOCUSED VERSION\n",
    "INCLUDE_MLS, INCLUDE_CENSUS, INCLUDE_NEIGHBORHOOD, INCLUDE_IMAGE = True, True, True, False\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS = 20000, 0.3, 42, -1, 16  # INCREASED clusters from 8 to 16\n",
    "N_EST, QUANTILES, MAX_SEGMENTS = 100, [0.1, 0.5, 0.9], 7\n",
    "LOCATION_WEIGHT = 3.0  # NEW: Weight multiplier for location features\n",
    "\n",
    "# INPUT PATHS\n",
    "TRAINING_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "PREDICTION_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "BASE_FEATS = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"effective_year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\"fireplace_code\",\"latitude\",\"longitude\",\"geo_cluster\"]\n",
    "ENG_FEATS = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\"]\n",
    "PRIOR_FEATS = [\"prior_sale_price\",\"prior_price_per_sqft\",\"sqft_per_prior_dollar\",\"years_since_last_sale\",\"expected_appreciation\",\"has_prior_sale\",\"recently_sold\"]\n",
    "CLUSTER_FEATS = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "CENSUS_FEATS = [\"total_population_25plus\",\"male_bachelors_degree\",\"female_bachelors_degree\",\"pct_bachelors_degree\",\"median_earnings_total\",\"median_earnings_male\",\"median_earnings_female\",\"median_household_income\",\"median_home_value\",\"median_gross_rent\",\"owner_occupied_units\",\"renter_occupied_units\",\"pct_owner_occupied\",\"occupied_units\",\"vacant_units\",\"median_age\",\"civilian_employed\",\"civilian_unemployed\",\"unemployment_rate\",\"income_education_score\"]\n",
    "ELECT_FEATS = [\"votes_gop\",\"votes_dem\",\"total_votes\",\"per_gop\",\"per_dem\",\"per_point_diff\",\"dem_margin\",\"rep_margin\"]\n",
    "IMG_FEATS = [\"topic_1\",\"topic_2\",\"topic_3\",\"topic_4\",\"topic_5\",\"topic_6\",\"topic_7\",\"topic_8\",\"topic_9\",\"topic_10\",\"gran_c_in\",\"gran_c_ex\",\"gran_c\",\"high_c_in\",\"high_c_ex\",\"high_c\"]\n",
    "\n",
    "# NEW: Location-based features to create\n",
    "LOCATION_FEATS = [\"lat_long_interaction\", \"distance_to_center\", \"neighborhood_density\"]\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8') if set(df[c].dropna().unique()).issubset({0,1}) else df[c].astype('int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return optimize_dtypes(df), y_col, id_col, state_col\n",
    "\n",
    "def assign_segments_simple(df):\n",
    "    \"\"\"Simple 5-7 segment strategy based on price and size\"\"\"\n",
    "    segments = pd.Series(['mid'] * len(df), index=df.index)\n",
    "\n",
    "    # Primary: Price tier (3 segments)\n",
    "    if 'currentsalesprice' in df.columns or 'sale_price' in df.columns:\n",
    "        price_col = 'currentsalesprice' if 'currentsalesprice' in df.columns else 'sale_price'\n",
    "        price = df[price_col].fillna(df[price_col].median())\n",
    "        q33, q67 = price.quantile([.33, .67])\n",
    "        price_tier = pd.Series(['mid'] * len(df), index=df.index)\n",
    "        price_tier[price < q33] = 'budget'\n",
    "        price_tier[price > q67] = 'premium'\n",
    "    else:\n",
    "        price_tier = pd.Series(['mid'] * len(df), index=df.index)\n",
    "\n",
    "    # Secondary: Size (2 segments: small vs large)\n",
    "    if 'living_sqft' in df.columns:\n",
    "        sqft = df['living_sqft'].fillna(df['living_sqft'].median())\n",
    "        sqft_median = sqft.median()\n",
    "        size_tier = pd.Series(['small'] * len(df), index=df.index)\n",
    "        size_tier[sqft > sqft_median] = 'large'\n",
    "    else:\n",
    "        size_tier = pd.Series(['small'] * len(df), index=df.index)\n",
    "\n",
    "    # Combine: price_size format (e.g., \"budget_small\", \"premium_large\")\n",
    "    segments = price_tier + '_' + size_tier\n",
    "\n",
    "    return segments\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    # Standard features\n",
    "    if 'living_sqft' in df.columns and 'bedrooms' in df.columns: df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns: df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns: df['lot_acres'] = df['lot_sqft']/43560\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'] = 2024-df['year_built']\n",
    "        df['is_new'] = (df['property_age']<=5).astype('int8')\n",
    "        df['age_squared'] = df['property_age']**2\n",
    "    if 'garage_spaces' in df.columns: df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "    if 'living_sqft' in df.columns: df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "    if INCLUDE_CENSUS and all(c in df.columns for c in ['median_household_income','pct_bachelors_degree']):\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "    if 'prior_sale_price' in df.columns and 'living_sqft' in df.columns:\n",
    "        df['prior_price_per_sqft'] = df['prior_sale_price']/(df['living_sqft']+1)\n",
    "        df['sqft_per_prior_dollar'] = df['living_sqft']/(df['prior_sale_price']+1)\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        if 'prior_sale_price' in df.columns:\n",
    "            df['expected_appreciation'] = df['prior_sale_price']*(1.04)**df['years_since_last_sale']\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df.loc[missing,'median_home_value'] if 'median_home_value' in df.columns and INCLUDE_CENSUS else df['prior_sale_price'].median()\n",
    "    if 'years_since_last_sale' in df.columns: df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "\n",
    "    # ====================================================================\n",
    "    # NEW: LOCATION-ENHANCED FEATURES\n",
    "    # ====================================================================\n",
    "    if all(c in df.columns for c in ['latitude', 'longitude']):\n",
    "        # Interaction term\n",
    "        df['lat_long_interaction'] = df['latitude'] * df['longitude']\n",
    "\n",
    "        # Distance to geographic center of dataset\n",
    "        lat_center = df['latitude'].median()\n",
    "        lon_center = df['longitude'].median()\n",
    "        df['distance_to_center'] = np.sqrt(\n",
    "            (df['latitude'] - lat_center)**2 + (df['longitude'] - lon_center)**2\n",
    "        )\n",
    "\n",
    "        # Neighborhood density (properties within ~0.01 degree radius)\n",
    "        # Approximation: count nearby properties\n",
    "        df['neighborhood_density'] = 0\n",
    "        if len(df) < 10000:  # Only for reasonable dataset sizes\n",
    "            try:\n",
    "                from scipy.spatial import cKDTree\n",
    "                coords = df[['latitude', 'longitude']].values\n",
    "                tree = cKDTree(coords)\n",
    "                # Count properties within ~0.5 mile (roughly 0.01 degrees)\n",
    "                counts = tree.query_ball_point(coords, r=0.01, return_length=True)\n",
    "                df['neighborhood_density'] = counts\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    if with_price and y_col in df.columns and 'living_sqft' in df.columns:\n",
    "        df['price_per_sqft'] = df[y_col]/(df['living_sqft']+1)\n",
    "        df['sqft_per_dollar'] = df['living_sqft']/(df[y_col]+1)\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df, kmeans_model=None):\n",
    "    \"\"\"Apply geo clustering with MORE clusters for finer location granularity\"\"\"\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans_model\n",
    "\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum() < N_CLUSTERS:\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans_model\n",
    "\n",
    "    df['geo_cluster'] = 0\n",
    "\n",
    "    if kmeans_model is None:\n",
    "        # Training: fit new model with MORE clusters\n",
    "        kmeans_model = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "        df.loc[valid,'geo_cluster'] = kmeans_model.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    else:\n",
    "        # Prediction: use existing model\n",
    "        df.loc[valid,'geo_cluster'] = kmeans_model.predict(df.loc[valid,['latitude','longitude']])\n",
    "\n",
    "    return df, kmeans_model\n",
    "\n",
    "def add_cluster_feats(train, test, y_col):\n",
    "    if 'geo_cluster' not in train.columns or y_col not in train.columns:\n",
    "        for d in [train,test]: d['cluster_avg_price'] = d['cluster_med_price'] = train[y_col].median() if y_col in train.columns else 0\n",
    "        return train, test\n",
    "    stats = train.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "    stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "    train = train.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    test = test.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    return train, test\n",
    "\n",
    "def filter_outliers(df, name, y_col):\n",
    "    orig = len(df)\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)]\n",
    "    if 'sqft_per_dollar' in df.columns: df = df[df['sqft_per_dollar']<=df['sqft_per_dollar'].quantile(.95)]\n",
    "    if 'price_per_sqft' in df.columns: df = df.drop(columns=['price_per_sqft','sqft_per_dollar'])\n",
    "    if 'lot_sqft' in df.columns: df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "    if 'year_built' in df.columns: df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "    if len(df)>=100:\n",
    "        try:\n",
    "            feats = [c for c in ['living_sqft','lot_sqft',y_col,'year_built'] if c in df.columns]\n",
    "            if len(feats)>=3:\n",
    "                X = df[feats].fillna(df[feats].median())\n",
    "                df = df[IsolationForest(contamination=.05,random_state=RAND_STATE,n_jobs=N_JOBS).fit_predict(X)==1]\n",
    "        except: pass\n",
    "    pct_filt = (orig-len(df))/orig*100 if orig>0 else 0\n",
    "    if pct_filt > 0: print(f\"  {name}: {orig:,}→{len(df):,} ({pct_filt:.1f}% filtered)\")\n",
    "    return df\n",
    "\n",
    "def apply_feature_weights(X, feat_names, location_weight=LOCATION_WEIGHT):\n",
    "    \"\"\"\n",
    "    Apply higher weights to location features by duplicating them in the feature matrix.\n",
    "    This gives XGBoost more opportunities to split on location features.\n",
    "    \"\"\"\n",
    "    X_weighted = X.copy()\n",
    "    location_feature_indices = []\n",
    "\n",
    "    # Identify location-related features\n",
    "    location_keywords = ['latitude', 'longitude', 'geo_cluster', 'cluster_avg_price',\n",
    "                        'cluster_med_price', 'lat_long_interaction', 'distance_to_center',\n",
    "                        'neighborhood_density']\n",
    "\n",
    "    for i, feat in enumerate(feat_names):\n",
    "        if any(keyword in feat for keyword in location_keywords):\n",
    "            location_feature_indices.append(i)\n",
    "\n",
    "    # If using sample weights approach (alternative method)\n",
    "    # We'll return the original X but track which features are location-based\n",
    "    return X_weighted, location_feature_indices\n",
    "\n",
    "def train_model(X, y, q, feat_names):\n",
    "    \"\"\"\n",
    "    Train model with emphasis on location features via feature_weights parameter\n",
    "    \"\"\"\n",
    "    # Create feature weights: higher for location features\n",
    "    feature_weights = np.ones(X.shape[1])\n",
    "\n",
    "    location_keywords = ['latitude', 'longitude', 'geo_cluster', 'cluster_avg_price',\n",
    "                        'cluster_med_price', 'lat_long_interaction', 'distance_to_center',\n",
    "                        'neighborhood_density']\n",
    "\n",
    "    for i, feat in enumerate(feat_names):\n",
    "        if any(keyword in feat for keyword in location_keywords):\n",
    "            feature_weights[i] = LOCATION_WEIGHT\n",
    "\n",
    "    # XGBoost with feature weights\n",
    "    model = XGBRegressor(\n",
    "        objective='reg:quantileerror',\n",
    "        quantile_alpha=q,\n",
    "        n_estimators=N_EST,\n",
    "        learning_rate=.1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        subsample=.8,\n",
    "        colsample_bytree=.8,\n",
    "        random_state=RAND_STATE,\n",
    "        n_jobs=N_JOBS,\n",
    "        tree_method='hist',\n",
    "        feature_weights=feature_weights  # NEW: Apply location weights\n",
    "    )\n",
    "\n",
    "    return model.fit(X, y, verbose=False)\n",
    "\n",
    "def get_feature_importance_per_segment(model, feat_names, top_n=20):\n",
    "    \"\"\"Extract feature importance for a single segment model\"\"\"\n",
    "    scores = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    importance_list = []\n",
    "    for k, v in scores.items():\n",
    "        idx = int(k[1:])\n",
    "        if idx < len(feat_names):\n",
    "            importance_list.append((feat_names[idx], v))\n",
    "\n",
    "    # Sort and normalize\n",
    "    importance_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    total_gain = sum(v for _, v in importance_list)\n",
    "\n",
    "    if total_gain > 0:\n",
    "        importance_df = pd.DataFrame([\n",
    "            {'feature': feat, 'gain': gain, 'importance': gain/total_gain}\n",
    "            for feat, gain in importance_list[:top_n]\n",
    "        ])\n",
    "    else:\n",
    "        importance_df = pd.DataFrame(columns=['feature', 'gain', 'importance'])\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "def feature_importance(models, feat_names, metrics):\n",
    "    \"\"\"Global feature importance weighted by segment size\"\"\"\n",
    "    rows = []\n",
    "    for seg, mdl in models.items():\n",
    "        scores = mdl['q50'].get_booster().get_score(importance_type=\"gain\")\n",
    "        w = metrics[seg][\"n_test\"]\n",
    "        for k,v in scores.items():\n",
    "            idx = int(k[1:])\n",
    "            if idx<len(feat_names): rows.append((feat_names[idx],v,w))\n",
    "    if not rows: return pd.DataFrame(columns=[\"feature\",\"total_gain\",\"importance\"])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"feature\",\"gain\",\"weight\"])\n",
    "\n",
    "    # Calculate both total_gain and weighted importance\n",
    "    out = df.assign(wg=df[\"gain\"]*df[\"weight\"]).groupby(\"feature\",as_index=False).agg(\n",
    "        total_gain=(\"gain\",\"sum\"),\n",
    "        weighted_gain=(\"wg\",\"sum\")\n",
    "    ).sort_values(\"weighted_gain\",ascending=False)\n",
    "\n",
    "    out[\"importance\"] = out[\"weighted_gain\"]/out[\"weighted_gain\"].sum()\n",
    "\n",
    "    return out[[\"feature\",\"total_gain\",\"importance\"]].head(100)\n",
    "\n",
    "def prepare_data(df, y_col, id_col, state_col, for_training=True):\n",
    "    print(f\"\\nPreparing data...\")\n",
    "    if for_training:\n",
    "        df = df[df[y_col]>=MIN_PRICE]\n",
    "        print(f\"{len(df):,} records after price filter\")\n",
    "    df, kmeans_model = geo_cluster(engineer(df, y_col))\n",
    "    feat_groups = []\n",
    "    if INCLUDE_MLS: feat_groups.extend([BASE_FEATS,ENG_FEATS,PRIOR_FEATS,CLUSTER_FEATS,LOCATION_FEATS])  # ADDED LOCATION_FEATS\n",
    "    if INCLUDE_CENSUS: feat_groups.append(CENSUS_FEATS)\n",
    "    if INCLUDE_NEIGHBORHOOD: feat_groups.append(ELECT_FEATS)\n",
    "    if INCLUDE_IMAGE: feat_groups.append(IMG_FEATS)\n",
    "    all_feats = [f for g in feat_groups for f in g]\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "\n",
    "    # Count location features\n",
    "    location_keywords = ['latitude', 'longitude', 'geo_cluster', 'cluster_avg_price',\n",
    "                        'cluster_med_price', 'lat_long_interaction', 'distance_to_center',\n",
    "                        'neighborhood_density']\n",
    "    location_feat_count = sum(1 for f in feats if any(kw in f for kw in location_keywords))\n",
    "    print(f\"  → {location_feat_count} location-based features (weighted {LOCATION_WEIGHT}x)\")\n",
    "\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "    if for_training:\n",
    "        return df.dropna(subset=[y_col]), feats, kmeans_model\n",
    "    else:\n",
    "        return df, feats, kmeans_model\n",
    "\n",
    "def train_segments(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining LOCATION-FOCUSED segmented models on {len(df):,} properties\")\n",
    "    print(f\"Location weight multiplier: {LOCATION_WEIGHT}x\")\n",
    "    print(f\"Geo clusters: {N_CLUSTERS}\")\n",
    "\n",
    "    df['seg'] = assign_segments_simple(df)\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    print(f\"{len(seg_cnts)} segments created\")\n",
    "    for seg, cnt in seg_cnts.head(MAX_SEGMENTS).items(): print(f\"  {seg}: {cnt:,}\")\n",
    "\n",
    "    small = seg_cnts[seg_cnts<50].index\n",
    "    if len(small)>0:\n",
    "        df.loc[df['seg'].isin(small),'seg'] = 'other'\n",
    "        print(f\"Merged {len(small)} small segments into 'other'\")\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    if len(seg_cnts) > MAX_SEGMENTS:\n",
    "        keep_segs = seg_cnts.head(MAX_SEGMENTS-1).index\n",
    "        df.loc[~df['seg'].isin(keep_segs),'seg'] = 'other'\n",
    "        print(f\"Consolidated to {MAX_SEGMENTS} segments maximum\")\n",
    "\n",
    "    models, metrics, preds_list, segment_importances = {}, {}, [], {}\n",
    "\n",
    "    for seg in df['seg'].unique():\n",
    "        seg_df = df[df['seg']==seg].copy()\n",
    "        if len(seg_df)<50: continue\n",
    "        seg_df = filter_outliers(seg_df, seg, y_col)\n",
    "        if len(seg_df)<50: continue\n",
    "        train_idx = seg_df.sample(frac=1-TEST_SIZE,random_state=RAND_STATE).index\n",
    "        train_df, test_df = seg_df.loc[train_idx].copy(), seg_df.loc[seg_df.index.difference(train_idx)].copy()\n",
    "        train_df, test_df = add_cluster_feats(train_df, test_df, y_col)\n",
    "        X_tr,y_tr = train_df[feats].values, train_df[y_col].values\n",
    "        X_te,y_te = test_df[feats].values, test_df[y_col].values\n",
    "        ids_te = test_df[id_col].values\n",
    "        states_te = test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "        seg_models, seg_preds = {}, []\n",
    "        for q in QUANTILES:\n",
    "            m = train_model(X_tr,y_tr,q,feats)  # Pass feat_names\n",
    "            seg_models[f\"q{int(q*100)}\"] = m\n",
    "            seg_preds.append(m.predict(X_te))\n",
    "        models[seg] = seg_models\n",
    "\n",
    "        segment_importances[seg] = get_feature_importance_per_segment(seg_models['q50'], feats, top_n=20)\n",
    "\n",
    "        y_pred = seg_preds[1]\n",
    "        mae,mape = mean_absolute_error(y_te,y_pred), np.mean(np.abs((y_te-y_pred)/y_te))*100\n",
    "        r2,cov = r2_score(y_te,y_pred), np.mean((y_te>=seg_preds[0])&(y_te<=seg_preds[2]))*100\n",
    "        metrics[seg] = {'n_train':len(X_tr),'n_test':len(X_te),'mae':mae,'mape':mape,'r2':r2,'cov':cov,'p_min':seg_df[y_col].min(),'p_max':seg_df[y_col].max(),'p_med':seg_df[y_col].median()}\n",
    "        print(f\"  {seg}: {len(test_df):,} test | MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.3f}\")\n",
    "        preds_list.append(pd.DataFrame({'property_id':ids_te,'state':states_te,'actual':y_te,'predicted':y_pred,'pred_lower':seg_preds[0],'pred_upper':seg_preds[2],'segment':seg}))\n",
    "\n",
    "    return {\n",
    "        'models': models,\n",
    "        'metrics': metrics,\n",
    "        'predictions': pd.concat(preds_list),\n",
    "        'feature_importance': feature_importance(models, feats, metrics),\n",
    "        'segment_importances': segment_importances,\n",
    "        'feature_names': feats\n",
    "    }\n",
    "\n",
    "def predict_new_properties(pred_df, models, feats, y_col, id_col, state_col, kmeans_model, train_cluster_stats):\n",
    "    \"\"\"Generate predictions for new properties using trained models\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"GENERATING PREDICTIONS FOR NEW PROPERTIES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Input properties: {len(pred_df):,}\")\n",
    "\n",
    "    pred_df = engineer(pred_df, y_col)\n",
    "    pred_df, _ = geo_cluster(pred_df, kmeans_model)\n",
    "\n",
    "    if 'geo_cluster' in pred_df.columns and train_cluster_stats is not None:\n",
    "        pred_df = pred_df.merge(train_cluster_stats, on='geo_cluster', how='left')\n",
    "        median_price = train_cluster_stats['cluster_avg_price'].median()\n",
    "        pred_df['cluster_avg_price'] = pred_df['cluster_avg_price'].fillna(median_price)\n",
    "        pred_df['cluster_med_price'] = pred_df['cluster_med_price'].fillna(median_price)\n",
    "\n",
    "    for feat in feats:\n",
    "        if feat not in pred_df.columns:\n",
    "            pred_df[feat] = 0\n",
    "        else:\n",
    "            pred_df[feat] = pred_df[feat].fillna(pred_df[feat].median() if pred_df[feat].notna().sum() > 0 else 0)\n",
    "\n",
    "    pred_df['seg'] = assign_segments_simple(pred_df)\n",
    "\n",
    "    preds_list = []\n",
    "    for seg in pred_df['seg'].unique():\n",
    "        seg_df = pred_df[pred_df['seg']==seg].copy()\n",
    "\n",
    "        if seg not in models:\n",
    "            print(f\"  Warning: Segment '{seg}' not in trained models, using 'mid_large' as fallback\")\n",
    "            seg = 'mid_large' if 'mid_large' in models else list(models.keys())[0]\n",
    "\n",
    "        X = seg_df[feats].values\n",
    "        ids = seg_df[id_col].values\n",
    "        states = seg_df[state_col].values if state_col and state_col in seg_df.columns else ['Unknown']*len(seg_df)\n",
    "\n",
    "        pred_lower = models[seg]['q10'].predict(X)\n",
    "        pred_mid = models[seg]['q50'].predict(X)\n",
    "        pred_upper = models[seg]['q90'].predict(X)\n",
    "\n",
    "        actual = seg_df[y_col].values if y_col in seg_df.columns else [np.nan]*len(seg_df)\n",
    "\n",
    "        preds_list.append(pd.DataFrame({\n",
    "            'property_id': ids,\n",
    "            'state': states,\n",
    "            'actual': actual,\n",
    "            'predicted': pred_mid,\n",
    "            'pred_lower': pred_lower,\n",
    "            'pred_upper': pred_upper,\n",
    "            'segment': seg,\n",
    "            'error': [actual[i] - pred_mid[i] if not np.isnan(actual[i]) else np.nan for i in range(len(actual))],\n",
    "            'pct_error': [100 * (actual[i] - pred_mid[i]) / actual[i] if not np.isnan(actual[i]) and actual[i] != 0 else np.nan for i in range(len(actual))]\n",
    "        }))\n",
    "\n",
    "        print(f\"  {seg}: {len(seg_df):,} properties predicted\")\n",
    "\n",
    "    result_df = pd.concat(preds_list, ignore_index=True)\n",
    "    print(f\"\\n✓ Generated {len(result_df):,} predictions\")\n",
    "\n",
    "    valid_actuals = result_df['actual'].notna().sum()\n",
    "    if valid_actuals > 0:\n",
    "        valid_preds = result_df[result_df['actual'].notna()].copy()\n",
    "        mae = mean_absolute_error(valid_preds['actual'], valid_preds['predicted'])\n",
    "        mape = np.mean(np.abs((valid_preds['actual'] - valid_preds['predicted']) / valid_preds['actual'])) * 100\n",
    "        r2 = r2_score(valid_preds['actual'], valid_preds['predicted'])\n",
    "        print(f\"  Validation metrics ({valid_actuals} properties):\")\n",
    "        print(f\"  MAE: ${mae:,.0f} | MAPE: {mape:.2f}% | R²: {r2:.4f}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def save_results(results, out_dir, new_predictions=None):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi = results['predictions'], results['metrics'], results['feature_importance']\n",
    "    seg_importances = results['segment_importances']\n",
    "\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    # Summary\n",
    "    ws = wb.create_sheet(\"Summary\",0)\n",
    "    ws['A1'], ws['A1'].font = 'LOCATION-FOCUSED SEGMENTED AVM', Font(bold=True,size=14)\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    data = [['Metric','Value'],\n",
    "            ['Properties',len(preds)],\n",
    "            ['Segments',len(metrics)],\n",
    "            ['R²',f'{r2:.4f}'],\n",
    "            ['MAE',f'${mae:,.0f}'],\n",
    "            ['MAPE%',f'{mape:.2f}%'],\n",
    "            ['Location Weight',f'{LOCATION_WEIGHT}x'],\n",
    "            ['Geo Clusters',N_CLUSTERS]]\n",
    "    if new_predictions is not None:\n",
    "        data.append(['New Predictions', len(new_predictions)])\n",
    "    for i,(k,v) in enumerate(data,5): ws[f'A{i}'],ws[f'A{i}'].font,ws[f'B{i}'] = k,Font(bold=True),v\n",
    "\n",
    "    # Segments\n",
    "    ws = wb.create_sheet(\"Segments\")\n",
    "    seg_df = pd.DataFrame([{**{'Segment':s},**m} for s,m in metrics.items()])\n",
    "    for i,h in enumerate(seg_df.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(seg_df.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    # Global Feature Importance\n",
    "    ws = wb.create_sheet(\"Global_Feature_Importance\")\n",
    "    ws['A1'] = 'Global Feature Importance (Weighted) - LOCATION EMPHASIZED'\n",
    "    ws['A1'].font = Font(bold=True, size=12)\n",
    "    for r_idx, row in enumerate(dataframe_to_rows(fi, index=False, header=True), 2):\n",
    "        for c_idx, value in enumerate(row, 1):\n",
    "            cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "            if r_idx == 2:\n",
    "                cell.font = Font(bold=True, color='FFFFFF')\n",
    "                cell.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "\n",
    "    # Per-Segment Feature Importance\n",
    "    for seg_name, seg_fi in seg_importances.items():\n",
    "        sheet_name = f\"FI_{seg_name}\"[:31]\n",
    "        ws = wb.create_sheet(sheet_name)\n",
    "        ws['A1'] = f'Feature Importance: {seg_name}'\n",
    "        ws['A1'].font = Font(bold=True, size=12)\n",
    "\n",
    "        for r_idx, row in enumerate(dataframe_to_rows(seg_fi, index=False, header=True), 2):\n",
    "            for c_idx, value in enumerate(row, 1):\n",
    "                cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "                if r_idx == 2:\n",
    "                    cell.font = Font(bold=True, color='FFFFFF')\n",
    "                    cell.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "\n",
    "    # Test Set Predictions\n",
    "    ws = wb.create_sheet(\"Test_Predictions\")\n",
    "    for i,h in enumerate(preds.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(preds.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    # New Predictions\n",
    "    if new_predictions is not None:\n",
    "        ws = wb.create_sheet(\"New_Predictions\")\n",
    "        for i,h in enumerate(new_predictions.columns,1):\n",
    "            c = ws.cell(1,i,h)\n",
    "            c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='4472C4',end_color='4472C4',fill_type='solid')\n",
    "        for i,row in enumerate(new_predictions.itertuples(index=False),2):\n",
    "            for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/location_focused_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "    preds.to_csv(f\"{out_dir}/location_test_predictions_{ts}.csv\",index=False)\n",
    "    seg_df.to_csv(f\"{out_dir}/location_segments_{ts}.csv\",index=False)\n",
    "    fi.to_csv(f\"{out_dir}/location_importance_{ts}.csv\",index=False)\n",
    "\n",
    "    for seg_name, seg_fi in seg_importances.items():\n",
    "        seg_fi.to_csv(f\"{out_dir}/location_importance_{seg_name}_{ts}.csv\", index=False)\n",
    "\n",
    "    if new_predictions is not None:\n",
    "        new_predictions.to_csv(f\"{out_dir}/location_new_predictions_{ts}.csv\", index=False)\n",
    "        print(f\"✓ New predictions CSV: {out_dir}/location_new_predictions_{ts}.csv\")\n",
    "\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved with timestamp: {ts}\")\n",
    "    print(f\"✓ Per-segment feature importance saved for {len(seg_importances)} segments\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60)\n",
    "    print(\"LOCATION-FOCUSED SEGMENTED AVM\")\n",
    "    print(f\"Location weight: {LOCATION_WEIGHT}x | Geo clusters: {N_CLUSTERS}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    df, y_col, id_col, state_col = load_data(TRAINING_INPUT_PATH)\n",
    "    df, feats, kmeans_model = prepare_data(df, y_col, id_col, state_col, for_training=True)\n",
    "\n",
    "    results = train_segments(df, feats, y_col, id_col, state_col)\n",
    "\n",
    "    train_cluster_stats = None\n",
    "    if 'geo_cluster' in df.columns:\n",
    "        train_cluster_stats = df.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "        train_cluster_stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "\n",
    "    new_predictions = None\n",
    "    if PREDICTION_INPUT_PATH:\n",
    "        pred_df, _, _, _ = load_data(PREDICTION_INPUT_PATH)\n",
    "        new_predictions = predict_new_properties(\n",
    "            pred_df, results['models'], feats, y_col, id_col, state_col,\n",
    "            kmeans_model, train_cluster_stats\n",
    "        )\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR, new_predictions)\n",
    "\n",
    "    preds = results['predictions']\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ LOCATION-FOCUSED TRAINING COMPLETE in {time.time()-t0:.1f}s\")\n",
    "    print(f\"  Test set: {len(preds):,} properties | {preds['segment'].nunique()} segments\")\n",
    "    print(f\"  R²: {r2:.4f} | MAE: ${mae:,.0f} | MAPE: {mape:.2f}%\")\n",
    "    if new_predictions is not None:\n",
    "        print(f\"  New predictions: {len(new_predictions):,} properties\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "if __name__ == \"__main__\": main()"
   ],
   "id": "6e8b04a3cfb86da4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOCATION-FOCUSED SEGMENTED AVM\n",
      "Location weight: 3.0x | Geo clusters: 16\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 129.6MB | Price:sale_price ID:property_id\n",
      "\n",
      "Preparing data...\n",
      "127,258 records after price filter\n",
      "51/60 features available\n",
      "  → 6 location-based features (weighted 3.0x)\n",
      "\n",
      "Training LOCATION-FOCUSED segmented models on 127,258 properties\n",
      "Location weight multiplier: 3.0x\n",
      "Geo clusters: 16\n",
      "6 segments created\n",
      "  budget_small: 23,028\n",
      "  mid_large: 22,880\n",
      "  premium_large: 21,935\n",
      "  mid_small: 20,638\n",
      "  premium_small: 19,967\n",
      "  budget_large: 18,810\n",
      "  premium_small: 19,967→15,479 (22.5% filtered)\n",
      "  premium_small: 4,644 test | MAE:$1,584,320 | MAPE:33.86% | R²:0.535\n",
      "  premium_large: 21,935→16,873 (23.1% filtered)\n",
      "  premium_large: 5,062 test | MAE:$632,527 | MAPE:21.30% | R²:0.410\n",
      "  mid_large: 22,880→17,671 (22.8% filtered)\n",
      "  mid_large: 5,301 test | MAE:$119,969 | MAPE:10.17% | R²:0.112\n",
      "  mid_small: 20,638→15,752 (23.7% filtered)\n",
      "  mid_small: 4,726 test | MAE:$113,622 | MAPE:9.50% | R²:0.169\n",
      "  budget_large: 18,810→14,526 (22.8% filtered)\n",
      "  budget_large: 4,358 test | MAE:$37,209 | MAPE:4.26% | R²:0.062\n",
      "  budget_small: 23,028→17,660 (23.3% filtered)\n",
      "  budget_small: 5,298 test | MAE:$39,253 | MAPE:4.56% | R²:0.025\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\n",
      "1 records | 0.0MB | Price:sale_price ID:property_id\n",
      "\n",
      "============================================================\n",
      "GENERATING PREDICTIONS FOR NEW PROPERTIES\n",
      "============================================================\n",
      "Input properties: 1\n",
      "  mid_small: 1 properties predicted\n",
      "\n",
      "✓ Generated 1 predictions\n",
      "  Validation metrics (1 properties):\n",
      "  MAE: $592,403 | MAPE: 112.62% | R²: nan\n",
      "\n",
      "Saving results...\n",
      "✓ New predictions CSV: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/location_new_predictions_20251223_231529.csv\n",
      "✓ Excel: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/location_focused_20251223_231529.xlsx\n",
      "✓ CSVs saved with timestamp: 20251223_231529\n",
      "✓ Per-segment feature importance saved for 6 segments\n",
      "\n",
      "============================================================\n",
      "✓ LOCATION-FOCUSED TRAINING COMPLETE in 10.4s\n",
      "  Test set: 29,389 properties | 6 segments\n",
      "  R²: 0.7021 | MAE: $411,803 | MAPE: 13.83%\n",
      "  New predictions: 1 properties\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T04:42:59.155654Z",
     "start_time": "2025-12-24T04:42:49.304775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import time, os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Config - FIXED PARAMETERS\n",
    "INCLUDE_MLS, INCLUDE_CENSUS, INCLUDE_NEIGHBORHOOD, INCLUDE_IMAGE = True, True, True, False\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS = 20000, 0.3, 42, -1, 8\n",
    "N_EST, QUANTILES, MAX_SEGMENTS = 100, [0.1, 0.5, 0.9], 7  # MAX 7 SEGMENTS\n",
    "MIN_ULTRA_HIGH_ASSESSED = 3000000  # $3M assessed value = ultra-high (roughly $5M+ market)\n",
    "\n",
    "# INPUT PATHS\n",
    "TRAINING_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "PREDICTION_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "BASE_FEATS = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"effective_year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\"fireplace_code\",\"latitude\",\"longitude\",\"geo_cluster\"]\n",
    "ENG_FEATS = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\"]\n",
    "PRIOR_FEATS = [\"prior_sale_price\",\"prior_price_per_sqft\",\"sqft_per_prior_dollar\",\"years_since_last_sale\",\"expected_appreciation\",\"has_prior_sale\",\"recently_sold\"]\n",
    "CLUSTER_FEATS = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "CENSUS_FEATS = [\"total_population_25plus\",\"male_bachelors_degree\",\"female_bachelors_degree\",\"pct_bachelors_degree\",\"median_earnings_total\",\"median_earnings_male\",\"median_earnings_female\",\"median_household_income\",\"median_home_value\",\"median_gross_rent\",\"owner_occupied_units\",\"renter_occupied_units\",\"pct_owner_occupied\",\"occupied_units\",\"vacant_units\",\"median_age\",\"civilian_employed\",\"civilian_unemployed\",\"unemployment_rate\",\"income_education_score\"]\n",
    "ELECT_FEATS = [\"votes_gop\",\"votes_dem\",\"total_votes\",\"per_gop\",\"per_dem\",\"per_point_diff\",\"dem_margin\",\"rep_margin\"]\n",
    "IMG_FEATS = [\"topic_1\",\"topic_2\",\"topic_3\",\"topic_4\",\"topic_5\",\"topic_6\",\"topic_7\",\"topic_8\",\"topic_9\",\"topic_10\",\"gran_c_in\",\"gran_c_ex\",\"gran_c\",\"high_c_in\",\"high_c_ex\",\"high_c\"]\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8') if set(df[c].dropna().unique()).issubset({0,1}) else df[c].astype('int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return optimize_dtypes(df), y_col, id_col, state_col\n",
    "\n",
    "def assign_segments_simple(df):\n",
    "    \"\"\"\n",
    "    REFACTORED: Use ASSESSED_TOTAL_VALUE (lagged) instead of current sale price\n",
    "    This avoids data leakage since assessed value is from prior year\n",
    "    \"\"\"\n",
    "    segments = pd.Series(['mid'] * len(df), index=df.index)\n",
    "\n",
    "    # Primary: Use ASSESSED VALUE for tier (lagged, no leakage)\n",
    "    if 'assessed_total_value' in df.columns:\n",
    "        assessed = df['assessed_total_value'].fillna(df['assessed_total_value'].median())\n",
    "\n",
    "        # Identify ultra-high properties first\n",
    "        ultra_high_mask = assessed >= MIN_ULTRA_HIGH_ASSESSED\n",
    "\n",
    "        # For non-ultra properties, calculate standard tiers\n",
    "        if ultra_high_mask.sum() >= 50:  # Enough ultra-high for separate segment\n",
    "            non_ultra = assessed[~ultra_high_mask]\n",
    "            if len(non_ultra) > 0:\n",
    "                q33, q67 = non_ultra.quantile([.33, .67])\n",
    "                price_tier = pd.Series(['mid'] * len(df), index=df.index)\n",
    "                price_tier[assessed < q33] = 'budget'\n",
    "                price_tier[(assessed >= q33) & (assessed < q67)] = 'mid'\n",
    "                price_tier[(assessed >= q67) & (~ultra_high_mask)] = 'premium'\n",
    "                price_tier[ultra_high_mask] = 'ultra_high'\n",
    "            else:\n",
    "                price_tier = pd.Series(['mid'] * len(df), index=df.index)\n",
    "        else:\n",
    "            # Not enough ultra-high, use standard quartiles\n",
    "            q33, q67 = assessed.quantile([.33, .67])\n",
    "            price_tier = pd.Series(['mid'] * len(df), index=df.index)\n",
    "            price_tier[assessed < q33] = 'budget'\n",
    "            price_tier[(assessed >= q33) & (assessed < q67)] = 'mid'\n",
    "            price_tier[assessed >= q67] = 'premium'\n",
    "    else:\n",
    "        print(\"  WARNING: assessed_total_value not found, using fallback segmentation\")\n",
    "        price_tier = pd.Series(['mid'] * len(df), index=df.index)\n",
    "\n",
    "    # Secondary: Size (2 segments: small vs large)\n",
    "    if 'living_sqft' in df.columns:\n",
    "        sqft = df['living_sqft'].fillna(df['living_sqft'].median())\n",
    "        sqft_median = sqft.median()\n",
    "        size_tier = pd.Series(['small'] * len(df), index=df.index)\n",
    "        size_tier[sqft > sqft_median] = 'large'\n",
    "    else:\n",
    "        size_tier = pd.Series(['small'] * len(df), index=df.index)\n",
    "\n",
    "    # Combine segments\n",
    "    # For ultra_high, ignore size tier (single segment)\n",
    "    segments = price_tier + '_' + size_tier\n",
    "    if 'assessed_total_value' in df.columns:\n",
    "        ultra_high_mask = df['assessed_total_value'].fillna(0) >= MIN_ULTRA_HIGH_ASSESSED\n",
    "        if ultra_high_mask.sum() >= 50:\n",
    "            segments[ultra_high_mask] = 'ultra_high'\n",
    "\n",
    "    return segments\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    if 'living_sqft' in df.columns and 'bedrooms' in df.columns: df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns: df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns: df['lot_acres'] = df['lot_sqft']/43560\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'] = 2024-df['year_built']\n",
    "        df['is_new'] = (df['property_age']<=5).astype('int8')\n",
    "        df['age_squared'] = df['property_age']**2\n",
    "    if 'garage_spaces' in df.columns: df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "    if 'living_sqft' in df.columns: df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "\n",
    "    # NEW: Assessed value ratios (no leakage - these are lagged)\n",
    "    if 'assessed_total_value' in df.columns and 'living_sqft' in df.columns:\n",
    "        df['assessed_per_sqft'] = df['assessed_total_value'] / (df['living_sqft'] + 1)\n",
    "    if 'assessed_land_value' in df.columns and 'assessed_total_value' in df.columns:\n",
    "        df['land_to_total_ratio'] = df['assessed_land_value'] / (df['assessed_total_value'] + 1)\n",
    "    if 'assessed_improvement_value' in df.columns and 'living_sqft' in df.columns:\n",
    "        df['improvement_per_sqft'] = df['assessed_improvement_value'] / (df['living_sqft'] + 1)\n",
    "\n",
    "    if INCLUDE_CENSUS and all(c in df.columns for c in ['median_household_income','pct_bachelors_degree']):\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "    if 'prior_sale_price' in df.columns and 'living_sqft' in df.columns:\n",
    "        df['prior_price_per_sqft'] = df['prior_sale_price']/(df['living_sqft']+1)\n",
    "        df['sqft_per_prior_dollar'] = df['living_sqft']/(df['prior_sale_price']+1)\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        if 'prior_sale_price' in df.columns:\n",
    "            df['expected_appreciation'] = df['prior_sale_price']*(1.04)**df['years_since_last_sale']\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df.loc[missing,'median_home_value'] if 'median_home_value' in df.columns and INCLUDE_CENSUS else df['prior_sale_price'].median()\n",
    "    if 'years_since_last_sale' in df.columns: df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "    if with_price and y_col in df.columns and 'living_sqft' in df.columns:\n",
    "        df['price_per_sqft'] = df[y_col]/(df['living_sqft']+1)\n",
    "        df['sqft_per_dollar'] = df['living_sqft']/(df[y_col]+1)\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df, kmeans_model=None):\n",
    "    \"\"\"Apply geo clustering - can use existing model for prediction\"\"\"\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans_model\n",
    "\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum() < N_CLUSTERS:\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans_model\n",
    "\n",
    "    df['geo_cluster'] = 0\n",
    "\n",
    "    if kmeans_model is None:\n",
    "        # Training: fit new model\n",
    "        kmeans_model = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "        df.loc[valid,'geo_cluster'] = kmeans_model.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    else:\n",
    "        # Prediction: use existing model\n",
    "        df.loc[valid,'geo_cluster'] = kmeans_model.predict(df.loc[valid,['latitude','longitude']])\n",
    "\n",
    "    return df, kmeans_model\n",
    "\n",
    "def add_cluster_feats(train, test, y_col):\n",
    "    if 'geo_cluster' not in train.columns or y_col not in train.columns:\n",
    "        for d in [train,test]: d['cluster_avg_price'] = d['cluster_med_price'] = train[y_col].median() if y_col in train.columns else 0\n",
    "        return train, test\n",
    "    stats = train.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "    stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "    train = train.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    test = test.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    return train, test\n",
    "\n",
    "def filter_outliers(df, name, y_col):\n",
    "    orig = len(df)\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)]\n",
    "    if 'sqft_per_dollar' in df.columns: df = df[df['sqft_per_dollar']<=df['sqft_per_dollar'].quantile(.95)]\n",
    "    if 'price_per_sqft' in df.columns: df = df.drop(columns=['price_per_sqft','sqft_per_dollar'])\n",
    "    if 'lot_sqft' in df.columns: df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "    if 'year_built' in df.columns: df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "    if len(df)>=100:\n",
    "        try:\n",
    "            feats = [c for c in ['living_sqft','lot_sqft',y_col,'year_built'] if c in df.columns]\n",
    "            if len(feats)>=3:\n",
    "                X = df[feats].fillna(df[feats].median())\n",
    "                df = df[IsolationForest(contamination=.05,random_state=RAND_STATE,n_jobs=N_JOBS).fit_predict(X)==1]\n",
    "        except: pass\n",
    "    pct_filt = (orig-len(df))/orig*100 if orig>0 else 0\n",
    "    if pct_filt > 0: print(f\"  {name}: {orig:,}→{len(df):,} ({pct_filt:.1f}% filtered)\")\n",
    "    return df\n",
    "\n",
    "def train_model(X, y, q):\n",
    "    return XGBRegressor(\n",
    "        objective='reg:quantileerror',\n",
    "        quantile_alpha=q,\n",
    "        n_estimators=N_EST,\n",
    "        learning_rate=.1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        subsample=.8,\n",
    "        colsample_bytree=.8,\n",
    "        random_state=RAND_STATE,\n",
    "        n_jobs=N_JOBS,\n",
    "        tree_method='hist'\n",
    "    ).fit(X, y, verbose=False)\n",
    "\n",
    "def get_feature_importance_per_segment(model, feat_names, top_n=20):\n",
    "    \"\"\"Extract feature importance for a single segment model\"\"\"\n",
    "    scores = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    importance_list = []\n",
    "    for k, v in scores.items():\n",
    "        idx = int(k[1:])\n",
    "        if idx < len(feat_names):\n",
    "            importance_list.append((feat_names[idx], v))\n",
    "\n",
    "    # Sort and normalize\n",
    "    importance_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    total_gain = sum(v for _, v in importance_list)\n",
    "\n",
    "    if total_gain > 0:\n",
    "        importance_df = pd.DataFrame([\n",
    "            {'feature': feat, 'gain': gain, 'importance': gain/total_gain}\n",
    "            for feat, gain in importance_list[:top_n]\n",
    "        ])\n",
    "    else:\n",
    "        importance_df = pd.DataFrame(columns=['feature', 'gain', 'importance'])\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "def feature_importance(models, feat_names, metrics):\n",
    "    \"\"\"Global feature importance weighted by segment size - NOW INCLUDES TOTAL_GAIN\"\"\"\n",
    "    rows = []\n",
    "    for seg, mdl in models.items():\n",
    "        scores = mdl['q50'].get_booster().get_score(importance_type=\"gain\")\n",
    "        w = metrics[seg][\"n_test\"]\n",
    "        for k,v in scores.items():\n",
    "            idx = int(k[1:])\n",
    "            if idx<len(feat_names): rows.append((feat_names[idx],v,w))\n",
    "    if not rows: return pd.DataFrame(columns=[\"feature\",\"total_gain\",\"importance\"])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"feature\",\"gain\",\"weight\"])\n",
    "\n",
    "    # Calculate both total_gain and weighted importance\n",
    "    out = df.assign(wg=df[\"gain\"]*df[\"weight\"]).groupby(\"feature\",as_index=False).agg(\n",
    "        total_gain=(\"gain\",\"sum\"),      # Sum of gain across all segments\n",
    "        weighted_gain=(\"wg\",\"sum\")      # Weighted by segment size\n",
    "    ).sort_values(\"weighted_gain\",ascending=False)\n",
    "\n",
    "    out[\"importance\"] = out[\"weighted_gain\"]/out[\"weighted_gain\"].sum()\n",
    "\n",
    "    return out[[\"feature\",\"total_gain\",\"importance\"]].head(100)\n",
    "\n",
    "def prepare_data(df, y_col, id_col, state_col, for_training=True):\n",
    "    print(f\"\\nPreparing data...\")\n",
    "    print(f\"Segmentation basis: ASSESSED_TOTAL_VALUE (lagged, no leakage)\")\n",
    "    if for_training:\n",
    "        df = df[df[y_col]>=MIN_PRICE]\n",
    "        print(f\"{len(df):,} records after price filter\")\n",
    "    df, kmeans_model = geo_cluster(engineer(df, y_col))\n",
    "    feat_groups = []\n",
    "    if INCLUDE_MLS: feat_groups.extend([BASE_FEATS,ENG_FEATS,PRIOR_FEATS,CLUSTER_FEATS])\n",
    "    if INCLUDE_CENSUS: feat_groups.append(CENSUS_FEATS)\n",
    "    if INCLUDE_NEIGHBORHOOD: feat_groups.append(ELECT_FEATS)\n",
    "    if INCLUDE_IMAGE: feat_groups.append(IMG_FEATS)\n",
    "    all_feats = [f for g in feat_groups for f in g]\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "\n",
    "    # Add new assessed-value features if they exist\n",
    "    assessed_feats = ['assessed_per_sqft', 'land_to_total_ratio', 'improvement_per_sqft']\n",
    "    for af in assessed_feats:\n",
    "        if af in df.columns and af not in feats:\n",
    "            feats.append(af)\n",
    "\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "    if for_training:\n",
    "        return df.dropna(subset=[y_col]), feats, kmeans_model\n",
    "    else:\n",
    "        return df, feats, kmeans_model\n",
    "\n",
    "def train_segments(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining segmented models on {len(df):,} properties\")\n",
    "\n",
    "    # Segment based on ASSESSED VALUE (not sale price)\n",
    "    df['seg'] = assign_segments_simple(df)\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    print(f\"{len(seg_cnts)} segments created\")\n",
    "    for seg, cnt in seg_cnts.head(MAX_SEGMENTS).items():\n",
    "        assessed_range = \"\"\n",
    "        if 'assessed_total_value' in df.columns and seg != 'other':\n",
    "            seg_df = df[df['seg']==seg]\n",
    "            if len(seg_df) > 0:\n",
    "                min_assessed = seg_df['assessed_total_value'].min()\n",
    "                max_assessed = seg_df['assessed_total_value'].max()\n",
    "                assessed_range = f\" (assessed: ${min_assessed:,.0f}-${max_assessed:,.0f})\"\n",
    "        print(f\"  {seg}: {cnt:,}{assessed_range}\")\n",
    "\n",
    "    # Merge small segments\n",
    "    small = seg_cnts[seg_cnts<50].index\n",
    "    if len(small)>0:\n",
    "        df.loc[df['seg'].isin(small),'seg'] = 'other'\n",
    "        print(f\"Merged {len(small)} small segments into 'other'\")\n",
    "\n",
    "    # If still too many segments, merge smallest ones\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    if len(seg_cnts) > MAX_SEGMENTS:\n",
    "        keep_segs = seg_cnts.head(MAX_SEGMENTS-1).index\n",
    "        df.loc[~df['seg'].isin(keep_segs),'seg'] = 'other'\n",
    "        print(f\"Consolidated to {MAX_SEGMENTS} segments maximum\")\n",
    "\n",
    "    models, metrics, preds_list, segment_importances = {}, {}, [], {}\n",
    "\n",
    "    for seg in df['seg'].unique():\n",
    "        seg_df = df[df['seg']==seg].copy()\n",
    "        if len(seg_df)<50: continue\n",
    "        seg_df = filter_outliers(seg_df, seg, y_col)\n",
    "        if len(seg_df)<50: continue\n",
    "        train_idx = seg_df.sample(frac=1-TEST_SIZE,random_state=RAND_STATE).index\n",
    "        train_df, test_df = seg_df.loc[train_idx].copy(), seg_df.loc[seg_df.index.difference(train_idx)].copy()\n",
    "        train_df, test_df = add_cluster_feats(train_df, test_df, y_col)\n",
    "        X_tr,y_tr = train_df[feats].values, train_df[y_col].values\n",
    "        X_te,y_te = test_df[feats].values, test_df[y_col].values\n",
    "        ids_te = test_df[id_col].values\n",
    "        states_te = test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "        seg_models, seg_preds = {}, []\n",
    "        for q in QUANTILES:\n",
    "            m = train_model(X_tr,y_tr,q)\n",
    "            seg_models[f\"q{int(q*100)}\"] = m\n",
    "            seg_preds.append(m.predict(X_te))\n",
    "        models[seg] = seg_models\n",
    "\n",
    "        # Get feature importance for this segment\n",
    "        segment_importances[seg] = get_feature_importance_per_segment(seg_models['q50'], feats, top_n=20)\n",
    "\n",
    "        y_pred = seg_preds[1]\n",
    "        mae,mape = mean_absolute_error(y_te,y_pred), np.mean(np.abs((y_te-y_pred)/y_te))*100\n",
    "        r2,cov = r2_score(y_te,y_pred), np.mean((y_te>=seg_preds[0])&(y_te<=seg_preds[2]))*100\n",
    "        metrics[seg] = {'n_train':len(X_tr),'n_test':len(X_te),'mae':mae,'mape':mape,'r2':r2,'cov':cov,'p_min':seg_df[y_col].min(),'p_max':seg_df[y_col].max(),'p_med':seg_df[y_col].median()}\n",
    "        print(f\"  {seg}: {len(test_df):,} test | MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.3f}\")\n",
    "        preds_list.append(pd.DataFrame({'property_id':ids_te,'state':states_te,'actual':y_te,'predicted':y_pred,'pred_lower':seg_preds[0],'pred_upper':seg_preds[2],'segment':seg}))\n",
    "\n",
    "    return {\n",
    "        'models': models,\n",
    "        'metrics': metrics,\n",
    "        'predictions': pd.concat(preds_list),\n",
    "        'feature_importance': feature_importance(models, feats, metrics),\n",
    "        'segment_importances': segment_importances,\n",
    "        'feature_names': feats\n",
    "    }\n",
    "\n",
    "def predict_new_properties(pred_df, models, feats, y_col, id_col, state_col, kmeans_model, train_cluster_stats):\n",
    "    \"\"\"Generate predictions for new properties using trained models\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"GENERATING PREDICTIONS FOR NEW PROPERTIES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Input properties: {len(pred_df):,}\")\n",
    "\n",
    "    # Engineer features (no price filtering for prediction)\n",
    "    pred_df = engineer(pred_df, y_col)\n",
    "    pred_df, _ = geo_cluster(pred_df, kmeans_model)\n",
    "\n",
    "    # Add cluster features from training data\n",
    "    if 'geo_cluster' in pred_df.columns and train_cluster_stats is not None:\n",
    "        pred_df = pred_df.merge(train_cluster_stats, on='geo_cluster', how='left')\n",
    "        median_price = train_cluster_stats['cluster_avg_price'].median()\n",
    "        pred_df['cluster_avg_price'] = pred_df['cluster_avg_price'].fillna(median_price)\n",
    "        pred_df['cluster_med_price'] = pred_df['cluster_med_price'].fillna(median_price)\n",
    "\n",
    "    # Fill missing features with median or 0\n",
    "    for feat in feats:\n",
    "        if feat not in pred_df.columns:\n",
    "            pred_df[feat] = 0\n",
    "        else:\n",
    "            pred_df[feat] = pred_df[feat].fillna(pred_df[feat].median() if pred_df[feat].notna().sum() > 0 else 0)\n",
    "\n",
    "    # Assign segments based on ASSESSED VALUE\n",
    "    pred_df['seg'] = assign_segments_simple(pred_df)\n",
    "\n",
    "    # Generate predictions\n",
    "    preds_list = []\n",
    "    for seg in pred_df['seg'].unique():\n",
    "        seg_df = pred_df[pred_df['seg']==seg].copy()\n",
    "\n",
    "        if seg not in models:\n",
    "            print(f\"  Warning: Segment '{seg}' not in trained models, using fallback\")\n",
    "            # Use the closest segment as fallback\n",
    "            if 'premium' in seg and 'premium_large' in models:\n",
    "                seg = 'premium_large'\n",
    "            elif 'ultra_high' in seg and 'ultra_high' in models:\n",
    "                seg = 'ultra_high'\n",
    "            else:\n",
    "                seg = list(models.keys())[0]\n",
    "\n",
    "        X = seg_df[feats].values\n",
    "        ids = seg_df[id_col].values\n",
    "        states = seg_df[state_col].values if state_col and state_col in seg_df.columns else ['Unknown']*len(seg_df)\n",
    "\n",
    "        # Get predictions from quantile models\n",
    "        pred_lower = models[seg]['q10'].predict(X)\n",
    "        pred_mid = models[seg]['q50'].predict(X)\n",
    "        pred_upper = models[seg]['q90'].predict(X)\n",
    "\n",
    "        actual = seg_df[y_col].values if y_col in seg_df.columns else [np.nan]*len(seg_df)\n",
    "\n",
    "        # Add assessed value to output for reference\n",
    "        assessed = seg_df['assessed_total_value'].values if 'assessed_total_value' in seg_df.columns else [np.nan]*len(seg_df)\n",
    "\n",
    "        preds_list.append(pd.DataFrame({\n",
    "            'property_id': ids,\n",
    "            'state': states,\n",
    "            'assessed_value': assessed,\n",
    "            'actual': actual,\n",
    "            'predicted': pred_mid,\n",
    "            'pred_lower': pred_lower,\n",
    "            'pred_upper': pred_upper,\n",
    "            'segment': seg,\n",
    "            'error': [actual[i] - pred_mid[i] if not np.isnan(actual[i]) else np.nan for i in range(len(actual))],\n",
    "            'pct_error': [100 * (actual[i] - pred_mid[i]) / actual[i] if not np.isnan(actual[i]) and actual[i] != 0 else np.nan for i in range(len(actual))]\n",
    "        }))\n",
    "\n",
    "        print(f\"  {seg}: {len(seg_df):,} properties predicted\")\n",
    "\n",
    "    result_df = pd.concat(preds_list, ignore_index=True)\n",
    "    print(f\"\\n✓ Generated {len(result_df):,} predictions\")\n",
    "\n",
    "    # Calculate metrics if actuals are available\n",
    "    valid_actuals = result_df['actual'].notna().sum()\n",
    "    if valid_actuals > 0:\n",
    "        valid_preds = result_df[result_df['actual'].notna()].copy()\n",
    "        mae = mean_absolute_error(valid_preds['actual'], valid_preds['predicted'])\n",
    "        mape = np.mean(np.abs((valid_preds['actual'] - valid_preds['predicted']) / valid_preds['actual'])) * 100\n",
    "        r2 = r2_score(valid_preds['actual'], valid_preds['predicted'])\n",
    "        print(f\"  Validation metrics ({valid_actuals} properties):\")\n",
    "        print(f\"  MAE: ${mae:,.0f} | MAPE: {mape:.2f}% | R²: {r2:.4f}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def save_results(results, out_dir, new_predictions=None):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi = results['predictions'], results['metrics'], results['feature_importance']\n",
    "    seg_importances = results['segment_importances']\n",
    "\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    # Summary\n",
    "    ws = wb.create_sheet(\"Summary\",0)\n",
    "    ws['A1'], ws['A1'].font = 'ASSESSED-VALUE SEGMENTED AVM', Font(bold=True,size=14)\n",
    "    ws['A2'] = 'Segmentation: Based on assessed_total_value (no leakage)'\n",
    "    ws['A2'].font = Font(italic=True, size=10)\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    data = [['Metric','Value'],['Properties',len(preds)],['Segments',len(metrics)],['R²',f'{r2:.4f}'],['MAE',f'${mae:,.0f}'],['MAPE%',f'{mape:.2f}%']]\n",
    "    if new_predictions is not None:\n",
    "        data.append(['New Predictions', len(new_predictions)])\n",
    "    for i,(k,v) in enumerate(data,5): ws[f'A{i}'],ws[f'A{i}'].font,ws[f'B{i}'] = k,Font(bold=True),v\n",
    "\n",
    "    # Segments\n",
    "    ws = wb.create_sheet(\"Segments\")\n",
    "    seg_df = pd.DataFrame([{**{'Segment':s},**m} for s,m in metrics.items()])\n",
    "    for i,h in enumerate(seg_df.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(seg_df.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    # Global Feature Importance\n",
    "    ws = wb.create_sheet(\"Global_Feature_Importance\")\n",
    "    ws['A1'] = 'Global Feature Importance (Weighted)'\n",
    "    ws['A1'].font = Font(bold=True, size=12)\n",
    "    for r_idx, row in enumerate(dataframe_to_rows(fi, index=False, header=True), 2):\n",
    "        for c_idx, value in enumerate(row, 1):\n",
    "            cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "            if r_idx == 2:  # Header row\n",
    "                cell.font = Font(bold=True, color='FFFFFF')\n",
    "                cell.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "\n",
    "    # Per-Segment Feature Importance (one sheet per segment)\n",
    "    for seg_name, seg_fi in seg_importances.items():\n",
    "        sheet_name = f\"FI_{seg_name}\"[:31]  # Excel sheet name limit\n",
    "        ws = wb.create_sheet(sheet_name)\n",
    "        ws['A1'] = f'Feature Importance: {seg_name}'\n",
    "        ws['A1'].font = Font(bold=True, size=12)\n",
    "\n",
    "        for r_idx, row in enumerate(dataframe_to_rows(seg_fi, index=False, header=True), 2):\n",
    "            for c_idx, value in enumerate(row, 1):\n",
    "                cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "                if r_idx == 2:  # Header row\n",
    "                    cell.font = Font(bold=True, color='FFFFFF')\n",
    "                    cell.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "\n",
    "    # Test Set Predictions\n",
    "    ws = wb.create_sheet(\"Test_Predictions\")\n",
    "    for i,h in enumerate(preds.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(preds.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    # New Predictions (if provided)\n",
    "    if new_predictions is not None:\n",
    "        ws = wb.create_sheet(\"New_Predictions\")\n",
    "        for i,h in enumerate(new_predictions.columns,1):\n",
    "            c = ws.cell(1,i,h)\n",
    "            c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='4472C4',end_color='4472C4',fill_type='solid')\n",
    "        for i,row in enumerate(new_predictions.itertuples(index=False),2):\n",
    "            for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/assessed_segmented_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "    preds.to_csv(f\"{out_dir}/assessed_test_predictions_{ts}.csv\",index=False)\n",
    "    seg_df.to_csv(f\"{out_dir}/assessed_segments_{ts}.csv\",index=False)\n",
    "    fi.to_csv(f\"{out_dir}/assessed_importance_{ts}.csv\",index=False)\n",
    "\n",
    "    # Save per-segment feature importance CSVs\n",
    "    for seg_name, seg_fi in seg_importances.items():\n",
    "        seg_fi.to_csv(f\"{out_dir}/assessed_importance_{seg_name}_{ts}.csv\", index=False)\n",
    "\n",
    "    # Save new predictions\n",
    "    if new_predictions is not None:\n",
    "        new_predictions.to_csv(f\"{out_dir}/assessed_new_predictions_{ts}.csv\", index=False)\n",
    "        print(f\"✓ New predictions CSV: {out_dir}/assessed_new_predictions_{ts}.csv\")\n",
    "\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved with timestamp: {ts}\")\n",
    "    print(f\"✓ Per-segment feature importance saved for {len(seg_importances)} segments\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60)\n",
    "    print(\"ASSESSED-VALUE SEGMENTED AVM (NO LEAKAGE)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Load and prepare training data\n",
    "    df, y_col, id_col, state_col = load_data(TRAINING_INPUT_PATH)\n",
    "    df, feats, kmeans_model = prepare_data(df, y_col, id_col, state_col, for_training=True)\n",
    "\n",
    "    # Train models\n",
    "    results = train_segments(df, feats, y_col, id_col, state_col)\n",
    "\n",
    "    # Get cluster stats for prediction data\n",
    "    train_cluster_stats = None\n",
    "    if 'geo_cluster' in df.columns:\n",
    "        train_cluster_stats = df.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "        train_cluster_stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "\n",
    "    # Generate predictions for new properties if input file provided\n",
    "    new_predictions = None\n",
    "    if PREDICTION_INPUT_PATH:\n",
    "        pred_df, _, _, _ = load_data(PREDICTION_INPUT_PATH)\n",
    "        new_predictions = predict_new_properties(\n",
    "            pred_df, results['models'], feats, y_col, id_col, state_col,\n",
    "            kmeans_model, train_cluster_stats\n",
    "        )\n",
    "\n",
    "    # Save results\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR, new_predictions)\n",
    "\n",
    "    # Print summary\n",
    "    preds = results['predictions']\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ TRAINING COMPLETE in {time.time()-t0:.1f}s\")\n",
    "    print(f\"  Test set: {len(preds):,} properties | {preds['segment'].nunique()} segments\")\n",
    "    print(f\"  R²: {r2:.4f} | MAE: ${mae:,.0f} | MAPE: {mape:.2f}%\")\n",
    "    if new_predictions is not None:\n",
    "        print(f\"  New predictions: {len(new_predictions):,} properties\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "if __name__ == \"__main__\": main()"
   ],
   "id": "66790c2c21365081",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ASSESSED-VALUE SEGMENTED AVM (NO LEAKAGE)\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 129.6MB | Price:sale_price ID:property_id\n",
      "\n",
      "Preparing data...\n",
      "Segmentation basis: ASSESSED_TOTAL_VALUE (lagged, no leakage)\n",
      "127,258 records after price filter\n",
      "51/57 features available\n",
      "\n",
      "Training segmented models on 127,258 properties\n",
      "  WARNING: assessed_total_value not found, using fallback segmentation\n",
      "2 segments created\n",
      "  mid_small: 63,633\n",
      "  mid_large: 63,625\n",
      "  mid_small: 63,633→48,757 (23.4% filtered)\n",
      "  mid_small: 14,627 test | MAE:$529,126 | MAPE:24.39% | R²:0.200\n",
      "  mid_large: 63,625→49,085 (22.9% filtered)\n",
      "  mid_large: 14,725 test | MAE:$310,870 | MAPE:19.92% | R²:0.512\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\n",
      "1 records | 0.0MB | Price:sale_price ID:property_id\n",
      "\n",
      "============================================================\n",
      "GENERATING PREDICTIONS FOR NEW PROPERTIES\n",
      "============================================================\n",
      "Input properties: 1\n",
      "  Warning: Segment 'premium_small' not in trained models, using fallback\n",
      "  mid_small: 1 properties predicted\n",
      "\n",
      "✓ Generated 1 predictions\n",
      "  Validation metrics (1 properties):\n",
      "  MAE: $432,836 | MAPE: 82.29% | R²: nan\n",
      "\n",
      "Saving results...\n",
      "✓ New predictions CSV: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/assessed_new_predictions_20251223_234257.csv\n",
      "✓ Excel: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/assessed_segmented_20251223_234257.xlsx\n",
      "✓ CSVs saved with timestamp: 20251223_234257\n",
      "✓ Per-segment feature importance saved for 2 segments\n",
      "\n",
      "============================================================\n",
      "✓ TRAINING COMPLETE in 9.8s\n",
      "  Test set: 29,352 properties | 2 segments\n",
      "  R²: 0.2755 | MAE: $419,633 | MAPE: 22.15%\n",
      "  New predictions: 1 properties\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T05:02:27.514918Z",
     "start_time": "2025-12-24T05:02:16.594925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import time, os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Config - FIXED PARAMETERS\n",
    "INCLUDE_MLS, INCLUDE_CENSUS, INCLUDE_NEIGHBORHOOD, INCLUDE_IMAGE = True, True, True, False\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS = 20000, 0.3, 42, -1, 8\n",
    "N_EST, QUANTILES, MAX_SEGMENTS = 100, [0.1, 0.5, 0.9], 7  # MAX 7 SEGMENTS\n",
    "MIN_ULTRA_HIGH = 5000000  # $5M value indicator threshold for ultra-high segment\n",
    "\n",
    "# INPUT PATHS\n",
    "TRAINING_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "PREDICTION_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "BASE_FEATS = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"effective_year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\"fireplace_code\",\"latitude\",\"longitude\",\"geo_cluster\"]\n",
    "ENG_FEATS = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\"]\n",
    "PRIOR_FEATS = [\"prior_sale_price\",\"prior_price_per_sqft\",\"sqft_per_prior_dollar\",\"years_since_last_sale\",\"expected_appreciation\",\"has_prior_sale\",\"recently_sold\"]\n",
    "CLUSTER_FEATS = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "CENSUS_FEATS = [\"total_population_25plus\",\"male_bachelors_degree\",\"female_bachelors_degree\",\"pct_bachelors_degree\",\"median_earnings_total\",\"median_earnings_male\",\"median_earnings_female\",\"median_household_income\",\"median_home_value\",\"median_gross_rent\",\"owner_occupied_units\",\"renter_occupied_units\",\"pct_owner_occupied\",\"occupied_units\",\"vacant_units\",\"median_age\",\"civilian_employed\",\"civilian_unemployed\",\"unemployment_rate\",\"income_education_score\"]\n",
    "ELECT_FEATS = [\"votes_gop\",\"votes_dem\",\"total_votes\",\"per_gop\",\"per_dem\",\"per_point_diff\",\"dem_margin\",\"rep_margin\"]\n",
    "IMG_FEATS = [\"topic_1\",\"topic_2\",\"topic_3\",\"topic_4\",\"topic_5\",\"topic_6\",\"topic_7\",\"topic_8\",\"topic_9\",\"topic_10\",\"gran_c_in\",\"gran_c_ex\",\"gran_c\",\"high_c_in\",\"high_c_ex\",\"high_c\"]\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8') if set(df[c].dropna().unique()).issubset({0,1}) else df[c].astype('int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return optimize_dtypes(df), y_col, id_col, state_col\n",
    "\n",
    "def assign_segments_robust(df):\n",
    "    \"\"\"\n",
    "    ROBUST segmentation using MULTIPLE lagged indicators with fallbacks\n",
    "    Priority: 1) Prior sale price, 2) Assessed value, 3) Median home value (census), 4) Sqft estimate\n",
    "    \"\"\"\n",
    "    segments = pd.Series(['mid'] * len(df), index=df.index)\n",
    "\n",
    "    # Build a composite \"value indicator\" from multiple lagged sources\n",
    "    value_indicator = pd.Series([np.nan] * len(df), index=df.index)\n",
    "    source_used = pd.Series(['none'] * len(df), index=df.index)\n",
    "\n",
    "    # Priority 1: Prior sale price (if recent)\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        # Use prior sale if within last 10 years, adjust for appreciation\n",
    "        recent_prior = (df['years_since_last_sale'] <= 10) & (df['prior_sale_price'] > 0)\n",
    "        if recent_prior.sum() > 0:\n",
    "            years = df.loc[recent_prior, 'years_since_last_sale'].fillna(5)\n",
    "            appreciated_value = df.loc[recent_prior, 'prior_sale_price'] * (1.04 ** years)\n",
    "            value_indicator[recent_prior] = appreciated_value\n",
    "            source_used[recent_prior] = 'prior_sale'\n",
    "\n",
    "    # Priority 2: Assessed value (if reasonable)\n",
    "    if 'assessed_total_value' in df.columns:\n",
    "        # Only use assessed value if it seems reasonable (>$10K and <$50M)\n",
    "        reasonable_assessed = (df['assessed_total_value'] > 10000) & (df['assessed_total_value'] < 50000000)\n",
    "        missing_indicator = value_indicator.isna()\n",
    "        use_assessed = reasonable_assessed & missing_indicator\n",
    "        if use_assessed.sum() > 0:\n",
    "            # Assessed values are typically 70-90% of market value, so adjust up\n",
    "            value_indicator[use_assessed] = df.loc[use_assessed, 'assessed_total_value'] * 1.15\n",
    "            source_used[use_assessed] = 'assessed'\n",
    "\n",
    "    # Priority 3: Median home value (census tract)\n",
    "    if 'median_home_value' in df.columns:\n",
    "        missing_indicator = value_indicator.isna()\n",
    "        has_census = df['median_home_value'].notna() & (df['median_home_value'] > 0)\n",
    "        use_census = has_census & missing_indicator\n",
    "        if use_census.sum() > 0:\n",
    "            value_indicator[use_census] = df.loc[use_census, 'median_home_value']\n",
    "            source_used[use_census] = 'census'\n",
    "\n",
    "    # Priority 4: Fallback - use living sqft * $150/sqft as rough estimate\n",
    "    if 'living_sqft' in df.columns:\n",
    "        missing_indicator = value_indicator.isna()\n",
    "        has_sqft = df['living_sqft'].notna() & (df['living_sqft'] > 0)\n",
    "        use_sqft = has_sqft & missing_indicator\n",
    "        if use_sqft.sum() > 0:\n",
    "            value_indicator[use_sqft] = df.loc[use_sqft, 'living_sqft'] * 150\n",
    "            source_used[use_sqft] = 'sqft_estimate'\n",
    "\n",
    "    # Final fallback: use median of all available indicators\n",
    "    if value_indicator.isna().sum() > 0:\n",
    "        fallback_value = value_indicator.median()\n",
    "        value_indicator = value_indicator.fillna(fallback_value)\n",
    "        source_used[value_indicator.isna()] = 'median_fallback'\n",
    "\n",
    "    # Now segment based on value_indicator\n",
    "    ultra_high_mask = value_indicator >= MIN_ULTRA_HIGH\n",
    "\n",
    "    if ultra_high_mask.sum() >= 50:\n",
    "        # Enough ultra-high properties for separate segment\n",
    "        non_ultra = value_indicator[~ultra_high_mask]\n",
    "        if len(non_ultra) > 0:\n",
    "            q33, q67 = non_ultra.quantile([.33, .67])\n",
    "            price_tier = pd.Series(['mid'] * len(df), index=df.index)\n",
    "            price_tier[value_indicator < q33] = 'budget'\n",
    "            price_tier[(value_indicator >= q33) & (value_indicator < q67)] = 'mid'\n",
    "            price_tier[(value_indicator >= q67) & (~ultra_high_mask)] = 'premium'\n",
    "            price_tier[ultra_high_mask] = 'ultra_high'\n",
    "        else:\n",
    "            price_tier = pd.Series(['mid'] * len(df), index=df.index)\n",
    "    else:\n",
    "        # Standard quartile segmentation\n",
    "        q33, q67 = value_indicator.quantile([.33, .67])\n",
    "        price_tier = pd.Series(['mid'] * len(df), index=df.index)\n",
    "        price_tier[value_indicator < q33] = 'budget'\n",
    "        price_tier[(value_indicator >= q33) & (value_indicator < q67)] = 'mid'\n",
    "        price_tier[value_indicator >= q67] = 'premium'\n",
    "\n",
    "    # Size tier\n",
    "    if 'living_sqft' in df.columns:\n",
    "        sqft = df['living_sqft'].fillna(df['living_sqft'].median())\n",
    "        sqft_median = sqft.median()\n",
    "        size_tier = pd.Series(['small'] * len(df), index=df.index)\n",
    "        size_tier[sqft > sqft_median] = 'large'\n",
    "    else:\n",
    "        size_tier = pd.Series(['small'] * len(df), index=df.index)\n",
    "\n",
    "    # Combine segments\n",
    "    segments = price_tier + '_' + size_tier\n",
    "    if ultra_high_mask.sum() >= 50:\n",
    "        segments[ultra_high_mask] = 'ultra_high'\n",
    "\n",
    "    # Store the value_indicator and source for diagnostics\n",
    "    df['_value_indicator'] = value_indicator\n",
    "    df['_value_source'] = source_used\n",
    "\n",
    "    return segments\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    # Standard features\n",
    "    if 'living_sqft' in df.columns and 'bedrooms' in df.columns:\n",
    "        df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns:\n",
    "            df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns:\n",
    "            df['lot_acres'] = df['lot_sqft']/43560\n",
    "\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'] = 2024-df['year_built']\n",
    "        df['is_new'] = (df['property_age']<=5).astype('int8')\n",
    "        df['age_squared'] = df['property_age']**2\n",
    "\n",
    "    if 'garage_spaces' in df.columns:\n",
    "        df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "\n",
    "    if 'living_sqft' in df.columns:\n",
    "        df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "\n",
    "    # NEW: Composite value features\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        df['prior_appreciated'] = df['prior_sale_price'] * (1.04 ** df['years_since_last_sale'].fillna(5))\n",
    "\n",
    "    if 'assessed_total_value' in df.columns and 'living_sqft' in df.columns:\n",
    "        # Only use if assessed value is reasonable\n",
    "        reasonable = (df['assessed_total_value'] > 10000) & (df['assessed_total_value'] < 50000000)\n",
    "        df['assessed_per_sqft'] = 0\n",
    "        df.loc[reasonable, 'assessed_per_sqft'] = df.loc[reasonable, 'assessed_total_value'] / (df.loc[reasonable, 'living_sqft'] + 1)\n",
    "\n",
    "    # Ratio of assessed to census median (if both available)\n",
    "    if 'assessed_total_value' in df.columns and 'median_home_value' in df.columns:\n",
    "        reasonable_assessed = (df['assessed_total_value'] > 10000)\n",
    "        reasonable_census = (df['median_home_value'] > 10000)\n",
    "        reasonable = reasonable_assessed & reasonable_census\n",
    "        df['assessed_to_census_ratio'] = 1.0\n",
    "        df.loc[reasonable, 'assessed_to_census_ratio'] = df.loc[reasonable, 'assessed_total_value'] / (df.loc[reasonable, 'median_home_value'] + 1)\n",
    "\n",
    "    if 'assessed_land_value' in df.columns and 'assessed_total_value' in df.columns:\n",
    "        reasonable = (df['assessed_total_value'] > 10000) & (df['assessed_land_value'] > 0)\n",
    "        df['land_to_total_ratio'] = 0\n",
    "        df.loc[reasonable, 'land_to_total_ratio'] = df.loc[reasonable, 'assessed_land_value'] / (df.loc[reasonable, 'assessed_total_value'] + 1)\n",
    "\n",
    "    if INCLUDE_CENSUS and all(c in df.columns for c in ['median_household_income','pct_bachelors_degree']):\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'living_sqft' in df.columns:\n",
    "        df['prior_price_per_sqft'] = df['prior_sale_price']/(df['living_sqft']+1)\n",
    "        df['sqft_per_prior_dollar'] = df['living_sqft']/(df['prior_sale_price']+1)\n",
    "\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        if 'prior_sale_price' in df.columns:\n",
    "            df['expected_appreciation'] = df['prior_sale_price']*(1.04)**df['years_since_last_sale']\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df.loc[missing,'median_home_value'] if 'median_home_value' in df.columns and INCLUDE_CENSUS else df['prior_sale_price'].median()\n",
    "\n",
    "    if 'years_since_last_sale' in df.columns:\n",
    "        df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "\n",
    "    if with_price and y_col in df.columns and 'living_sqft' in df.columns:\n",
    "        df['price_per_sqft'] = df[y_col]/(df['living_sqft']+1)\n",
    "        df['sqft_per_dollar'] = df['living_sqft']/(df[y_col]+1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df, kmeans_model=None):\n",
    "    \"\"\"Apply geo clustering - can use existing model for prediction\"\"\"\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans_model\n",
    "\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum() < N_CLUSTERS:\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans_model\n",
    "\n",
    "    df['geo_cluster'] = 0\n",
    "\n",
    "    if kmeans_model is None:\n",
    "        # Training: fit new model\n",
    "        kmeans_model = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "        df.loc[valid,'geo_cluster'] = kmeans_model.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    else:\n",
    "        # Prediction: use existing model\n",
    "        df.loc[valid,'geo_cluster'] = kmeans_model.predict(df.loc[valid,['latitude','longitude']])\n",
    "\n",
    "    return df, kmeans_model\n",
    "\n",
    "def add_cluster_feats(train, test, y_col):\n",
    "    if 'geo_cluster' not in train.columns or y_col not in train.columns:\n",
    "        for d in [train,test]:\n",
    "            d['cluster_avg_price'] = d['cluster_med_price'] = train[y_col].median() if y_col in train.columns else 0\n",
    "        return train, test\n",
    "\n",
    "    stats = train.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "    stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "    train = train.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    test = test.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    return train, test\n",
    "\n",
    "def filter_outliers(df, name, y_col):\n",
    "    orig = len(df)\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)]\n",
    "\n",
    "    if 'sqft_per_dollar' in df.columns:\n",
    "        df = df[df['sqft_per_dollar']<=df['sqft_per_dollar'].quantile(.95)]\n",
    "\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        df = df.drop(columns=['price_per_sqft','sqft_per_dollar'])\n",
    "\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "\n",
    "    if 'year_built' in df.columns:\n",
    "        df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "\n",
    "    if len(df)>=100:\n",
    "        try:\n",
    "            feats = [c for c in ['living_sqft','lot_sqft',y_col,'year_built'] if c in df.columns]\n",
    "            if len(feats)>=3:\n",
    "                X = df[feats].fillna(df[feats].median())\n",
    "                df = df[IsolationForest(contamination=.05,random_state=RAND_STATE,n_jobs=N_JOBS).fit_predict(X)==1]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    pct_filt = (orig-len(df))/orig*100 if orig>0 else 0\n",
    "    if pct_filt > 0:\n",
    "        print(f\"  {name}: {orig:,}→{len(df):,} ({pct_filt:.1f}% filtered)\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def train_model(X, y, q):\n",
    "    return XGBRegressor(\n",
    "        objective='reg:quantileerror',\n",
    "        quantile_alpha=q,\n",
    "        n_estimators=N_EST,\n",
    "        learning_rate=.1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        subsample=.8,\n",
    "        colsample_bytree=.8,\n",
    "        random_state=RAND_STATE,\n",
    "        n_jobs=N_JOBS,\n",
    "        tree_method='hist'\n",
    "    ).fit(X, y, verbose=False)\n",
    "\n",
    "def get_feature_importance_per_segment(model, feat_names, top_n=20):\n",
    "    \"\"\"Extract feature importance for a single segment model\"\"\"\n",
    "    scores = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    importance_list = []\n",
    "    for k, v in scores.items():\n",
    "        idx = int(k[1:])\n",
    "        if idx < len(feat_names):\n",
    "            importance_list.append((feat_names[idx], v))\n",
    "\n",
    "    # Sort and normalize\n",
    "    importance_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    total_gain = sum(v for _, v in importance_list)\n",
    "\n",
    "    if total_gain > 0:\n",
    "        importance_df = pd.DataFrame([\n",
    "            {'feature': feat, 'gain': gain, 'importance': gain/total_gain}\n",
    "            for feat, gain in importance_list[:top_n]\n",
    "        ])\n",
    "    else:\n",
    "        importance_df = pd.DataFrame(columns=['feature', 'gain', 'importance'])\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "def feature_importance(models, feat_names, metrics):\n",
    "    \"\"\"Global feature importance weighted by segment size\"\"\"\n",
    "    rows = []\n",
    "    for seg, mdl in models.items():\n",
    "        scores = mdl['q50'].get_booster().get_score(importance_type=\"gain\")\n",
    "        w = metrics[seg][\"n_test\"]\n",
    "        for k,v in scores.items():\n",
    "            idx = int(k[1:])\n",
    "            if idx<len(feat_names):\n",
    "                rows.append((feat_names[idx],v,w))\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"feature\",\"total_gain\",\"importance\"])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"feature\",\"gain\",\"weight\"])\n",
    "\n",
    "    # Calculate both total_gain and weighted importance\n",
    "    out = df.assign(wg=df[\"gain\"]*df[\"weight\"]).groupby(\"feature\",as_index=False).agg(\n",
    "        total_gain=(\"gain\",\"sum\"),\n",
    "        weighted_gain=(\"wg\",\"sum\")\n",
    "    ).sort_values(\"weighted_gain\",ascending=False)\n",
    "\n",
    "    out[\"importance\"] = out[\"weighted_gain\"]/out[\"weighted_gain\"].sum()\n",
    "\n",
    "    return out[[\"feature\",\"total_gain\",\"importance\"]].head(100)\n",
    "\n",
    "def prepare_data(df, y_col, id_col, state_col, for_training=True):\n",
    "    print(f\"\\nPreparing data...\")\n",
    "    print(f\"Segmentation: ROBUST multi-source value indicator (no leakage)\")\n",
    "\n",
    "    if for_training:\n",
    "        df = df[df[y_col]>=MIN_PRICE]\n",
    "        print(f\"{len(df):,} records after price filter\")\n",
    "\n",
    "    df, kmeans_model = geo_cluster(engineer(df, y_col))\n",
    "\n",
    "    feat_groups = []\n",
    "    if INCLUDE_MLS:\n",
    "        feat_groups.extend([BASE_FEATS,ENG_FEATS,PRIOR_FEATS,CLUSTER_FEATS])\n",
    "    if INCLUDE_CENSUS:\n",
    "        feat_groups.append(CENSUS_FEATS)\n",
    "    if INCLUDE_NEIGHBORHOOD:\n",
    "        feat_groups.append(ELECT_FEATS)\n",
    "    if INCLUDE_IMAGE:\n",
    "        feat_groups.append(IMG_FEATS)\n",
    "\n",
    "    all_feats = [f for g in feat_groups for f in g]\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "\n",
    "    # Add new value-based features if they exist\n",
    "    value_feats = ['prior_appreciated', 'assessed_per_sqft', 'assessed_to_census_ratio', 'land_to_total_ratio']\n",
    "    for vf in value_feats:\n",
    "        if vf in df.columns and vf not in feats:\n",
    "            feats.append(vf)\n",
    "\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "\n",
    "    if for_training:\n",
    "        return df.dropna(subset=[y_col]), feats, kmeans_model\n",
    "    else:\n",
    "        return df, feats, kmeans_model\n",
    "\n",
    "def train_segments(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining segmented models on {len(df):,} properties\")\n",
    "    print(\"Segmentation using ROBUST multi-source value indicator\")\n",
    "\n",
    "    # Segment using robust approach\n",
    "    df['seg'] = assign_segments_robust(df)\n",
    "\n",
    "    # Diagnostic: Show what data sources were used\n",
    "    if '_value_indicator' in df.columns and '_value_source' in df.columns:\n",
    "        source_counts = df['_value_source'].value_counts()\n",
    "        print(f\"\\nValue indicator sources:\")\n",
    "        for source, count in source_counts.items():\n",
    "            print(f\"  {source}: {count:,} ({100*count/len(df):.1f}%)\")\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    print(f\"\\n{len(seg_cnts)} segments created\")\n",
    "\n",
    "    for seg, cnt in seg_cnts.head(MAX_SEGMENTS).items():\n",
    "        if '_value_indicator' in df.columns and seg != 'other':\n",
    "            seg_df = df[df['seg']==seg]\n",
    "            if len(seg_df) > 0:\n",
    "                min_val = seg_df['_value_indicator'].min()\n",
    "                max_val = seg_df['_value_indicator'].max()\n",
    "                median_val = seg_df['_value_indicator'].median()\n",
    "                print(f\"  {seg}: {cnt:,} (value indicator: ${min_val:,.0f}-${max_val:,.0f}, median ${median_val:,.0f})\")\n",
    "        else:\n",
    "            print(f\"  {seg}: {cnt:,}\")\n",
    "\n",
    "    # Merge small segments\n",
    "    small = seg_cnts[seg_cnts<50].index\n",
    "    if len(small)>0:\n",
    "        df.loc[df['seg'].isin(small),'seg'] = 'other'\n",
    "        print(f\"\\nMerged {len(small)} small segments into 'other'\")\n",
    "\n",
    "    # If still too many segments, merge smallest ones\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    if len(seg_cnts) > MAX_SEGMENTS:\n",
    "        keep_segs = seg_cnts.head(MAX_SEGMENTS-1).index\n",
    "        df.loc[~df['seg'].isin(keep_segs),'seg'] = 'other'\n",
    "        print(f\"Consolidated to {MAX_SEGMENTS} segments maximum\")\n",
    "\n",
    "    # Clean up temp columns\n",
    "    if '_value_indicator' in df.columns:\n",
    "        df = df.drop(columns=['_value_indicator'])\n",
    "    if '_value_source' in df.columns:\n",
    "        df = df.drop(columns=['_value_source'])\n",
    "\n",
    "    models, metrics, preds_list, segment_importances = {}, {}, [], {}\n",
    "\n",
    "    for seg in df['seg'].unique():\n",
    "        seg_df = df[df['seg']==seg].copy()\n",
    "        if len(seg_df)<50: continue\n",
    "\n",
    "        seg_df = filter_outliers(seg_df, seg, y_col)\n",
    "        if len(seg_df)<50: continue\n",
    "\n",
    "        train_idx = seg_df.sample(frac=1-TEST_SIZE,random_state=RAND_STATE).index\n",
    "        train_df, test_df = seg_df.loc[train_idx].copy(), seg_df.loc[seg_df.index.difference(train_idx)].copy()\n",
    "        train_df, test_df = add_cluster_feats(train_df, test_df, y_col)\n",
    "\n",
    "        X_tr,y_tr = train_df[feats].values, train_df[y_col].values\n",
    "        X_te,y_te = test_df[feats].values, test_df[y_col].values\n",
    "        ids_te = test_df[id_col].values\n",
    "        states_te = test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "\n",
    "        seg_models, seg_preds = {}, []\n",
    "        for q in QUANTILES:\n",
    "            m = train_model(X_tr,y_tr,q)\n",
    "            seg_models[f\"q{int(q*100)}\"] = m\n",
    "            seg_preds.append(m.predict(X_te))\n",
    "\n",
    "        models[seg] = seg_models\n",
    "\n",
    "        # Get feature importance for this segment\n",
    "        segment_importances[seg] = get_feature_importance_per_segment(seg_models['q50'], feats, top_n=20)\n",
    "\n",
    "        y_pred = seg_preds[1]\n",
    "        mae,mape = mean_absolute_error(y_te,y_pred), np.mean(np.abs((y_te-y_pred)/y_te))*100\n",
    "        r2,cov = r2_score(y_te,y_pred), np.mean((y_te>=seg_preds[0])&(y_te<=seg_preds[2]))*100\n",
    "        metrics[seg] = {'n_train':len(X_tr),'n_test':len(X_te),'mae':mae,'mape':mape,'r2':r2,'cov':cov,'p_min':seg_df[y_col].min(),'p_max':seg_df[y_col].max(),'p_med':seg_df[y_col].median()}\n",
    "        print(f\"  {seg}: {len(test_df):,} test | MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.3f}\")\n",
    "\n",
    "        preds_list.append(pd.DataFrame({\n",
    "            'property_id':ids_te,\n",
    "            'state':states_te,\n",
    "            'actual':y_te,\n",
    "            'predicted':y_pred,\n",
    "            'pred_lower':seg_preds[0],\n",
    "            'pred_upper':seg_preds[2],\n",
    "            'segment':seg\n",
    "        }))\n",
    "\n",
    "    return {\n",
    "        'models': models,\n",
    "        'metrics': metrics,\n",
    "        'predictions': pd.concat(preds_list),\n",
    "        'feature_importance': feature_importance(models, feats, metrics),\n",
    "        'segment_importances': segment_importances,\n",
    "        'feature_names': feats\n",
    "    }\n",
    "\n",
    "def predict_new_properties(pred_df, models, feats, y_col, id_col, state_col, kmeans_model, train_cluster_stats):\n",
    "    \"\"\"Generate predictions for new properties using trained models\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"GENERATING PREDICTIONS FOR NEW PROPERTIES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Input properties: {len(pred_df):,}\")\n",
    "\n",
    "    # Engineer features (no price filtering for prediction)\n",
    "    pred_df = engineer(pred_df, y_col)\n",
    "    pred_df, _ = geo_cluster(pred_df, kmeans_model)\n",
    "\n",
    "    # Add cluster features from training data\n",
    "    if 'geo_cluster' in pred_df.columns and train_cluster_stats is not None:\n",
    "        pred_df = pred_df.merge(train_cluster_stats, on='geo_cluster', how='left')\n",
    "        median_price = train_cluster_stats['cluster_avg_price'].median()\n",
    "        pred_df['cluster_avg_price'] = pred_df['cluster_avg_price'].fillna(median_price)\n",
    "        pred_df['cluster_med_price'] = pred_df['cluster_med_price'].fillna(median_price)\n",
    "\n",
    "    # Fill missing features with median or 0\n",
    "    for feat in feats:\n",
    "        if feat not in pred_df.columns:\n",
    "            pred_df[feat] = 0\n",
    "        else:\n",
    "            pred_df[feat] = pred_df[feat].fillna(pred_df[feat].median() if pred_df[feat].notna().sum() > 0 else 0)\n",
    "\n",
    "    # Assign segments based on ROBUST value indicator\n",
    "    pred_df['seg'] = assign_segments_robust(pred_df)\n",
    "\n",
    "    # Show value indicator sources for prediction data\n",
    "    if '_value_source' in pred_df.columns:\n",
    "        source_counts = pred_df['_value_source'].value_counts()\n",
    "        print(f\"\\nValue indicator sources (prediction data):\")\n",
    "        for source, count in source_counts.items():\n",
    "            print(f\"  {source}: {count:,} ({100*count/len(pred_df):.1f}%)\")\n",
    "\n",
    "    # Generate predictions\n",
    "    preds_list = []\n",
    "    for seg in pred_df['seg'].unique():\n",
    "        seg_df = pred_df[pred_df['seg']==seg].copy()\n",
    "\n",
    "        if seg not in models:\n",
    "            print(f\"  Warning: Segment '{seg}' not in trained models, using fallback\")\n",
    "            # Use the closest segment as fallback\n",
    "            if 'premium' in seg and 'premium_large' in models:\n",
    "                seg = 'premium_large'\n",
    "            elif 'ultra_high' in seg and 'ultra_high' in models:\n",
    "                seg = 'ultra_high'\n",
    "            elif 'mid_large' in models:\n",
    "                seg = 'mid_large'\n",
    "            else:\n",
    "                seg = list(models.keys())[0]\n",
    "\n",
    "        X = seg_df[feats].values\n",
    "        ids = seg_df[id_col].values\n",
    "        states = seg_df[state_col].values if state_col and state_col in seg_df.columns else ['Unknown']*len(seg_df)\n",
    "\n",
    "        # Get predictions from quantile models\n",
    "        pred_lower = models[seg]['q10'].predict(X)\n",
    "        pred_mid = models[seg]['q50'].predict(X)\n",
    "        pred_upper = models[seg]['q90'].predict(X)\n",
    "\n",
    "        actual = seg_df[y_col].values if y_col in seg_df.columns else [np.nan]*len(seg_df)\n",
    "\n",
    "        # Add value indicator to output for reference\n",
    "        value_ind = seg_df['_value_indicator'].values if '_value_indicator' in seg_df.columns else [np.nan]*len(seg_df)\n",
    "        value_src = seg_df['_value_source'].values if '_value_source' in seg_df.columns else ['unknown']*len(seg_df)\n",
    "\n",
    "        preds_list.append(pd.DataFrame({\n",
    "            'property_id': ids,\n",
    "            'state': states,\n",
    "            'value_indicator': value_ind,\n",
    "            'value_source': value_src,\n",
    "            'actual': actual,\n",
    "            'predicted': pred_mid,\n",
    "            'pred_lower': pred_lower,\n",
    "            'pred_upper': pred_upper,\n",
    "            'segment': seg,\n",
    "            'error': [actual[i] - pred_mid[i] if not np.isnan(actual[i]) else np.nan for i in range(len(actual))],\n",
    "            'pct_error': [100 * (actual[i] - pred_mid[i]) / actual[i] if not np.isnan(actual[i]) and actual[i] != 0 else np.nan for i in range(len(actual))]\n",
    "        }))\n",
    "\n",
    "        print(f\"  {seg}: {len(seg_df):,} properties predicted\")\n",
    "\n",
    "    result_df = pd.concat(preds_list, ignore_index=True)\n",
    "    print(f\"\\n✓ Generated {len(result_df):,} predictions\")\n",
    "\n",
    "    # Clean up temp columns\n",
    "    if '_value_indicator' in pred_df.columns:\n",
    "        pred_df = pred_df.drop(columns=['_value_indicator'])\n",
    "    if '_value_source' in pred_df.columns:\n",
    "        pred_df = pred_df.drop(columns=['_value_source'])\n",
    "\n",
    "    # Calculate metrics if actuals are available\n",
    "    valid_actuals = result_df['actual'].notna().sum()\n",
    "    if valid_actuals > 0:\n",
    "        valid_preds = result_df[result_df['actual'].notna()].copy()\n",
    "        mae = mean_absolute_error(valid_preds['actual'], valid_preds['predicted'])\n",
    "        mape = np.mean(np.abs((valid_preds['actual'] - valid_preds['predicted']) / valid_preds['actual'])) * 100\n",
    "        r2 = r2_score(valid_preds['actual'], valid_preds['predicted'])\n",
    "        print(f\"  Validation metrics ({valid_actuals} properties):\")\n",
    "        print(f\"  MAE: ${mae:,.0f} | MAPE: {mape:.2f}% | R²: {r2:.4f}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def save_results(results, out_dir, new_predictions=None):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi = results['predictions'], results['metrics'], results['feature_importance']\n",
    "    seg_importances = results['segment_importances']\n",
    "\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    # Summary\n",
    "    ws = wb.create_sheet(\"Summary\",0)\n",
    "    ws['A1'], ws['A1'].font = 'ROBUST MULTI-SOURCE SEGMENTED AVM', Font(bold=True,size=14)\n",
    "    ws['A2'] = 'Segmentation: Multi-source value indicator (prior sale, assessed, census)'\n",
    "    ws['A2'].font = Font(italic=True, size=10)\n",
    "\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "\n",
    "    data = [\n",
    "        ['Metric','Value'],\n",
    "        ['Properties',len(preds)],\n",
    "        ['Segments',len(metrics)],\n",
    "        ['R²',f'{r2:.4f}'],\n",
    "        ['MAE',f'${mae:,.0f}'],\n",
    "        ['MAPE%',f'{mape:.2f}%']\n",
    "    ]\n",
    "\n",
    "    if new_predictions is not None:\n",
    "        data.append(['New Predictions', len(new_predictions)])\n",
    "\n",
    "    for i,(k,v) in enumerate(data,5):\n",
    "        ws[f'A{i}'],ws[f'A{i}'].font,ws[f'B{i}'] = k,Font(bold=True),v\n",
    "\n",
    "    # Segments\n",
    "    ws = wb.create_sheet(\"Segments\")\n",
    "    seg_df = pd.DataFrame([{**{'Segment':s},**m} for s,m in metrics.items()])\n",
    "    for i,h in enumerate(seg_df.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(seg_df.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1):\n",
    "            ws.cell(i,j,v)\n",
    "\n",
    "    # Global Feature Importance\n",
    "    ws = wb.create_sheet(\"Global_Feature_Importance\")\n",
    "    ws['A1'] = 'Global Feature Importance (Weighted)'\n",
    "    ws['A1'].font = Font(bold=True, size=12)\n",
    "    for r_idx, row in enumerate(dataframe_to_rows(fi, index=False, header=True), 2):\n",
    "        for c_idx, value in enumerate(row, 1):\n",
    "            cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "            if r_idx == 2:  # Header row\n",
    "                cell.font = Font(bold=True, color='FFFFFF')\n",
    "                cell.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "\n",
    "    # Per-Segment Feature Importance\n",
    "    for seg_name, seg_fi in seg_importances.items():\n",
    "        sheet_name = f\"FI_{seg_name}\"[:31]\n",
    "        ws = wb.create_sheet(sheet_name)\n",
    "        ws['A1'] = f'Feature Importance: {seg_name}'\n",
    "        ws['A1'].font = Font(bold=True, size=12)\n",
    "\n",
    "        for r_idx, row in enumerate(dataframe_to_rows(seg_fi, index=False, header=True), 2):\n",
    "            for c_idx, value in enumerate(row, 1):\n",
    "                cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "                if r_idx == 2:\n",
    "                    cell.font = Font(bold=True, color='FFFFFF')\n",
    "                    cell.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "\n",
    "    # Test Set Predictions\n",
    "    ws = wb.create_sheet(\"Test_Predictions\")\n",
    "    for i,h in enumerate(preds.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(preds.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1):\n",
    "            ws.cell(i,j,v)\n",
    "\n",
    "    # New Predictions\n",
    "    if new_predictions is not None:\n",
    "        ws = wb.create_sheet(\"New_Predictions\")\n",
    "        for i,h in enumerate(new_predictions.columns,1):\n",
    "            c = ws.cell(1,i,h)\n",
    "            c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='4472C4',end_color='4472C4',fill_type='solid')\n",
    "        for i,row in enumerate(new_predictions.itertuples(index=False),2):\n",
    "            for j,v in enumerate(row,1):\n",
    "                ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/robust_segmented_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "    preds.to_csv(f\"{out_dir}/robust_test_predictions_{ts}.csv\",index=False)\n",
    "    seg_df.to_csv(f\"{out_dir}/robust_segments_{ts}.csv\",index=False)\n",
    "    fi.to_csv(f\"{out_dir}/robust_importance_{ts}.csv\",index=False)\n",
    "\n",
    "    # Save per-segment feature importance CSVs\n",
    "    for seg_name, seg_fi in seg_importances.items():\n",
    "        seg_fi.to_csv(f\"{out_dir}/robust_importance_{seg_name}_{ts}.csv\", index=False)\n",
    "\n",
    "    # Save new predictions\n",
    "    if new_predictions is not None:\n",
    "        new_predictions.to_csv(f\"{out_dir}/robust_new_predictions_{ts}.csv\", index=False)\n",
    "        print(f\"✓ New predictions CSV: {out_dir}/robust_new_predictions_{ts}.csv\")\n",
    "\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved with timestamp: {ts}\")\n",
    "    print(f\"✓ Per-segment feature importance saved for {len(seg_importances)} segments\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60)\n",
    "    print(\"ROBUST MULTI-SOURCE SEGMENTED AVM\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Load and prepare training data\n",
    "    df, y_col, id_col, state_col = load_data(TRAINING_INPUT_PATH)\n",
    "    df, feats, kmeans_model = prepare_data(df, y_col, id_col, state_col, for_training=True)\n",
    "\n",
    "    # Train models\n",
    "    results = train_segments(df, feats, y_col, id_col, state_col)\n",
    "\n",
    "    # Get cluster stats for prediction data\n",
    "    train_cluster_stats = None\n",
    "    if 'geo_cluster' in df.columns:\n",
    "        train_cluster_stats = df.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "        train_cluster_stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "\n",
    "    # Generate predictions for new properties if input file provided\n",
    "    new_predictions = None\n",
    "    if PREDICTION_INPUT_PATH:\n",
    "        pred_df, _, _, _ = load_data(PREDICTION_INPUT_PATH)\n",
    "        new_predictions = predict_new_properties(\n",
    "            pred_df, results['models'], feats, y_col, id_col, state_col,\n",
    "            kmeans_model, train_cluster_stats\n",
    "        )\n",
    "\n",
    "    # Save results\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR, new_predictions)\n",
    "\n",
    "    # Print summary\n",
    "    preds = results['predictions']\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ TRAINING COMPLETE in {time.time()-t0:.1f}s\")\n",
    "    print(f\"  Test set: {len(preds):,} properties | {preds['segment'].nunique()} segments\")\n",
    "    print(f\"  R²: {r2:.4f} | MAE: ${mae:,.0f} | MAPE: {mape:.2f}%\")\n",
    "    if new_predictions is not None:\n",
    "        print(f\"  New predictions: {len(new_predictions):,} properties\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "if __name__ == \"__main__\": main()"
   ],
   "id": "d7cae9144ad090cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ROBUST MULTI-SOURCE SEGMENTED AVM\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 129.6MB | Price:sale_price ID:property_id\n",
      "\n",
      "Preparing data...\n",
      "Segmentation: ROBUST multi-source value indicator (no leakage)\n",
      "127,258 records after price filter\n",
      "51/57 features available\n",
      "\n",
      "Training segmented models on 127,258 properties\n",
      "Segmentation using ROBUST multi-source value indicator\n",
      "\n",
      "Value indicator sources:\n",
      "  census: 127,258 (100.0%)\n",
      "\n",
      "6 segments created\n",
      "  budget_small: 26,330 (value indicator: $12,000-$383,400, median $157,267)\n",
      "  premium_large: 25,173 (value indicator: $504,650-$2,000,001, median $878,633)\n",
      "  mid_large: 22,791 (value indicator: $384,950-$504,225, median $434,750)\n",
      "  mid_small: 20,420 (value indicator: $384,950-$504,225, median $434,750)\n",
      "  premium_small: 16,883 (value indicator: $504,650-$2,000,001, median $800,350)\n",
      "  budget_large: 15,661 (value indicator: $21,250-$383,580, median $260,467)\n",
      "  budget_small: 26,330→19,763 (24.9% filtered)\n",
      "  budget_small: 5,929 test | MAE:$1,000,139 | MAPE:33.83% | R²:0.277\n",
      "  mid_large: 22,791→17,756 (22.1% filtered)\n",
      "  mid_large: 5,327 test | MAE:$335,143 | MAPE:20.62% | R²:0.499\n",
      "  mid_small: 20,420→15,793 (22.7% filtered)\n",
      "  mid_small: 4,738 test | MAE:$509,146 | MAPE:24.81% | R²:0.280\n",
      "  budget_large: 15,661→11,727 (25.1% filtered)\n",
      "  budget_large: 3,518 test | MAE:$214,109 | MAPE:15.84% | R²:0.374\n",
      "  premium_small: 16,883→13,173 (22.0% filtered)\n",
      "  premium_small: 3,952 test | MAE:$172,995 | MAPE:14.20% | R²:0.290\n",
      "  premium_large: 25,173→19,538 (22.4% filtered)\n",
      "  premium_large: 5,861 test | MAE:$321,015 | MAPE:19.99% | R²:0.568\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\n",
      "1 records | 0.0MB | Price:sale_price ID:property_id\n",
      "\n",
      "============================================================\n",
      "GENERATING PREDICTIONS FOR NEW PROPERTIES\n",
      "============================================================\n",
      "Input properties: 1\n",
      "\n",
      "Value indicator sources (prediction data):\n",
      "  census: 1 (100.0%)\n",
      "  premium_small: 1 properties predicted\n",
      "\n",
      "✓ Generated 1 predictions\n",
      "  Validation metrics (1 properties):\n",
      "  MAE: $350,534 | MAPE: 66.64% | R²: nan\n",
      "\n",
      "Saving results...\n",
      "✓ New predictions CSV: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/robust_new_predictions_20251224_000225.csv\n",
      "✓ Excel: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/robust_segmented_20251224_000225.xlsx\n",
      "✓ CSVs saved with timestamp: 20251224_000225\n",
      "✓ Per-segment feature importance saved for 6 segments\n",
      "\n",
      "============================================================\n",
      "✓ TRAINING COMPLETE in 10.9s\n",
      "  Test set: 29,325 properties | 6 segments\n",
      "  R²: 0.3676 | MAE: $458,512 | MAPE: 22.40%\n",
      "  New predictions: 1 properties\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T05:20:38.518690Z",
     "start_time": "2025-12-24T05:20:27.947771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import time, os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Config - FIXED PARAMETERS\n",
    "INCLUDE_MLS, INCLUDE_CENSUS, INCLUDE_NEIGHBORHOOD, INCLUDE_IMAGE = True, True, True, False\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS = 20000, 0.3, 42, -1, 8\n",
    "N_EST, QUANTILES, MAX_SEGMENTS = 100, [0.1, 0.5, 0.9], 7  # MAX 7 SEGMENTS\n",
    "\n",
    "# INPUT PATHS\n",
    "TRAINING_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "PREDICTION_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "BASE_FEATS = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"effective_year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\"fireplace_code\",\"latitude\",\"longitude\",\"geo_cluster\"]\n",
    "ENG_FEATS = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\"]\n",
    "PRIOR_FEATS = [\"prior_sale_price\",\"prior_price_per_sqft\",\"sqft_per_prior_dollar\",\"years_since_last_sale\",\"expected_appreciation\",\"has_prior_sale\",\"recently_sold\"]\n",
    "CLUSTER_FEATS = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "CENSUS_FEATS = [\"total_population_25plus\",\"male_bachelors_degree\",\"female_bachelors_degree\",\"pct_bachelors_degree\",\"median_earnings_total\",\"median_earnings_male\",\"median_earnings_female\",\"median_household_income\",\"median_home_value\",\"median_gross_rent\",\"owner_occupied_units\",\"renter_occupied_units\",\"pct_owner_occupied\",\"occupied_units\",\"vacant_units\",\"median_age\",\"civilian_employed\",\"civilian_unemployed\",\"unemployment_rate\",\"income_education_score\"]\n",
    "ELECT_FEATS = [\"votes_gop\",\"votes_dem\",\"total_votes\",\"per_gop\",\"per_dem\",\"per_point_diff\",\"dem_margin\",\"rep_margin\"]\n",
    "IMG_FEATS = [\"topic_1\",\"topic_2\",\"topic_3\",\"topic_4\",\"topic_5\",\"topic_6\",\"topic_7\",\"topic_8\",\"topic_9\",\"topic_10\",\"gran_c_in\",\"gran_c_ex\",\"gran_c\",\"high_c_in\",\"high_c_ex\",\"high_c\"]\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8') if set(df[c].dropna().unique()).issubset({0,1}) else df[c].astype('int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return optimize_dtypes(df), y_col, id_col, state_col\n",
    "\n",
    "def assign_segments_improved(df, training_quantiles=None):\n",
    "    \"\"\"\n",
    "    IMPROVED: Better handling of assessed values with variable multipliers\n",
    "    Key changes:\n",
    "    1. Accept low assessed values (might be legit tear-downs, land value)\n",
    "    2. Use variable assessment ratio based on value tier\n",
    "    3. No quality adjustment on census median\n",
    "    4. Finer segmentation (6 tiers)\n",
    "    \"\"\"\n",
    "    segments = pd.Series(['mid_mid'] * len(df), index=df.index)\n",
    "\n",
    "    # ====================================================================\n",
    "    # Build value indicator with IMPROVED priority system\n",
    "    # ====================================================================\n",
    "    value_indicator = pd.Series([np.nan] * len(df), index=df.index)\n",
    "    source_used = pd.Series(['none'] * len(df), index=df.index)\n",
    "\n",
    "    # Priority 1: Prior sale price (if recent)\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        recent_prior = (df['years_since_last_sale'] <= 10) & (df['prior_sale_price'] > 10000)\n",
    "        if recent_prior.sum() > 0:\n",
    "            years = df.loc[recent_prior, 'years_since_last_sale'].fillna(5)\n",
    "            appreciated_value = df.loc[recent_prior, 'prior_sale_price'] * (1.04 ** years)\n",
    "            value_indicator[recent_prior] = appreciated_value\n",
    "            source_used[recent_prior] = 'prior_sale'\n",
    "\n",
    "    # Priority 2: Assessed value (ACCEPT ALL VALUES >$1K, even low ones)\n",
    "    if 'assessed_total_value' in df.columns:\n",
    "        # CHANGED: Don't reject low assessed values - they might be legit!\n",
    "        # Only reject obviously corrupt data (<$1K or >$100M)\n",
    "        reasonable_assessed = (df['assessed_total_value'] > 1000) & (df['assessed_total_value'] < 100000000)\n",
    "        missing_indicator = value_indicator.isna()\n",
    "        use_assessed = reasonable_assessed & missing_indicator\n",
    "\n",
    "        if use_assessed.sum() > 0:\n",
    "            # CHANGED: Use assessment ratio that varies by value\n",
    "            assessed_vals = df.loc[use_assessed, 'assessed_total_value']\n",
    "\n",
    "            # Low assessed values (<$50K) likely ARE the market value (tear-downs, vacant lots)\n",
    "            # High assessed values need typical 1.15x multiplier\n",
    "            multiplier = pd.Series([1.0] * len(assessed_vals), index=assessed_vals.index)\n",
    "            multiplier[assessed_vals < 50000] = 1.0   # Very low = use as-is\n",
    "            multiplier[(assessed_vals >= 50000) & (assessed_vals < 200000)] = 1.1\n",
    "            multiplier[(assessed_vals >= 200000) & (assessed_vals < 500000)] = 1.15\n",
    "            multiplier[assessed_vals >= 500000] = 1.2  # High value = higher assessment ratio\n",
    "\n",
    "            value_indicator[use_assessed] = assessed_vals * multiplier\n",
    "            source_used[use_assessed] = 'assessed'\n",
    "\n",
    "    # Priority 3: Census median (use directly, no quality adjustment)\n",
    "    if 'median_home_value' in df.columns:\n",
    "        missing_indicator = value_indicator.isna()\n",
    "        has_census = df['median_home_value'].notna() & (df['median_home_value'] > 0)\n",
    "        use_census = has_census & missing_indicator\n",
    "        if use_census.sum() > 0:\n",
    "            # Use census median directly - it's already neighborhood-level\n",
    "            value_indicator[use_census] = df.loc[use_census, 'median_home_value']\n",
    "            source_used[use_census] = 'census'\n",
    "\n",
    "    # Priority 4: Estimate from sqft + location\n",
    "    if 'living_sqft' in df.columns:\n",
    "        missing_indicator = value_indicator.isna()\n",
    "        has_sqft = df['living_sqft'].notna() & (df['living_sqft'] > 0)\n",
    "        use_sqft = has_sqft & missing_indicator\n",
    "        if use_sqft.sum() > 0:\n",
    "            # Base price per sqft varies by location (use census median if available)\n",
    "            base_ppsf = pd.Series([150.0] * len(df.loc[use_sqft]), index=df.loc[use_sqft].index)\n",
    "\n",
    "            if 'median_home_value' in df.columns:\n",
    "                # Estimate neighborhood price per sqft from census\n",
    "                census_median = df.loc[use_sqft, 'median_home_value'].fillna(300000)\n",
    "                estimated_nbhd_sqft = 2000  # Assume typical house\n",
    "                base_ppsf = (census_median / estimated_nbhd_sqft).clip(50, 500)\n",
    "\n",
    "            value_indicator[use_sqft] = df.loc[use_sqft, 'living_sqft'] * base_ppsf\n",
    "            source_used[use_sqft] = 'sqft_estimate'\n",
    "\n",
    "    # Final fallback\n",
    "    if value_indicator.isna().sum() > 0:\n",
    "        fallback_value = value_indicator.median()\n",
    "        value_indicator = value_indicator.fillna(fallback_value)\n",
    "        source_used[value_indicator.isna()] = 'median_fallback'\n",
    "\n",
    "    # ====================================================================\n",
    "    # Segment based on value indicator with FINER GRANULARITY\n",
    "    # ====================================================================\n",
    "    if training_quantiles is None:\n",
    "        # Training mode: calculate quantiles (6 tiers)\n",
    "        q20, q40, q60, q80, q95 = value_indicator.quantile([0.20, 0.40, 0.60, 0.80, 0.95])\n",
    "        training_quantiles = {\n",
    "            'value_q20': q20,\n",
    "            'value_q40': q40,\n",
    "            'value_q60': q60,\n",
    "            'value_q80': q80,\n",
    "            'value_q95': q95\n",
    "        }\n",
    "        print(f\"\\nValue indicator quantiles:\")\n",
    "        print(f\"  Q20: ${q20:,.0f} | Q40: ${q40:,.0f} | Q60: ${q60:,.0f} | Q80: ${q80:,.0f} | Q95: ${q95:,.0f}\")\n",
    "    else:\n",
    "        q20, q40, q60, q80, q95 = training_quantiles['value_q20'], training_quantiles['value_q40'], training_quantiles['value_q60'], training_quantiles['value_q80'], training_quantiles['value_q95']\n",
    "\n",
    "    # Create 6 tiers with finer granularity\n",
    "    tier = pd.Series(['mid'] * len(df), index=df.index)\n",
    "    tier[value_indicator < q20] = 'budget'\n",
    "    tier[(value_indicator >= q20) & (value_indicator < q40)] = 'economy'\n",
    "    tier[(value_indicator >= q40) & (value_indicator < q60)] = 'mid'\n",
    "    tier[(value_indicator >= q60) & (value_indicator < q80)] = 'premium'\n",
    "    tier[(value_indicator >= q80) & (value_indicator < q95)] = 'luxury'\n",
    "    tier[value_indicator >= q95] = 'ultra'\n",
    "\n",
    "    # Size category\n",
    "    if 'living_sqft' in df.columns:\n",
    "        sqft = df['living_sqft'].fillna(df['living_sqft'].median())\n",
    "        sqft_median = sqft.median()\n",
    "        size = pd.Series(['small'] * len(df), index=df.index)\n",
    "        size[sqft > sqft_median] = 'large'\n",
    "    else:\n",
    "        size = pd.Series(['small'] * len(df), index=df.index)\n",
    "\n",
    "    # Combine segments\n",
    "    segments = tier + '_' + size\n",
    "\n",
    "    # Store diagnostics\n",
    "    df['_value_indicator'] = value_indicator\n",
    "    df['_value_source'] = source_used\n",
    "\n",
    "    return segments, training_quantiles\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    # Standard features\n",
    "    if 'living_sqft' in df.columns and 'bedrooms' in df.columns:\n",
    "        df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns:\n",
    "            df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns:\n",
    "            df['lot_acres'] = df['lot_sqft']/43560\n",
    "\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'] = 2024-df['year_built']\n",
    "        df['is_new'] = (df['property_age']<=5).astype('int8')\n",
    "        df['age_squared'] = df['property_age']**2\n",
    "\n",
    "    if 'garage_spaces' in df.columns:\n",
    "        df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "\n",
    "    if 'living_sqft' in df.columns:\n",
    "        df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "\n",
    "    # Value-based features (lagged, no leakage)\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        df['prior_appreciated'] = df['prior_sale_price'] * (1.04 ** df['years_since_last_sale'].fillna(5))\n",
    "\n",
    "    if 'assessed_total_value' in df.columns and 'living_sqft' in df.columns:\n",
    "        reasonable = (df['assessed_total_value'] > 1000) & (df['assessed_total_value'] < 100000000)\n",
    "        df['assessed_per_sqft'] = 0\n",
    "        df.loc[reasonable, 'assessed_per_sqft'] = df.loc[reasonable, 'assessed_total_value'] / (df.loc[reasonable, 'living_sqft'] + 1)\n",
    "\n",
    "    if 'assessed_total_value' in df.columns and 'median_home_value' in df.columns:\n",
    "        reasonable_assessed = (df['assessed_total_value'] > 1000)\n",
    "        reasonable_census = (df['median_home_value'] > 10000)\n",
    "        reasonable = reasonable_assessed & reasonable_census\n",
    "        df['assessed_to_census_ratio'] = 1.0\n",
    "        df.loc[reasonable, 'assessed_to_census_ratio'] = df.loc[reasonable, 'assessed_total_value'] / (df.loc[reasonable, 'median_home_value'] + 1)\n",
    "\n",
    "    if 'assessed_land_value' in df.columns and 'assessed_total_value' in df.columns:\n",
    "        reasonable = (df['assessed_total_value'] > 1000) & (df['assessed_land_value'] > 0)\n",
    "        df['land_to_total_ratio'] = 0\n",
    "        df.loc[reasonable, 'land_to_total_ratio'] = df.loc[reasonable, 'assessed_land_value'] / (df.loc[reasonable, 'assessed_total_value'] + 1)\n",
    "\n",
    "    if INCLUDE_CENSUS and all(c in df.columns for c in ['median_household_income','pct_bachelors_degree']):\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'living_sqft' in df.columns:\n",
    "        df['prior_price_per_sqft'] = df['prior_sale_price']/(df['living_sqft']+1)\n",
    "        df['sqft_per_prior_dollar'] = df['living_sqft']/(df['prior_sale_price']+1)\n",
    "\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        if 'prior_sale_price' in df.columns:\n",
    "            df['expected_appreciation'] = df['prior_sale_price']*(1.04)**df['years_since_last_sale']\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df.loc[missing,'median_home_value'] if 'median_home_value' in df.columns and INCLUDE_CENSUS else df['prior_sale_price'].median()\n",
    "\n",
    "    if 'years_since_last_sale' in df.columns:\n",
    "        df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "\n",
    "    if with_price and y_col in df.columns and 'living_sqft' in df.columns:\n",
    "        df['price_per_sqft'] = df[y_col]/(df['living_sqft']+1)\n",
    "        df['sqft_per_dollar'] = df['living_sqft']/(df[y_col]+1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df, kmeans_model=None):\n",
    "    \"\"\"Apply geo clustering - can use existing model for prediction\"\"\"\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans_model\n",
    "\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum() < N_CLUSTERS:\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans_model\n",
    "\n",
    "    df['geo_cluster'] = 0\n",
    "\n",
    "    if kmeans_model is None:\n",
    "        # Training: fit new model\n",
    "        kmeans_model = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "        df.loc[valid,'geo_cluster'] = kmeans_model.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    else:\n",
    "        # Prediction: use existing model\n",
    "        df.loc[valid,'geo_cluster'] = kmeans_model.predict(df.loc[valid,['latitude','longitude']])\n",
    "\n",
    "    return df, kmeans_model\n",
    "\n",
    "def add_cluster_feats(train, test, y_col):\n",
    "    if 'geo_cluster' not in train.columns or y_col not in train.columns:\n",
    "        for d in [train,test]:\n",
    "            d['cluster_avg_price'] = d['cluster_med_price'] = train[y_col].median() if y_col in train.columns else 0\n",
    "        return train, test\n",
    "\n",
    "    stats = train.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "    stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "    train = train.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    test = test.merge(stats, on='geo_cluster', how='left').fillna({c:train[y_col].median() for c in ['cluster_avg_price','cluster_med_price']})\n",
    "    return train, test\n",
    "\n",
    "def filter_outliers(df, name, y_col):\n",
    "    orig = len(df)\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)]\n",
    "\n",
    "    if 'sqft_per_dollar' in df.columns:\n",
    "        df = df[df['sqft_per_dollar']<=df['sqft_per_dollar'].quantile(.95)]\n",
    "\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        df = df.drop(columns=['price_per_sqft','sqft_per_dollar'])\n",
    "\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "\n",
    "    if 'year_built' in df.columns:\n",
    "        df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "\n",
    "    if len(df)>=100:\n",
    "        try:\n",
    "            feats = [c for c in ['living_sqft','lot_sqft',y_col,'year_built'] if c in df.columns]\n",
    "            if len(feats)>=3:\n",
    "                X = df[feats].fillna(df[feats].median())\n",
    "                df = df[IsolationForest(contamination=.05,random_state=RAND_STATE,n_jobs=N_JOBS).fit_predict(X)==1]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    pct_filt = (orig-len(df))/orig*100 if orig>0 else 0\n",
    "    if pct_filt > 0:\n",
    "        print(f\"  {name}: {orig:,}→{len(df):,} ({pct_filt:.1f}% filtered)\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def train_model(X, y, q):\n",
    "    return XGBRegressor(\n",
    "        objective='reg:quantileerror',\n",
    "        quantile_alpha=q,\n",
    "        n_estimators=N_EST,\n",
    "        learning_rate=.1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        subsample=.8,\n",
    "        colsample_bytree=.8,\n",
    "        random_state=RAND_STATE,\n",
    "        n_jobs=N_JOBS,\n",
    "        tree_method='hist'\n",
    "    ).fit(X, y, verbose=False)\n",
    "\n",
    "def get_feature_importance_per_segment(model, feat_names, top_n=20):\n",
    "    \"\"\"Extract feature importance for a single segment model\"\"\"\n",
    "    scores = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    importance_list = []\n",
    "    for k, v in scores.items():\n",
    "        idx = int(k[1:])\n",
    "        if idx < len(feat_names):\n",
    "            importance_list.append((feat_names[idx], v))\n",
    "\n",
    "    importance_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    total_gain = sum(v for _, v in importance_list)\n",
    "\n",
    "    if total_gain > 0:\n",
    "        importance_df = pd.DataFrame([\n",
    "            {'feature': feat, 'gain': gain, 'importance': gain/total_gain}\n",
    "            for feat, gain in importance_list[:top_n]\n",
    "        ])\n",
    "    else:\n",
    "        importance_df = pd.DataFrame(columns=['feature', 'gain', 'importance'])\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "def feature_importance(models, feat_names, metrics):\n",
    "    \"\"\"Global feature importance weighted by segment size\"\"\"\n",
    "    rows = []\n",
    "    for seg, mdl in models.items():\n",
    "        scores = mdl['q50'].get_booster().get_score(importance_type=\"gain\")\n",
    "        w = metrics[seg][\"n_test\"]\n",
    "        for k,v in scores.items():\n",
    "            idx = int(k[1:])\n",
    "            if idx<len(feat_names):\n",
    "                rows.append((feat_names[idx],v,w))\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"feature\",\"total_gain\",\"importance\"])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"feature\",\"gain\",\"weight\"])\n",
    "\n",
    "    out = df.assign(wg=df[\"gain\"]*df[\"weight\"]).groupby(\"feature\",as_index=False).agg(\n",
    "        total_gain=(\"gain\",\"sum\"),\n",
    "        weighted_gain=(\"wg\",\"sum\")\n",
    "    ).sort_values(\"weighted_gain\",ascending=False)\n",
    "\n",
    "    out[\"importance\"] = out[\"weighted_gain\"]/out[\"weighted_gain\"].sum()\n",
    "\n",
    "    return out[[\"feature\",\"total_gain\",\"importance\"]].head(100)\n",
    "\n",
    "def prepare_data(df, y_col, id_col, state_col, for_training=True):\n",
    "    print(f\"\\nPreparing data...\")\n",
    "    print(f\"Segmentation: IMPROVED multi-source $ value (variable assessment ratios)\")\n",
    "\n",
    "    if for_training:\n",
    "        df = df[df[y_col]>=MIN_PRICE]\n",
    "        print(f\"{len(df):,} records after price filter\")\n",
    "\n",
    "    df, kmeans_model = geo_cluster(engineer(df, y_col))\n",
    "\n",
    "    feat_groups = []\n",
    "    if INCLUDE_MLS:\n",
    "        feat_groups.extend([BASE_FEATS,ENG_FEATS,PRIOR_FEATS,CLUSTER_FEATS])\n",
    "    if INCLUDE_CENSUS:\n",
    "        feat_groups.append(CENSUS_FEATS)\n",
    "    if INCLUDE_NEIGHBORHOOD:\n",
    "        feat_groups.append(ELECT_FEATS)\n",
    "    if INCLUDE_IMAGE:\n",
    "        feat_groups.append(IMG_FEATS)\n",
    "\n",
    "    all_feats = [f for g in feat_groups for f in g]\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "\n",
    "    # Add new value-based features if they exist\n",
    "    value_feats = ['prior_appreciated', 'assessed_per_sqft', 'assessed_to_census_ratio', 'land_to_total_ratio']\n",
    "    for vf in value_feats:\n",
    "        if vf in df.columns and vf not in feats:\n",
    "            feats.append(vf)\n",
    "\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "\n",
    "    if for_training:\n",
    "        return df.dropna(subset=[y_col]), feats, kmeans_model\n",
    "    else:\n",
    "        return df, feats, kmeans_model\n",
    "\n",
    "def train_segments(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining segmented models on {len(df):,} properties\")\n",
    "    print(\"Segmentation: IMPROVED multi-source $ value (6 tiers)\")\n",
    "\n",
    "    # Segment using improved approach\n",
    "    df['seg'], training_quantiles = assign_segments_improved(df, training_quantiles=None)\n",
    "\n",
    "    # Diagnostic: Show what data sources were used\n",
    "    if '_value_source' in df.columns:\n",
    "        source_counts = df['_value_source'].value_counts()\n",
    "        print(f\"\\nValue source distribution:\")\n",
    "        for source, count in source_counts.items():\n",
    "            print(f\"  {source}: {count:,} ({100*count/len(df):.1f}%)\")\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    print(f\"\\n{len(seg_cnts)} segments created\")\n",
    "\n",
    "    for seg, cnt in seg_cnts.head(MAX_SEGMENTS).items():\n",
    "        if '_value_indicator' in df.columns and seg != 'other':\n",
    "            seg_df = df[df['seg']==seg]\n",
    "            if len(seg_df) > 0:\n",
    "                min_val = seg_df['_value_indicator'].min()\n",
    "                max_val = seg_df['_value_indicator'].max()\n",
    "                median_val = seg_df['_value_indicator'].median()\n",
    "                print(f\"  {seg}: {cnt:,} (value: ${min_val:,.0f}-${max_val:,.0f}, median ${median_val:,.0f})\")\n",
    "        else:\n",
    "            print(f\"  {seg}: {cnt:,}\")\n",
    "\n",
    "    # Merge small segments\n",
    "    small = seg_cnts[seg_cnts<50].index\n",
    "    if len(small)>0:\n",
    "        df.loc[df['seg'].isin(small),'seg'] = 'other'\n",
    "        print(f\"\\nMerged {len(small)} small segments into 'other'\")\n",
    "\n",
    "    # If still too many segments, merge smallest ones\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    if len(seg_cnts) > MAX_SEGMENTS:\n",
    "        keep_segs = seg_cnts.head(MAX_SEGMENTS-1).index\n",
    "        df.loc[~df['seg'].isin(keep_segs),'seg'] = 'other'\n",
    "        print(f\"Consolidated to {MAX_SEGMENTS} segments maximum\")\n",
    "\n",
    "    # Clean up temp columns\n",
    "    if '_value_indicator' in df.columns:\n",
    "        df = df.drop(columns=['_value_indicator'])\n",
    "    if '_value_source' in df.columns:\n",
    "        df = df.drop(columns=['_value_source'])\n",
    "\n",
    "    models, metrics, preds_list, segment_importances = {}, {}, [], {}\n",
    "\n",
    "    for seg in df['seg'].unique():\n",
    "        seg_df = df[df['seg']==seg].copy()\n",
    "        if len(seg_df)<50: continue\n",
    "\n",
    "        seg_df = filter_outliers(seg_df, seg, y_col)\n",
    "        if len(seg_df)<50: continue\n",
    "\n",
    "        train_idx = seg_df.sample(frac=1-TEST_SIZE,random_state=RAND_STATE).index\n",
    "        train_df, test_df = seg_df.loc[train_idx].copy(), seg_df.loc[seg_df.index.difference(train_idx)].copy()\n",
    "        train_df, test_df = add_cluster_feats(train_df, test_df, y_col)\n",
    "\n",
    "        X_tr,y_tr = train_df[feats].values, train_df[y_col].values\n",
    "        X_te,y_te = test_df[feats].values, test_df[y_col].values\n",
    "        ids_te = test_df[id_col].values\n",
    "        states_te = test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "\n",
    "        seg_models, seg_preds = {}, []\n",
    "        for q in QUANTILES:\n",
    "            m = train_model(X_tr,y_tr,q)\n",
    "            seg_models[f\"q{int(q*100)}\"] = m\n",
    "            seg_preds.append(m.predict(X_te))\n",
    "\n",
    "        models[seg] = seg_models\n",
    "        segment_importances[seg] = get_feature_importance_per_segment(seg_models['q50'], feats, top_n=20)\n",
    "\n",
    "        y_pred = seg_preds[1]\n",
    "        mae,mape = mean_absolute_error(y_te,y_pred), np.mean(np.abs((y_te-y_pred)/y_te))*100\n",
    "        r2,cov = r2_score(y_te,y_pred), np.mean((y_te>=seg_preds[0])&(y_te<=seg_preds[2]))*100\n",
    "        metrics[seg] = {'n_train':len(X_tr),'n_test':len(X_te),'mae':mae,'mape':mape,'r2':r2,'cov':cov,'p_min':seg_df[y_col].min(),'p_max':seg_df[y_col].max(),'p_med':seg_df[y_col].median()}\n",
    "        print(f\"  {seg}: {len(test_df):,} test | MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.3f}\")\n",
    "\n",
    "        preds_list.append(pd.DataFrame({\n",
    "            'property_id':ids_te,\n",
    "            'state':states_te,\n",
    "            'actual':y_te,\n",
    "            'predicted':y_pred,\n",
    "            'pred_lower':seg_preds[0],\n",
    "            'pred_upper':seg_preds[2],\n",
    "            'segment':seg\n",
    "        }))\n",
    "\n",
    "    return {\n",
    "        'models': models,\n",
    "        'metrics': metrics,\n",
    "        'predictions': pd.concat(preds_list),\n",
    "        'feature_importance': feature_importance(models, feats, metrics),\n",
    "        'segment_importances': segment_importances,\n",
    "        'feature_names': feats,\n",
    "        'training_quantiles': training_quantiles  # Pass quantiles for prediction\n",
    "    }\n",
    "\n",
    "def predict_new_properties(pred_df, models, feats, y_col, id_col, state_col, kmeans_model, train_cluster_stats, training_quantiles):\n",
    "    \"\"\"Generate predictions for new properties using trained models\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"GENERATING PREDICTIONS FOR NEW PROPERTIES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Input properties: {len(pred_df):,}\")\n",
    "\n",
    "    # Engineer features (no price filtering for prediction)\n",
    "    pred_df = engineer(pred_df, y_col)\n",
    "    pred_df, _ = geo_cluster(pred_df, kmeans_model)\n",
    "\n",
    "    # Add cluster features from training data\n",
    "    if 'geo_cluster' in pred_df.columns and train_cluster_stats is not None:\n",
    "        pred_df = pred_df.merge(train_cluster_stats, on='geo_cluster', how='left')\n",
    "        median_price = train_cluster_stats['cluster_avg_price'].median()\n",
    "        pred_df['cluster_avg_price'] = pred_df['cluster_avg_price'].fillna(median_price)\n",
    "        pred_df['cluster_med_price'] = pred_df['cluster_med_price'].fillna(median_price)\n",
    "\n",
    "    # Fill missing features\n",
    "    for feat in feats:\n",
    "        if feat not in pred_df.columns:\n",
    "            pred_df[feat] = 0\n",
    "        else:\n",
    "            pred_df[feat] = pred_df[feat].fillna(pred_df[feat].median() if pred_df[feat].notna().sum() > 0 else 0)\n",
    "\n",
    "    # Assign segments using TRAINING quantiles\n",
    "    pred_df['seg'], _ = assign_segments_improved(pred_df, training_quantiles=training_quantiles)\n",
    "\n",
    "    # Show diagnostics\n",
    "    if '_value_source' in pred_df.columns:\n",
    "        source_counts = pred_df['_value_source'].value_counts()\n",
    "        print(f\"\\nValue source distribution (prediction data):\")\n",
    "        for source, count in source_counts.items():\n",
    "            print(f\"  {source}: {count:,} ({100*count/len(pred_df):.1f}%)\")\n",
    "\n",
    "    # Generate predictions\n",
    "    preds_list = []\n",
    "    for seg in pred_df['seg'].unique():\n",
    "        seg_df = pred_df[pred_df['seg']==seg].copy()\n",
    "\n",
    "        if seg not in models:\n",
    "            print(f\"  Warning: Segment '{seg}' not in trained models, using fallback\")\n",
    "            # Find closest segment\n",
    "            available_segs = list(models.keys())\n",
    "\n",
    "            # Try to find a similar segment\n",
    "            if 'ultra' in seg and any('luxury' in s for s in available_segs):\n",
    "                seg = [s for s in available_segs if 'luxury' in s][0]\n",
    "            elif 'luxury' in seg and any('premium' in s for s in available_segs):\n",
    "                seg = [s for s in available_segs if 'premium' in s][0]\n",
    "            elif 'premium' in seg and any('mid' in s for s in available_segs):\n",
    "                seg = [s for s in available_segs if 'mid' in s][0]\n",
    "            elif 'mid' in seg and any('mid' in s for s in available_segs):\n",
    "                seg = [s for s in available_segs if 'mid' in s][0]\n",
    "            elif 'economy' in seg and any('budget' in s for s in available_segs):\n",
    "                seg = [s for s in available_segs if 'budget' in s][0]\n",
    "            else:\n",
    "                seg = available_segs[0]\n",
    "\n",
    "        X = seg_df[feats].values\n",
    "        ids = seg_df[id_col].values\n",
    "        states = seg_df[state_col].values if state_col and state_col in seg_df.columns else ['Unknown']*len(seg_df)\n",
    "\n",
    "        # Get predictions\n",
    "        pred_lower = models[seg]['q10'].predict(X)\n",
    "        pred_mid = models[seg]['q50'].predict(X)\n",
    "        pred_upper = models[seg]['q90'].predict(X)\n",
    "\n",
    "        actual = seg_df[y_col].values if y_col in seg_df.columns else [np.nan]*len(seg_df)\n",
    "\n",
    "        # Get diagnostics\n",
    "        value_ind = seg_df['_value_indicator'].values if '_value_indicator' in seg_df.columns else [np.nan]*len(seg_df)\n",
    "        value_src = seg_df['_value_source'].values if '_value_source' in seg_df.columns else ['unknown']*len(seg_df)\n",
    "\n",
    "        preds_list.append(pd.DataFrame({\n",
    "            'property_id': ids,\n",
    "            'state': states,\n",
    "            'value_indicator': value_ind,\n",
    "            'value_source': value_src,\n",
    "            'actual': actual,\n",
    "            'predicted': pred_mid,\n",
    "            'pred_lower': pred_lower,\n",
    "            'pred_upper': pred_upper,\n",
    "            'segment': seg,\n",
    "            'error': [actual[i] - pred_mid[i] if not np.isnan(actual[i]) else np.nan for i in range(len(actual))],\n",
    "            'pct_error': [100 * (actual[i] - pred_mid[i]) / actual[i] if not np.isnan(actual[i]) and actual[i] != 0 else np.nan for i in range(len(actual))]\n",
    "        }))\n",
    "\n",
    "        print(f\"  {seg}: {len(seg_df):,} properties predicted\")\n",
    "\n",
    "    result_df = pd.concat(preds_list, ignore_index=True)\n",
    "    print(f\"\\n✓ Generated {len(result_df):,} predictions\")\n",
    "\n",
    "    # Clean up temp columns\n",
    "    if '_value_indicator' in pred_df.columns:\n",
    "        pred_df = pred_df.drop(columns=['_value_indicator'])\n",
    "    if '_value_source' in pred_df.columns:\n",
    "        pred_df = pred_df.drop(columns=['_value_source'])\n",
    "\n",
    "    # Calculate metrics if actuals available\n",
    "    valid_actuals = result_df['actual'].notna().sum()\n",
    "    if valid_actuals > 0:\n",
    "        valid_preds = result_df[result_df['actual'].notna()].copy()\n",
    "        mae = mean_absolute_error(valid_preds['actual'], valid_preds['predicted'])\n",
    "        mape = np.mean(np.abs((valid_preds['actual'] - valid_preds['predicted']) / valid_preds['actual'])) * 100\n",
    "        r2 = r2_score(valid_preds['actual'], valid_preds['predicted'])\n",
    "        print(f\"  Validation metrics ({valid_actuals} properties):\")\n",
    "        print(f\"  MAE: ${mae:,.0f} | MAPE: {mape:.2f}% | R²: {r2:.4f}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def save_results(results, out_dir, new_predictions=None):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi = results['predictions'], results['metrics'], results['feature_importance']\n",
    "    seg_importances = results['segment_importances']\n",
    "\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    # Summary\n",
    "    ws = wb.create_sheet(\"Summary\",0)\n",
    "    ws['A1'], ws['A1'].font = 'IMPROVED SEGMENTED AVM', Font(bold=True,size=14)\n",
    "    ws['A2'] = 'Segmentation: Variable assessment ratios + 6 tiers'\n",
    "    ws['A2'].font = Font(italic=True, size=10)\n",
    "\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "\n",
    "    data = [\n",
    "        ['Metric','Value'],\n",
    "        ['Properties',len(preds)],\n",
    "        ['Segments',len(metrics)],\n",
    "        ['R²',f'{r2:.4f}'],\n",
    "        ['MAE',f'${mae:,.0f}'],\n",
    "        ['MAPE%',f'{mape:.2f}%']\n",
    "    ]\n",
    "\n",
    "    if new_predictions is not None:\n",
    "        data.append(['New Predictions', len(new_predictions)])\n",
    "\n",
    "    for i,(k,v) in enumerate(data,5):\n",
    "        ws[f'A{i}'],ws[f'A{i}'].font,ws[f'B{i}'] = k,Font(bold=True),v\n",
    "\n",
    "    # Segments\n",
    "    ws = wb.create_sheet(\"Segments\")\n",
    "    seg_df = pd.DataFrame([{**{'Segment':s},**m} for s,m in metrics.items()])\n",
    "    for i,h in enumerate(seg_df.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(seg_df.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1):\n",
    "            ws.cell(i,j,v)\n",
    "\n",
    "    # Global Feature Importance\n",
    "    ws = wb.create_sheet(\"Global_Feature_Importance\")\n",
    "    ws['A1'] = 'Global Feature Importance (Weighted)'\n",
    "    ws['A1'].font = Font(bold=True, size=12)\n",
    "    for r_idx, row in enumerate(dataframe_to_rows(fi, index=False, header=True), 2):\n",
    "        for c_idx, value in enumerate(row, 1):\n",
    "            cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "            if r_idx == 2:\n",
    "                cell.font = Font(bold=True, color='FFFFFF')\n",
    "                cell.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "\n",
    "    # Per-Segment Feature Importance\n",
    "    for seg_name, seg_fi in seg_importances.items():\n",
    "        sheet_name = f\"FI_{seg_name}\"[:31]\n",
    "        ws = wb.create_sheet(sheet_name)\n",
    "        ws['A1'] = f'Feature Importance: {seg_name}'\n",
    "        ws['A1'].font = Font(bold=True, size=12)\n",
    "\n",
    "        for r_idx, row in enumerate(dataframe_to_rows(seg_fi, index=False, header=True), 2):\n",
    "            for c_idx, value in enumerate(row, 1):\n",
    "                cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "                if r_idx == 2:\n",
    "                    cell.font = Font(bold=True, color='FFFFFF')\n",
    "                    cell.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "\n",
    "    # Test Set Predictions\n",
    "    ws = wb.create_sheet(\"Test_Predictions\")\n",
    "    for i,h in enumerate(preds.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(preds.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1):\n",
    "            ws.cell(i,j,v)\n",
    "\n",
    "    # New Predictions\n",
    "    if new_predictions is not None:\n",
    "        ws = wb.create_sheet(\"New_Predictions\")\n",
    "        for i,h in enumerate(new_predictions.columns,1):\n",
    "            c = ws.cell(1,i,h)\n",
    "            c.font,c.fill = Font(bold=True,color='FFFFFF'),PatternFill(start_color='4472C4',end_color='4472C4',fill_type='solid')\n",
    "        for i,row in enumerate(new_predictions.itertuples(index=False),2):\n",
    "            for j,v in enumerate(row,1):\n",
    "                ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/improved_segmented_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "    preds.to_csv(f\"{out_dir}/improved_test_predictions_{ts}.csv\",index=False)\n",
    "    seg_df.to_csv(f\"{out_dir}/improved_segments_{ts}.csv\",index=False)\n",
    "    fi.to_csv(f\"{out_dir}/improved_importance_{ts}.csv\",index=False)\n",
    "\n",
    "    for seg_name, seg_fi in seg_importances.items():\n",
    "        seg_fi.to_csv(f\"{out_dir}/improved_importance_{seg_name}_{ts}.csv\", index=False)\n",
    "\n",
    "    if new_predictions is not None:\n",
    "        new_predictions.to_csv(f\"{out_dir}/improved_new_predictions_{ts}.csv\", index=False)\n",
    "        print(f\"✓ New predictions CSV: {out_dir}/improved_new_predictions_{ts}.csv\")\n",
    "\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved with timestamp: {ts}\")\n",
    "    print(f\"✓ Per-segment feature importance saved for {len(seg_importances)} segments\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60)\n",
    "    print(\"IMPROVED SEGMENTED AVM\")\n",
    "    print(\"Variable Assessment Ratios + 6 Tiers\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Load and prepare training data\n",
    "    df, y_col, id_col, state_col = load_data(TRAINING_INPUT_PATH)\n",
    "    df, feats, kmeans_model = prepare_data(df, y_col, id_col, state_col, for_training=True)\n",
    "\n",
    "    # Train models\n",
    "    results = train_segments(df, feats, y_col, id_col, state_col)\n",
    "\n",
    "    # Get cluster stats and training quantiles\n",
    "    train_cluster_stats = None\n",
    "    if 'geo_cluster' in df.columns:\n",
    "        train_cluster_stats = df.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "        train_cluster_stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "\n",
    "    training_quantiles = results['training_quantiles']\n",
    "\n",
    "    # Generate predictions for new properties\n",
    "    new_predictions = None\n",
    "    if PREDICTION_INPUT_PATH:\n",
    "        pred_df, _, _, _ = load_data(PREDICTION_INPUT_PATH)\n",
    "        new_predictions = predict_new_properties(\n",
    "            pred_df, results['models'], feats, y_col, id_col, state_col,\n",
    "            kmeans_model, train_cluster_stats, training_quantiles\n",
    "        )\n",
    "\n",
    "    # Save results\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR, new_predictions)\n",
    "\n",
    "    # Print summary\n",
    "    preds = results['predictions']\n",
    "    r2 = r2_score(preds['actual'],preds['predicted'])\n",
    "    mae = mean_absolute_error(preds['actual'],preds['predicted'])\n",
    "    mape = np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ TRAINING COMPLETE in {time.time()-t0:.1f}s\")\n",
    "    print(f\"  Test set: {len(preds):,} properties | {preds['segment'].nunique()} segments\")\n",
    "    print(f\"  R²: {r2:.4f} | MAE: ${mae:,.0f} | MAPE: {mape:.2f}%\")\n",
    "    if new_predictions is not None:\n",
    "        print(f\"  New predictions: {len(new_predictions):,} properties\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "if __name__ == \"__main__\": main()"
   ],
   "id": "c43562e75f067f8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IMPROVED SEGMENTED AVM\n",
      "Variable Assessment Ratios + 6 Tiers\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 129.6MB | Price:sale_price ID:property_id\n",
      "\n",
      "Preparing data...\n",
      "Segmentation: IMPROVED multi-source $ value (variable assessment ratios)\n",
      "127,258 records after price filter\n",
      "51/57 features available\n",
      "\n",
      "Training segmented models on 127,258 properties\n",
      "Segmentation: IMPROVED multi-source $ value (6 tiers)\n",
      "\n",
      "Value indicator quantiles:\n",
      "  Q20: $225,450 | Q40: $434,750 | Q60: $434,750 | Q80: $741,940 | Q95: $1,445,367\n",
      "\n",
      "Value source distribution:\n",
      "  census: 127,258 (100.0%)\n",
      "\n",
      "10 segments created\n",
      "  premium_large: 27,925 (value: $434,750-$732,125, median $434,750)\n",
      "  premium_small: 26,120 (value: $434,750-$732,125, median $434,750)\n",
      "  budget_small: 19,274 (value: $12,000-$225,433, median $134,133)\n",
      "  economy_large: 13,496 (value: $225,450-$433,875, median $335,713)\n",
      "  luxury_large: 11,219 (value: $741,940-$1,426,334, median $926,975)\n",
      "  economy_small: 8,817 (value: $225,450-$433,875, median $320,400)\n",
      "  luxury_small: 7,745 (value: $741,940-$1,426,334, median $923,550)\n",
      "Consolidated to 7 segments maximum\n",
      "  budget_small: 19,274→14,516 (24.7% filtered)\n",
      "  budget_small: 4,355 test | MAE:$1,076,522 | MAPE:36.05% | R²:0.301\n",
      "  premium_large: 27,925→21,724 (22.2% filtered)\n",
      "  premium_large: 6,517 test | MAE:$316,380 | MAPE:20.19% | R²:0.491\n",
      "  premium_small: 26,120→20,225 (22.6% filtered)\n",
      "  premium_small: 6,067 test | MAE:$375,827 | MAPE:20.85% | R²:0.324\n",
      "  other: 20,407→15,856 (22.3% filtered)\n",
      "  other: 4,757 test | MAE:$293,861 | MAPE:19.39% | R²:0.596\n",
      "  economy_small: 8,817→6,634 (24.8% filtered)\n",
      "  economy_small: 1,990 test | MAE:$282,584 | MAPE:18.07% | R²:0.129\n",
      "  economy_large: 13,496→10,241 (24.1% filtered)\n",
      "  economy_large: 3,072 test | MAE:$165,178 | MAPE:13.28% | R²:0.361\n",
      "  luxury_large: 11,219→8,657 (22.8% filtered)\n",
      "  luxury_large: 2,597 test | MAE:$314,474 | MAPE:19.73% | R²:0.520\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\n",
      "1 records | 0.0MB | Price:sale_price ID:property_id\n",
      "\n",
      "============================================================\n",
      "GENERATING PREDICTIONS FOR NEW PROPERTIES\n",
      "============================================================\n",
      "Input properties: 1\n",
      "\n",
      "Value source distribution (prediction data):\n",
      "  assessed: 1 (100.0%)\n",
      "  budget_small: 1 properties predicted\n",
      "\n",
      "✓ Generated 1 predictions\n",
      "  Validation metrics (1 properties):\n",
      "  MAE: $2,057,850 | MAPE: 391.23% | R²: nan\n",
      "\n",
      "Saving results...\n",
      "✓ New predictions CSV: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/improved_new_predictions_20251224_002036.csv\n",
      "✓ Excel: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/improved_segmented_20251224_002036.xlsx\n",
      "✓ CSVs saved with timestamp: 20251224_002036\n",
      "✓ Per-segment feature importance saved for 7 segments\n",
      "\n",
      "============================================================\n",
      "✓ TRAINING COMPLETE in 10.5s\n",
      "  Test set: 29,355 properties | 7 segments\n",
      "  R²: 0.4116 | MAE: $419,506 | MAPE: 21.64%\n",
      "  New predictions: 1 properties\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T05:38:59.796993Z",
     "start_time": "2025-12-24T05:38:48.804261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings, time, os\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Config\n",
    "INCLUDE_MLS, INCLUDE_CENSUS, INCLUDE_NEIGHBORHOOD, INCLUDE_IMAGE = True, True, True, False\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS, N_EST, MAX_SEGMENTS = 20000, 0.3, 42, -1, 8, 100, 7\n",
    "QUANTILES = [0.1, 0.5, 0.9]\n",
    "\n",
    "TRAINING_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "PREDICTION_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "BASE_FEATS = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"effective_year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\"fireplace_code\",\"latitude\",\"longitude\",\"geo_cluster\"]\n",
    "ENG_FEATS = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\"]\n",
    "PRIOR_FEATS = [\"prior_sale_price\",\"prior_price_per_sqft\",\"sqft_per_prior_dollar\",\"years_since_last_sale\",\"expected_appreciation\",\"has_prior_sale\",\"recently_sold\"]\n",
    "CLUSTER_FEATS = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "CENSUS_FEATS = [\"total_population_25plus\",\"male_bachelors_degree\",\"female_bachelors_degree\",\"pct_bachelors_degree\",\"median_earnings_total\",\"median_earnings_male\",\"median_earnings_female\",\"median_household_income\",\"median_home_value\",\"median_gross_rent\",\"owner_occupied_units\",\"renter_occupied_units\",\"pct_owner_occupied\",\"occupied_units\",\"vacant_units\",\"median_age\",\"civilian_employed\",\"civilian_unemployed\",\"unemployment_rate\",\"income_education_score\"]\n",
    "ELECT_FEATS = [\"votes_gop\",\"votes_dem\",\"total_votes\",\"per_gop\",\"per_dem\",\"per_point_diff\",\"dem_margin\",\"rep_margin\"]\n",
    "IMG_FEATS = [\"topic_1\",\"topic_2\",\"topic_3\",\"topic_4\",\"topic_5\",\"topic_6\",\"topic_7\",\"topic_8\",\"topic_9\",\"topic_10\",\"gran_c_in\",\"gran_c_ex\",\"gran_c\",\"high_c_in\",\"high_c_ex\",\"high_c\"]\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8' if set(df[c].dropna().unique()).issubset({0,1}) else 'int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = optimize_dtypes(pd.read_csv(path, low_memory=False))\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return df, y_col, id_col, state_col\n",
    "\n",
    "def filter_bad_assessed(df):\n",
    "    if 'assessed_total_value' not in df.columns:\n",
    "        return pd.Series([False]*len(df), index=df.index)\n",
    "\n",
    "    valid = (df['assessed_total_value']>10000) & (df['assessed_total_value']<100000000)\n",
    "\n",
    "    if 'living_sqft' in df.columns:\n",
    "        has_sqft = (df['living_sqft'].notna()) & (df['living_sqft']>100)\n",
    "        ppsf = df['assessed_total_value']/df['living_sqft']\n",
    "        valid &= ~(has_sqft & ((ppsf<20)|(ppsf>2000)))\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        has_prior = (df['prior_sale_price']>10000) & (df['years_since_last_sale']<=15)\n",
    "        valid &= ~(has_prior & ((df['assessed_total_value']/df['prior_sale_price'])<0.10))\n",
    "\n",
    "    if 'median_home_value' in df.columns:\n",
    "        has_census = (df['median_home_value']>10000)\n",
    "        valid &= ~(has_census & ((df['assessed_total_value']/df['median_home_value'])<0.05))\n",
    "\n",
    "    invalid_cnt = ((df['assessed_total_value'].notna()) & ~valid).sum()\n",
    "    if invalid_cnt>0:\n",
    "        print(f\"  ⚠️  Filtered {invalid_cnt:,} bad assessed values\")\n",
    "\n",
    "    return valid\n",
    "\n",
    "def assign_segments(df, train_q=None):\n",
    "    value_ind = pd.Series([np.nan]*len(df), index=df.index)\n",
    "    source = pd.Series(['none']*len(df), index=df.index)\n",
    "\n",
    "    # Priority 1: Prior sale\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        recent = (df['years_since_last_sale']<=10) & (df['prior_sale_price']>10000)\n",
    "        if recent.sum()>0:\n",
    "            yrs = df.loc[recent,'years_since_last_sale'].fillna(5)\n",
    "            value_ind[recent] = df.loc[recent,'prior_sale_price']*(1.04**yrs)\n",
    "            source[recent] = 'prior_sale'\n",
    "\n",
    "    # Priority 2: Assessed\n",
    "    if 'assessed_total_value' in df.columns:\n",
    "        valid = filter_bad_assessed(df) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            vals = df.loc[valid,'assessed_total_value']\n",
    "            mult = pd.Series([1.0]*len(vals), index=vals.index)\n",
    "            mult[vals<50000], mult[(vals>=50000)&(vals<200000)] = 1.0, 1.1\n",
    "            mult[(vals>=200000)&(vals<500000)], mult[vals>=500000] = 1.15, 1.2\n",
    "            value_ind[valid], source[valid] = vals*mult, 'assessed'\n",
    "\n",
    "    # Priority 3: Census (PROPERTY-ADJUSTED)\n",
    "    if 'median_home_value' in df.columns:\n",
    "        valid = (df['median_home_value']>0) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            base_val = df.loc[valid,'median_home_value']\n",
    "            # Adjust census median by property characteristics\n",
    "            if 'living_sqft' in df.columns:\n",
    "                typical_sqft = 2000\n",
    "                sqft_ratio = (df.loc[valid,'living_sqft']/typical_sqft).clip(0.4, 3.0)\n",
    "                base_val *= sqft_ratio\n",
    "            if 'full_baths' in df.columns:\n",
    "                typical_baths = 2\n",
    "                bath_adj = 1 + (df.loc[valid,'full_baths']-typical_baths)*0.08\n",
    "                base_val *= bath_adj.clip(0.85, 1.3)\n",
    "            if 'garage_spaces' in df.columns:\n",
    "                garage_adj = 1 + df.loc[valid,'garage_spaces']*0.05\n",
    "                base_val *= garage_adj.clip(1.0, 1.2)\n",
    "            value_ind[valid], source[valid] = base_val, 'census_adjusted'\n",
    "\n",
    "    # Priority 4: Sqft estimate\n",
    "    if 'living_sqft' in df.columns:\n",
    "        valid = (df['living_sqft']>0) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            ppsf = pd.Series([150.0]*valid.sum(), index=df[valid].index)\n",
    "            if 'median_home_value' in df.columns:\n",
    "                ppsf = (df.loc[valid,'median_home_value'].fillna(300000)/2000).clip(50,500)\n",
    "            value_ind[valid], source[valid] = df.loc[valid,'living_sqft']*ppsf, 'sqft_estimate'\n",
    "\n",
    "    value_ind = value_ind.fillna(value_ind.median())\n",
    "    source[value_ind.isna()] = 'fallback'\n",
    "\n",
    "    # Segment\n",
    "    if train_q is None:\n",
    "        q20,q40,q60,q80,q95 = value_ind.quantile([0.20,0.40,0.60,0.80,0.95])\n",
    "        train_q = {'value_q20':q20,'value_q40':q40,'value_q60':q60,'value_q80':q80,'value_q95':q95}\n",
    "        print(f\"\\nValue quantiles: Q20:${q20:,.0f} Q40:${q40:,.0f} Q60:${q60:,.0f} Q80:${q80:,.0f} Q95:${q95:,.0f}\")\n",
    "    else:\n",
    "        q20,q40,q60,q80,q95 = train_q['value_q20'],train_q['value_q40'],train_q['value_q60'],train_q['value_q80'],train_q['value_q95']\n",
    "\n",
    "    tier = pd.Series(['mid']*len(df), index=df.index)\n",
    "    tier[value_ind<q20], tier[(value_ind>=q20)&(value_ind<q40)] = 'budget', 'economy'\n",
    "    tier[(value_ind>=q60)&(value_ind<q80)], tier[(value_ind>=q80)&(value_ind<q95)], tier[value_ind>=q95] = 'premium', 'luxury', 'ultra'\n",
    "\n",
    "    size = pd.Series(['small']*len(df), index=df.index)\n",
    "    if 'living_sqft' in df.columns:\n",
    "        sqft = df['living_sqft'].fillna(df['living_sqft'].median())\n",
    "        size[sqft>sqft.median()] = 'large'\n",
    "\n",
    "    df['_value_indicator'], df['_value_source'] = value_ind, source\n",
    "    return tier+'_'+size, train_q\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    if 'living_sqft' in df.columns:\n",
    "        if 'bedrooms' in df.columns: df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "        df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns: df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns: df['lot_acres'] = df['lot_sqft']/43560\n",
    "\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'], df['is_new'], df['age_squared'] = 2024-df['year_built'], ((2024-df['year_built'])<=5).astype('int8'), (2024-df['year_built'])**2\n",
    "\n",
    "    if 'garage_spaces' in df.columns:\n",
    "        df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        df['prior_appreciated'] = df['prior_sale_price']*(1.04**df['years_since_last_sale'].fillna(5))\n",
    "\n",
    "    if 'assessed_total_value' in df.columns:\n",
    "        valid = filter_bad_assessed(df)\n",
    "        if 'living_sqft' in df.columns:\n",
    "            df['assessed_per_sqft'] = 0\n",
    "            df.loc[valid,'assessed_per_sqft'] = df.loc[valid,'assessed_total_value']/(df.loc[valid,'living_sqft']+1)\n",
    "        if 'median_home_value' in df.columns:\n",
    "            reasonable = valid & (df['median_home_value']>10000)\n",
    "            df['assessed_to_census_ratio'] = 1.0\n",
    "            df.loc[reasonable,'assessed_to_census_ratio'] = df.loc[reasonable,'assessed_total_value']/(df.loc[reasonable,'median_home_value']+1)\n",
    "        if 'assessed_land_value' in df.columns:\n",
    "            reasonable = valid & (df['assessed_land_value']>0)\n",
    "            df['land_to_total_ratio'] = 0\n",
    "            df.loc[reasonable,'land_to_total_ratio'] = df.loc[reasonable,'assessed_land_value']/(df.loc[reasonable,'assessed_total_value']+1)\n",
    "\n",
    "    if INCLUDE_CENSUS and all(c in df.columns for c in ['median_household_income','pct_bachelors_degree']):\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'living_sqft' in df.columns:\n",
    "        df['prior_price_per_sqft'], df['sqft_per_prior_dollar'] = df['prior_sale_price']/(df['living_sqft']+1), df['living_sqft']/(df['prior_sale_price']+1)\n",
    "\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        if 'prior_sale_price' in df.columns:\n",
    "            df['expected_appreciation'] = df['prior_sale_price']*(1.04)**df['years_since_last_sale']\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df.loc[missing,'median_home_value'] if 'median_home_value' in df.columns and INCLUDE_CENSUS else df['prior_sale_price'].median()\n",
    "\n",
    "    if 'years_since_last_sale' in df.columns:\n",
    "        df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "\n",
    "    if with_price and y_col in df.columns and 'living_sqft' in df.columns:\n",
    "        df['price_per_sqft'], df['sqft_per_dollar'] = df[y_col]/(df['living_sqft']+1), df['living_sqft']/(df[y_col]+1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df, kmeans=None):\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans\n",
    "\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum()<N_CLUSTERS:\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans\n",
    "\n",
    "    df['geo_cluster'] = 0\n",
    "    if kmeans is None:\n",
    "        kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "        df.loc[valid,'geo_cluster'] = kmeans.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    else:\n",
    "        df.loc[valid,'geo_cluster'] = kmeans.predict(df.loc[valid,['latitude','longitude']])\n",
    "\n",
    "    return df, kmeans\n",
    "\n",
    "def add_cluster_feats(train, test, y_col):\n",
    "    if 'geo_cluster' not in train.columns or y_col not in train.columns:\n",
    "        for d in [train,test]:\n",
    "            d['cluster_avg_price'] = d['cluster_med_price'] = train[y_col].median() if y_col in train.columns else 0\n",
    "        return train, test\n",
    "\n",
    "    stats = train.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "    stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "    med = train[y_col].median()\n",
    "    train = train.merge(stats, on='geo_cluster', how='left').fillna({'cluster_avg_price':med,'cluster_med_price':med})\n",
    "    test = test.merge(stats, on='geo_cluster', how='left').fillna({'cluster_avg_price':med,'cluster_med_price':med})\n",
    "    return train, test\n",
    "\n",
    "def filter_outliers(df, name, y_col):\n",
    "    orig = len(df)\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)].drop(columns=['price_per_sqft','sqft_per_dollar'])\n",
    "\n",
    "    if 'lot_sqft' in df.columns: df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "    if 'year_built' in df.columns: df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "\n",
    "    if len(df)>=100:\n",
    "        try:\n",
    "            feats = [c for c in ['living_sqft','lot_sqft',y_col,'year_built'] if c in df.columns]\n",
    "            if len(feats)>=3:\n",
    "                X = df[feats].fillna(df[feats].median())\n",
    "                df = df[IsolationForest(contamination=.05, random_state=RAND_STATE, n_jobs=N_JOBS).fit_predict(X)==1]\n",
    "        except: pass\n",
    "\n",
    "    if (pct:=(orig-len(df))/orig*100)>0: print(f\"  {name}: {orig:,}→{len(df):,} ({pct:.1f}% filtered)\")\n",
    "    return df\n",
    "\n",
    "def train_model(X, y, q):\n",
    "    return XGBRegressor(objective='reg:quantileerror', quantile_alpha=q, n_estimators=N_EST, learning_rate=.1,\n",
    "                       max_depth=5, min_child_weight=3, subsample=.8, colsample_bytree=.8,\n",
    "                       random_state=RAND_STATE, n_jobs=N_JOBS, tree_method='hist').fit(X, y, verbose=False)\n",
    "\n",
    "def get_feat_importance(model, feats, top_n=20):\n",
    "    scores = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    imp = [(feats[int(k[1:])],v) for k,v in scores.items() if int(k[1:])<len(feats)]\n",
    "    imp.sort(key=lambda x: x[1], reverse=True)\n",
    "    total = sum(v for _,v in imp)\n",
    "    return pd.DataFrame([{'feature':f,'gain':g,'importance':g/total} for f,g in imp[:top_n]]) if total>0 else pd.DataFrame(columns=['feature','gain','importance'])\n",
    "\n",
    "def feature_importance(models, feats, metrics):\n",
    "    rows = []\n",
    "    for seg,mdl in models.items():\n",
    "        scores = mdl['q50'].get_booster().get_score(importance_type=\"gain\")\n",
    "        w = metrics[seg][\"n_test\"]\n",
    "        rows.extend([(feats[int(k[1:])],v,w) for k,v in scores.items() if int(k[1:])<len(feats)])\n",
    "\n",
    "    if not rows: return pd.DataFrame(columns=[\"feature\",\"total_gain\",\"importance\"])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"feature\",\"gain\",\"weight\"])\n",
    "    out = df.assign(wg=df[\"gain\"]*df[\"weight\"]).groupby(\"feature\",as_index=False).agg(\n",
    "        total_gain=(\"gain\",\"sum\"), weighted_gain=(\"wg\",\"sum\")).sort_values(\"weighted_gain\",ascending=False)\n",
    "    out[\"importance\"] = out[\"weighted_gain\"]/out[\"weighted_gain\"].sum()\n",
    "    return out[[\"feature\",\"total_gain\",\"importance\"]].head(100)\n",
    "\n",
    "def prepare_data(df, y_col, id_col, state_col, for_training=True):\n",
    "    print(f\"\\nPreparing data (property-adjusted census segmentation)...\")\n",
    "    if for_training: df = df[df[y_col]>=MIN_PRICE]\n",
    "\n",
    "    df, kmeans = geo_cluster(engineer(df, y_col))\n",
    "\n",
    "    feat_groups = []\n",
    "    if INCLUDE_MLS: feat_groups.extend([BASE_FEATS,ENG_FEATS,PRIOR_FEATS,CLUSTER_FEATS])\n",
    "    if INCLUDE_CENSUS: feat_groups.append(CENSUS_FEATS)\n",
    "    if INCLUDE_NEIGHBORHOOD: feat_groups.append(ELECT_FEATS)\n",
    "    if INCLUDE_IMAGE: feat_groups.append(IMG_FEATS)\n",
    "\n",
    "    all_feats = [f for g in feat_groups for f in g]\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "    feats.extend([vf for vf in ['prior_appreciated','assessed_per_sqft','assessed_to_census_ratio','land_to_total_ratio'] if vf in df.columns and vf not in feats])\n",
    "\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "\n",
    "    return (df.dropna(subset=[y_col]),feats,kmeans) if for_training else (df,feats,kmeans)\n",
    "\n",
    "def train_segments(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining on {len(df):,} properties (with property-adjusted segmentation)\")\n",
    "\n",
    "    df['seg'], train_q = assign_segments(df)\n",
    "\n",
    "    if '_value_source' in df.columns:\n",
    "        print(f\"\\nValue sources: {dict(df['_value_source'].value_counts())}\")\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    print(f\"\\n{len(seg_cnts)} segments\")\n",
    "\n",
    "    for seg,cnt in seg_cnts.head(MAX_SEGMENTS).items():\n",
    "        if '_value_indicator' in df.columns and seg!='other':\n",
    "            seg_df = df[df['seg']==seg]\n",
    "            if len(seg_df)>0:\n",
    "                print(f\"  {seg}: {cnt:,} (${seg_df['_value_indicator'].min():,.0f}-${seg_df['_value_indicator'].max():,.0f}, med ${seg_df['_value_indicator'].median():,.0f})\")\n",
    "        else:\n",
    "            print(f\"  {seg}: {cnt:,}\")\n",
    "\n",
    "    small = seg_cnts[seg_cnts<50].index\n",
    "    if len(small)>0:\n",
    "        df.loc[df['seg'].isin(small),'seg'] = 'other'\n",
    "        print(f\"\\nMerged {len(small)} small segments\")\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    if len(seg_cnts)>MAX_SEGMENTS:\n",
    "        keep = seg_cnts.head(MAX_SEGMENTS-1).index\n",
    "        df.loc[~df['seg'].isin(keep),'seg'] = 'other'\n",
    "        print(f\"Consolidated to {MAX_SEGMENTS} segments\")\n",
    "\n",
    "    df = df.drop(columns=[c for c in ['_value_indicator','_value_source'] if c in df.columns])\n",
    "\n",
    "    models, metrics, preds_list, seg_imps = {}, {}, [], {}\n",
    "\n",
    "    for seg in df['seg'].unique():\n",
    "        seg_df = filter_outliers(df[df['seg']==seg].copy(), seg, y_col)\n",
    "        if len(seg_df)<50: continue\n",
    "\n",
    "        train_idx = seg_df.sample(frac=1-TEST_SIZE, random_state=RAND_STATE).index\n",
    "        train_df, test_df = seg_df.loc[train_idx].copy(), seg_df.loc[seg_df.index.difference(train_idx)].copy()\n",
    "        train_df, test_df = add_cluster_feats(train_df, test_df, y_col)\n",
    "\n",
    "        X_tr, y_tr = train_df[feats].values, train_df[y_col].values\n",
    "        X_te, y_te = test_df[feats].values, test_df[y_col].values\n",
    "        ids, states = test_df[id_col].values, test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "\n",
    "        seg_models = {f\"q{int(q*100)}\":train_model(X_tr,y_tr,q) for q in QUANTILES}\n",
    "        seg_preds = [seg_models[f\"q{int(q*100)}\"].predict(X_te) for q in QUANTILES]\n",
    "\n",
    "        models[seg] = seg_models\n",
    "        seg_imps[seg] = get_feat_importance(seg_models['q50'], feats)\n",
    "\n",
    "        mae, mape = mean_absolute_error(y_te,seg_preds[1]), np.mean(np.abs((y_te-seg_preds[1])/y_te))*100\n",
    "        r2, cov = r2_score(y_te,seg_preds[1]), np.mean((y_te>=seg_preds[0])&(y_te<=seg_preds[2]))*100\n",
    "        metrics[seg] = {'n_train':len(X_tr),'n_test':len(X_te),'mae':mae,'mape':mape,'r2':r2,'cov':cov,\n",
    "                       'p_min':seg_df[y_col].min(),'p_max':seg_df[y_col].max(),'p_med':seg_df[y_col].median()}\n",
    "        print(f\"  {seg}: {len(test_df):,} test | MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.3f}\")\n",
    "\n",
    "        preds_list.append(pd.DataFrame({\n",
    "            'property_id':ids, 'state':states, 'actual':y_te, 'predicted':seg_preds[1],\n",
    "            'pred_lower':seg_preds[0], 'pred_upper':seg_preds[2], 'segment':seg\n",
    "        }))\n",
    "\n",
    "    return {'models':models, 'metrics':metrics, 'predictions':pd.concat(preds_list),\n",
    "            'feature_importance':feature_importance(models,feats,metrics), 'segment_importances':seg_imps,\n",
    "            'feature_names':feats, 'training_quantiles':train_q}\n",
    "\n",
    "def predict_new(pred_df, models, feats, y_col, id_col, state_col, kmeans, train_stats, train_q):\n",
    "    print(f\"\\n{'='*60}\\nPREDICTING {len(pred_df):,} NEW PROPERTIES\\n{'='*60}\")\n",
    "\n",
    "    pred_df = engineer(pred_df, y_col)\n",
    "    pred_df, _ = geo_cluster(pred_df, kmeans)\n",
    "\n",
    "    if 'geo_cluster' in pred_df.columns and train_stats is not None:\n",
    "        pred_df = pred_df.merge(train_stats, on='geo_cluster', how='left')\n",
    "        med = train_stats['cluster_avg_price'].median()\n",
    "        pred_df['cluster_avg_price'] = pred_df['cluster_avg_price'].fillna(med)\n",
    "        pred_df['cluster_med_price'] = pred_df['cluster_med_price'].fillna(med)\n",
    "\n",
    "    for feat in feats:\n",
    "        if feat not in pred_df.columns: pred_df[feat] = 0\n",
    "        else: pred_df[feat] = pred_df[feat].fillna(pred_df[feat].median() if pred_df[feat].notna().sum()>0 else 0)\n",
    "\n",
    "    pred_df['seg'], _ = assign_segments(pred_df, train_q)\n",
    "\n",
    "    if '_value_source' in pred_df.columns:\n",
    "        print(f\"Value sources: {dict(pred_df['_value_source'].value_counts())}\")\n",
    "\n",
    "    preds_list = []\n",
    "    for seg in pred_df['seg'].unique():\n",
    "        seg_df = pred_df[pred_df['seg']==seg].copy()\n",
    "\n",
    "        if seg not in models:\n",
    "            print(f\"  Warning: '{seg}' not in models, using fallback\")\n",
    "            avail = list(models.keys())\n",
    "            seg = next((s for s in avail if any(x in seg for x in ['ultra','luxury','premium','mid','economy','budget'] if x in s)), avail[0])\n",
    "\n",
    "        X = seg_df[feats].values\n",
    "        ids, states = seg_df[id_col].values, seg_df[state_col].values if state_col and state_col in seg_df.columns else ['Unknown']*len(seg_df)\n",
    "\n",
    "        preds = [models[seg][f\"q{int(q*100)}\"].predict(X) for q in QUANTILES]\n",
    "        actual = seg_df[y_col].values if y_col in seg_df.columns else [np.nan]*len(seg_df)\n",
    "\n",
    "        value_ind = seg_df['_value_indicator'].values if '_value_indicator' in seg_df.columns else [np.nan]*len(seg_df)\n",
    "        value_src = seg_df['_value_source'].values if '_value_source' in seg_df.columns else ['unknown']*len(seg_df)\n",
    "\n",
    "        preds_list.append(pd.DataFrame({\n",
    "            'property_id':ids, 'state':states, 'value_indicator':value_ind, 'value_source':value_src,\n",
    "            'actual':actual, 'predicted':preds[1], 'pred_lower':preds[0], 'pred_upper':preds[2], 'segment':seg,\n",
    "            'error':[actual[i]-preds[1][i] if not np.isnan(actual[i]) else np.nan for i in range(len(actual))],\n",
    "            'pct_error':[100*(actual[i]-preds[1][i])/actual[i] if not np.isnan(actual[i]) and actual[i]!=0 else np.nan for i in range(len(actual))]\n",
    "        }))\n",
    "\n",
    "        print(f\"  {seg}: {len(seg_df):,} predicted\")\n",
    "\n",
    "    result = pd.concat(preds_list, ignore_index=True)\n",
    "    print(f\"\\n✓ Generated {len(result):,} predictions\")\n",
    "\n",
    "    pred_df = pred_df.drop(columns=[c for c in ['_value_indicator','_value_source'] if c in pred_df.columns])\n",
    "\n",
    "    valid = result['actual'].notna().sum()\n",
    "    if valid>0:\n",
    "        valid_df = result[result['actual'].notna()]\n",
    "        mae, mape = mean_absolute_error(valid_df['actual'],valid_df['predicted']), np.mean(np.abs((valid_df['actual']-valid_df['predicted'])/valid_df['actual']))*100\n",
    "        r2 = r2_score(valid_df['actual'],valid_df['predicted'])\n",
    "        print(f\"  Validation ({valid}): MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.4f}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def save_results(results, out_dir, new_preds=None):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi, seg_imps = results['predictions'], results['metrics'], results['feature_importance'], results['segment_importances']\n",
    "\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    # Summary\n",
    "    ws = wb.create_sheet(\"Summary\", 0)\n",
    "    ws['A1'].font, ws['A1'].value = Font(bold=True,size=14), 'IMPROVED SEGMENTED AVM'\n",
    "    ws['A2'].font, ws['A2'].value = Font(italic=True,size=10), 'Validated assessments + property-adjusted census + 6 tiers'\n",
    "\n",
    "    r2, mae, mape = r2_score(preds['actual'],preds['predicted']), mean_absolute_error(preds['actual'],preds['predicted']), np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "\n",
    "    data = [['Metric','Value'], ['Properties',len(preds)], ['Segments',len(metrics)], ['R²',f'{r2:.4f}'], ['MAE',f'${mae:,.0f}'], ['MAPE%',f'{mape:.2f}%']]\n",
    "    if new_preds is not None: data.append(['New Predictions',len(new_preds)])\n",
    "\n",
    "    for i,(k,v) in enumerate(data,5):\n",
    "        ws[f'A{i}'].font, ws[f'A{i}'].value, ws[f'B{i}'].value = Font(bold=True), k, v\n",
    "\n",
    "    # Segments\n",
    "    ws = wb.create_sheet(\"Segments\")\n",
    "    seg_df = pd.DataFrame([{**{'Segment':s},**m} for s,m in metrics.items()])\n",
    "    for i,h in enumerate(seg_df.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font, c.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(seg_df.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    # Feature Importance\n",
    "    for title,data,name in [('Global Feature Importance',fi,'Global_Feature_Importance')]+[(f'FI: {s}',si,f\"FI_{s}\"[:31]) for s,si in seg_imps.items()]:\n",
    "        ws = wb.create_sheet(name)\n",
    "        ws['A1'].value, ws['A1'].font = title, Font(bold=True,size=12)\n",
    "        for r_idx,row in enumerate(dataframe_to_rows(data,index=False,header=True),2):\n",
    "            for c_idx,value in enumerate(row,1):\n",
    "                cell = ws.cell(row=r_idx,column=c_idx,value=value)\n",
    "                if r_idx==2: cell.font, cell.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "\n",
    "    # Predictions\n",
    "    for sheet_name,data,color in [('Test_Predictions',preds,'366092'), ('New_Predictions',new_preds,'4472C4')]:\n",
    "        if data is None: continue\n",
    "        ws = wb.create_sheet(sheet_name)\n",
    "        for i,h in enumerate(data.columns,1):\n",
    "            c = ws.cell(1,i,h)\n",
    "            c.font, c.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color=color,end_color=color,fill_type='solid')\n",
    "        for i,row in enumerate(data.itertuples(index=False),2):\n",
    "            for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/improved_segmented_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "\n",
    "    for name,data in [('test_predictions',preds), ('segments',seg_df), ('importance',fi)]+[(f'importance_{s}',si) for s,si in seg_imps.items()]+([('new_predictions',new_preds)] if new_preds is not None else []):\n",
    "        data.to_csv(f\"{out_dir}/improved_{name}_{ts}.csv\", index=False)\n",
    "\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved with timestamp: {ts}\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60+\"\\nIMPROVED SEGMENTED AVM\\nValidated Assessments + Property-Adjusted Census + 6 Tiers\\n\"+\"=\"*60)\n",
    "\n",
    "    df, y_col, id_col, state_col = load_data(TRAINING_INPUT_PATH)\n",
    "    df, feats, kmeans = prepare_data(df, y_col, id_col, state_col, for_training=True)\n",
    "    results = train_segments(df, feats, y_col, id_col, state_col)\n",
    "\n",
    "    train_stats = df.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index() if 'geo_cluster' in df.columns else None\n",
    "    if train_stats is not None: train_stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "\n",
    "    new_preds = None\n",
    "    if PREDICTION_INPUT_PATH:\n",
    "        pred_df, _, _, _ = load_data(PREDICTION_INPUT_PATH)\n",
    "        new_preds = predict_new(pred_df, results['models'], feats, y_col, id_col, state_col, kmeans, train_stats, results['training_quantiles'])\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR, new_preds)\n",
    "\n",
    "    preds = results['predictions']\n",
    "    r2, mae, mape = r2_score(preds['actual'],preds['predicted']), mean_absolute_error(preds['actual'],preds['predicted']), np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    print(f\"\\n{'='*60}\\n✓ COMPLETE in {time.time()-t0:.1f}s\\n  Test: {len(preds):,} | {preds['segment'].nunique()} segments\\n  R²:{r2:.4f} | MAE:${mae:,.0f} | MAPE:{mape:.2f}%\")\n",
    "    if new_preds is not None: print(f\"  New predictions: {len(new_preds):,}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__==\"__main__\": main()"
   ],
   "id": "3024f5e0d5303464",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IMPROVED SEGMENTED AVM\n",
      "Validated Assessments + Property-Adjusted Census + 6 Tiers\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 90.4MB | Price:sale_price ID:property_id\n",
      "\n",
      "Preparing data (property-adjusted census segmentation)...\n",
      "  ⚠️  Filtered 29,431 bad assessed values\n",
      "51/57 features available\n",
      "\n",
      "Training on 127,258 properties (with property-adjusted segmentation)\n",
      "\n",
      "Value quantiles: Q20:$234,441 Q40:$588,227 Q60:$962,426 Q80:$1,552,373 Q95:$3,183,290\n",
      "\n",
      "Value sources: {'census_adjusted': np.int64(127258)}\n",
      "\n",
      "12 segments\n",
      "  budget_small: 24,655 ($4,637-$234,400, med $105,480)\n",
      "  economy_small: 19,718 ($234,502-$588,222, med $409,100)\n",
      "  premium_large: 19,457 ($962,428-$1,552,302, med $1,218,987)\n",
      "  luxury_large: 16,594 ($1,552,423-$3,183,260, med $1,944,059)\n",
      "  mid_large: 14,703 ($588,240-$962,426, med $801,360)\n",
      "  mid_small: 10,749 ($588,229-$962,413, med $709,203)\n",
      "  ultra_large: 6,341 ($3,183,461-$9,360,005, med $4,285,591)\n",
      "\n",
      "Merged 1 small segments\n",
      "Consolidated to 7 segments\n",
      "  economy_small: 19,718→15,755 (20.1% filtered)\n",
      "  economy_small: 4,727 test | MAE:$300,622 | MAPE:18.91% | R²:0.334\n",
      "  luxury_large: 16,594→13,567 (18.2% filtered)\n",
      "  luxury_large: 4,070 test | MAE:$351,264 | MAPE:23.60% | R²:0.331\n",
      "  mid_large: 14,703→11,861 (19.3% filtered)\n",
      "  mid_large: 3,558 test | MAE:$207,350 | MAPE:15.79% | R²:0.396\n",
      "  budget_small: 24,655→19,847 (19.5% filtered)\n",
      "  budget_small: 5,954 test | MAE:$1,289,008 | MAPE:43.57% | R²:0.298\n",
      "  other: 21,382→17,325 (19.0% filtered)\n",
      "  other: 5,197 test | MAE:$331,875 | MAPE:21.33% | R²:0.567\n",
      "  premium_large: 19,457→15,933 (18.1% filtered)\n",
      "  premium_large: 4,780 test | MAE:$262,953 | MAPE:18.36% | R²:0.380\n",
      "  mid_small: 10,749→8,783 (18.3% filtered)\n",
      "  mid_small: 2,635 test | MAE:$160,115 | MAPE:13.17% | R²:0.210\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\n",
      "1 records | 0.0MB | Price:sale_price ID:property_id\n",
      "\n",
      "============================================================\n",
      "PREDICTING 1 NEW PROPERTIES\n",
      "============================================================\n",
      "  ⚠️  Filtered 1 bad assessed values\n",
      "  ⚠️  Filtered 1 bad assessed values\n",
      "Value sources: {'census_adjusted': np.int64(1)}\n",
      "  economy_small: 1 predicted\n",
      "\n",
      "✓ Generated 1 predictions\n",
      "  Validation (1): MAE:$390,420 | MAPE:74.22% | R²:nan\n",
      "\n",
      "Saving results...\n",
      "✓ Excel: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/improved_segmented_20251224_003858.xlsx\n",
      "✓ CSVs saved with timestamp: 20251224_003858\n",
      "\n",
      "============================================================\n",
      "✓ COMPLETE in 11.0s\n",
      "  Test: 30,921 | 7 segments\n",
      "  R²:0.4116 | MAE:$474,330 | MAPE:23.75%\n",
      "  New predictions: 1\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T05:48:01.426753Z",
     "start_time": "2025-12-24T05:47:49.949420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings, time, os\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Config\n",
    "INCLUDE_MLS, INCLUDE_CENSUS, INCLUDE_NEIGHBORHOOD, INCLUDE_IMAGE = True, True, True, False\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS, N_EST, MAX_SEGMENTS = 20000, 0.3, 42, -1, 8, 100, 7\n",
    "QUANTILES = [0.1, 0.5, 0.9]\n",
    "\n",
    "TRAINING_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "PREDICTION_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "BASE_FEATS = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"effective_year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\"fireplace_code\",\"latitude\",\"longitude\",\"geo_cluster\"]\n",
    "ENG_FEATS = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\"]\n",
    "PRIOR_FEATS = [\"prior_sale_price\",\"prior_price_per_sqft\",\"sqft_per_prior_dollar\",\"years_since_last_sale\",\"expected_appreciation\",\"has_prior_sale\",\"recently_sold\"]\n",
    "CLUSTER_FEATS = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "CENSUS_FEATS = [\"total_population_25plus\",\"male_bachelors_degree\",\"female_bachelors_degree\",\"pct_bachelors_degree\",\"median_earnings_total\",\"median_earnings_male\",\"median_earnings_female\",\"median_household_income\",\"median_home_value\",\"median_gross_rent\",\"owner_occupied_units\",\"renter_occupied_units\",\"pct_owner_occupied\",\"occupied_units\",\"vacant_units\",\"median_age\",\"civilian_employed\",\"civilian_unemployed\",\"unemployment_rate\",\"income_education_score\"]\n",
    "ELECT_FEATS = [\"votes_gop\",\"votes_dem\",\"total_votes\",\"per_gop\",\"per_dem\",\"per_point_diff\",\"dem_margin\",\"rep_margin\"]\n",
    "IMG_FEATS = [\"topic_1\",\"topic_2\",\"topic_3\",\"topic_4\",\"topic_5\",\"topic_6\",\"topic_7\",\"topic_8\",\"topic_9\",\"topic_10\",\"gran_c_in\",\"gran_c_ex\",\"gran_c\",\"high_c_in\",\"high_c_ex\",\"high_c\"]\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8' if set(df[c].dropna().unique()).issubset({0,1}) else 'int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = optimize_dtypes(pd.read_csv(path, low_memory=False))\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return df, y_col, id_col, state_col\n",
    "\n",
    "def filter_bad_assessed(df):\n",
    "    if 'assessed_total_value' not in df.columns:\n",
    "        return pd.Series([False]*len(df), index=df.index)\n",
    "\n",
    "    valid = (df['assessed_total_value']>10000) & (df['assessed_total_value']<100000000)\n",
    "\n",
    "    if 'living_sqft' in df.columns:\n",
    "        has_sqft = (df['living_sqft'].notna()) & (df['living_sqft']>100)\n",
    "        ppsf = df['assessed_total_value']/df['living_sqft']\n",
    "        valid &= ~(has_sqft & ((ppsf<20)|(ppsf>2000)))\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        has_prior = (df['prior_sale_price']>10000) & (df['years_since_last_sale']<=15)\n",
    "        valid &= ~(has_prior & ((df['assessed_total_value']/df['prior_sale_price'])<0.10))\n",
    "\n",
    "    if 'median_home_value' in df.columns:\n",
    "        has_census = (df['median_home_value']>10000)\n",
    "        valid &= ~(has_census & ((df['assessed_total_value']/df['median_home_value'])<0.05))\n",
    "\n",
    "    invalid_cnt = ((df['assessed_total_value'].notna()) & ~valid).sum()\n",
    "    if invalid_cnt>0:\n",
    "        print(f\"  ⚠️  Filtered {invalid_cnt:,} bad assessed values\")\n",
    "\n",
    "    return valid\n",
    "\n",
    "def assign_segments(df, train_q=None):\n",
    "    value_ind = pd.Series([np.nan]*len(df), index=df.index)\n",
    "    source = pd.Series(['none']*len(df), index=df.index)\n",
    "\n",
    "    # Priority 1: Prior sale\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        recent = (df['years_since_last_sale']<=10) & (df['prior_sale_price']>10000)\n",
    "        if recent.sum()>0:\n",
    "            yrs = df.loc[recent,'years_since_last_sale'].fillna(5)\n",
    "            value_ind[recent] = df.loc[recent,'prior_sale_price']*(1.04**yrs)\n",
    "            source[recent] = 'prior_sale'\n",
    "\n",
    "    # Priority 2: COMPOSITE approach (assessed + adjusted census)\n",
    "    missing_val = value_ind.isna()\n",
    "    if missing_val.sum()>0:\n",
    "        composite_vals = []\n",
    "\n",
    "        # Get assessed value (with relaxed filter for composite)\n",
    "        assessed_component = pd.Series([np.nan]*len(df), index=df.index)\n",
    "        if 'assessed_total_value' in df.columns:\n",
    "            # Use assessed if it passes basic sanity (more lenient than filter_bad_assessed)\n",
    "            reasonable = (df['assessed_total_value']>50000) & (df['assessed_total_value']<50000000)\n",
    "            if 'living_sqft' in df.columns:\n",
    "                has_sqft = (df['living_sqft']>100)\n",
    "                ppsf = df['assessed_total_value']/df['living_sqft']\n",
    "                reasonable &= ~(has_sqft & ((ppsf<50)|(ppsf>1500)))  # More lenient than 20-2000\n",
    "\n",
    "            if reasonable.sum()>0:\n",
    "                vals = df.loc[reasonable,'assessed_total_value']\n",
    "                mult = pd.Series([1.0]*len(vals), index=vals.index)\n",
    "                mult[vals<200000], mult[(vals>=200000)&(vals<500000)] = 1.1, 1.15\n",
    "                mult[vals>=500000] = 1.2\n",
    "                assessed_component[reasonable] = vals*mult\n",
    "\n",
    "        # Get AGGRESSIVELY adjusted census\n",
    "        census_component = pd.Series([np.nan]*len(df), index=df.index)\n",
    "        if 'median_home_value' in df.columns:\n",
    "            has_census = (df['median_home_value']>10000)\n",
    "            if has_census.sum()>0:\n",
    "                base = df.loc[has_census,'median_home_value'].copy()\n",
    "\n",
    "                # MUCH MORE AGGRESSIVE adjustments\n",
    "                if 'living_sqft' in df.columns:\n",
    "                    # Sqft adjustment: 0.5x to 4.0x (linear, not clipped hard)\n",
    "                    sqft_ratio = (df.loc[has_census,'living_sqft']/1800).clip(0.5, 4.0)\n",
    "                    base *= sqft_ratio\n",
    "\n",
    "                # Quality multiplier from multiple features\n",
    "                quality = pd.Series([1.0]*has_census.sum(), index=df[has_census].index)\n",
    "\n",
    "                if 'full_baths' in df.columns:\n",
    "                    # Each bath above 2 = +25%, below 2 = -15%\n",
    "                    bath_diff = df.loc[has_census,'full_baths'].fillna(2) - 2\n",
    "                    quality *= (1 + bath_diff*0.25).clip(0.7, 2.0)\n",
    "\n",
    "                if 'bedrooms' in df.columns:\n",
    "                    # Each bed above 3 = +15%, below 3 = -10%\n",
    "                    bed_diff = df.loc[has_census,'bedrooms'].fillna(3) - 3\n",
    "                    quality *= (1 + bed_diff*0.15).clip(0.75, 1.8)\n",
    "\n",
    "                if 'garage_spaces' in df.columns:\n",
    "                    # Each garage space = +15%\n",
    "                    garage = df.loc[has_census,'garage_spaces'].fillna(0)\n",
    "                    quality *= (1 + garage*0.15).clip(1.0, 1.6)\n",
    "\n",
    "                if 'year_built' in df.columns:\n",
    "                    # Newer homes worth more\n",
    "                    age = (2024 - df.loc[has_census,'year_built']).fillna(50)\n",
    "                    age_adj = (1 - (age-20)/200).clip(0.8, 1.3)  # Sweet spot at 20 years\n",
    "                    quality *= age_adj\n",
    "\n",
    "                census_component[has_census] = base * quality\n",
    "\n",
    "        # Take the MAXIMUM of assessed and census_adjusted (avoid under-estimation)\n",
    "        for idx in missing_val[missing_val].index:\n",
    "            vals_to_compare = []\n",
    "            if not pd.isna(assessed_component[idx]):\n",
    "                vals_to_compare.append((assessed_component[idx], 'assessed_composite'))\n",
    "            if not pd.isna(census_component[idx]):\n",
    "                vals_to_compare.append((census_component[idx], 'census_composite'))\n",
    "\n",
    "            if vals_to_compare:\n",
    "                # Take the higher value to avoid under-segmentation\n",
    "                best_val, best_source = max(vals_to_compare, key=lambda x: x[0])\n",
    "                value_ind[idx] = best_val\n",
    "                source[idx] = best_source\n",
    "\n",
    "    # Fallback: Sqft estimate\n",
    "    if 'living_sqft' in df.columns:\n",
    "        valid = (df['living_sqft']>0) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            ppsf = pd.Series([200.0]*valid.sum(), index=df[valid].index)\n",
    "            if 'median_home_value' in df.columns:\n",
    "                ppsf = (df.loc[valid,'median_home_value'].fillna(350000)/1800).clip(80,600)\n",
    "            value_ind[valid], source[valid] = df.loc[valid,'living_sqft']*ppsf, 'sqft_estimate'\n",
    "\n",
    "    value_ind = value_ind.fillna(value_ind.median())\n",
    "    source[value_ind.isna()] = 'fallback'\n",
    "\n",
    "    # Segment\n",
    "    if train_q is None:\n",
    "        q20,q40,q60,q80,q95 = value_ind.quantile([0.20,0.40,0.60,0.80,0.95])\n",
    "        train_q = {'value_q20':q20,'value_q40':q40,'value_q60':q60,'value_q80':q80,'value_q95':q95}\n",
    "        print(f\"\\nValue quantiles: Q20:${q20:,.0f} Q40:${q40:,.0f} Q60:${q60:,.0f} Q80:${q80:,.0f} Q95:${q95:,.0f}\")\n",
    "    else:\n",
    "        q20,q40,q60,q80,q95 = train_q['value_q20'],train_q['value_q40'],train_q['value_q60'],train_q['value_q80'],train_q['value_q95']\n",
    "\n",
    "    tier = pd.Series(['mid']*len(df), index=df.index)\n",
    "    tier[value_ind<q20], tier[(value_ind>=q20)&(value_ind<q40)] = 'budget', 'economy'\n",
    "    tier[(value_ind>=q60)&(value_ind<q80)], tier[(value_ind>=q80)&(value_ind<q95)], tier[value_ind>=q95] = 'premium', 'luxury', 'ultra'\n",
    "\n",
    "    size = pd.Series(['small']*len(df), index=df.index)\n",
    "    if 'living_sqft' in df.columns:\n",
    "        sqft = df['living_sqft'].fillna(df['living_sqft'].median())\n",
    "        size[sqft>sqft.median()] = 'large'\n",
    "\n",
    "    df['_value_indicator'], df['_value_source'] = value_ind, source\n",
    "    return tier+'_'+size, train_q\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    if 'living_sqft' in df.columns:\n",
    "        if 'bedrooms' in df.columns: df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "        df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns: df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns: df['lot_acres'] = df['lot_sqft']/43560\n",
    "\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'], df['is_new'], df['age_squared'] = 2024-df['year_built'], ((2024-df['year_built'])<=5).astype('int8'), (2024-df['year_built'])**2\n",
    "\n",
    "    if 'garage_spaces' in df.columns:\n",
    "        df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        df['prior_appreciated'] = df['prior_sale_price']*(1.04**df['years_since_last_sale'].fillna(5))\n",
    "\n",
    "    if 'assessed_total_value' in df.columns:\n",
    "        valid = filter_bad_assessed(df)\n",
    "        if 'living_sqft' in df.columns:\n",
    "            df['assessed_per_sqft'] = 0\n",
    "            df.loc[valid,'assessed_per_sqft'] = df.loc[valid,'assessed_total_value']/(df.loc[valid,'living_sqft']+1)\n",
    "        if 'median_home_value' in df.columns:\n",
    "            reasonable = valid & (df['median_home_value']>10000)\n",
    "            df['assessed_to_census_ratio'] = 1.0\n",
    "            df.loc[reasonable,'assessed_to_census_ratio'] = df.loc[reasonable,'assessed_total_value']/(df.loc[reasonable,'median_home_value']+1)\n",
    "        if 'assessed_land_value' in df.columns:\n",
    "            reasonable = valid & (df['assessed_land_value']>0)\n",
    "            df['land_to_total_ratio'] = 0\n",
    "            df.loc[reasonable,'land_to_total_ratio'] = df.loc[reasonable,'assessed_land_value']/(df.loc[reasonable,'assessed_total_value']+1)\n",
    "\n",
    "    if INCLUDE_CENSUS and all(c in df.columns for c in ['median_household_income','pct_bachelors_degree']):\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'living_sqft' in df.columns:\n",
    "        df['prior_price_per_sqft'], df['sqft_per_prior_dollar'] = df['prior_sale_price']/(df['living_sqft']+1), df['living_sqft']/(df['prior_sale_price']+1)\n",
    "\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        if 'prior_sale_price' in df.columns:\n",
    "            df['expected_appreciation'] = df['prior_sale_price']*(1.04)**df['years_since_last_sale']\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df.loc[missing,'median_home_value'] if 'median_home_value' in df.columns and INCLUDE_CENSUS else df['prior_sale_price'].median()\n",
    "\n",
    "    if 'years_since_last_sale' in df.columns:\n",
    "        df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "\n",
    "    if with_price and y_col in df.columns and 'living_sqft' in df.columns:\n",
    "        df['price_per_sqft'], df['sqft_per_dollar'] = df[y_col]/(df['living_sqft']+1), df['living_sqft']/(df[y_col]+1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df, kmeans=None):\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans\n",
    "\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum()<N_CLUSTERS:\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans\n",
    "\n",
    "    df['geo_cluster'] = 0\n",
    "    if kmeans is None:\n",
    "        kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "        df.loc[valid,'geo_cluster'] = kmeans.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    else:\n",
    "        df.loc[valid,'geo_cluster'] = kmeans.predict(df.loc[valid,['latitude','longitude']])\n",
    "\n",
    "    return df, kmeans\n",
    "\n",
    "def add_cluster_feats(train, test, y_col):\n",
    "    if 'geo_cluster' not in train.columns or y_col not in train.columns:\n",
    "        for d in [train,test]:\n",
    "            d['cluster_avg_price'] = d['cluster_med_price'] = train[y_col].median() if y_col in train.columns else 0\n",
    "        return train, test\n",
    "\n",
    "    stats = train.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "    stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "    med = train[y_col].median()\n",
    "    train = train.merge(stats, on='geo_cluster', how='left').fillna({'cluster_avg_price':med,'cluster_med_price':med})\n",
    "    test = test.merge(stats, on='geo_cluster', how='left').fillna({'cluster_avg_price':med,'cluster_med_price':med})\n",
    "    return train, test\n",
    "\n",
    "def filter_outliers(df, name, y_col):\n",
    "    orig = len(df)\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)].drop(columns=['price_per_sqft','sqft_per_dollar'])\n",
    "\n",
    "    if 'lot_sqft' in df.columns: df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "    if 'year_built' in df.columns: df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "\n",
    "    if len(df)>=100:\n",
    "        try:\n",
    "            feats = [c for c in ['living_sqft','lot_sqft',y_col,'year_built'] if c in df.columns]\n",
    "            if len(feats)>=3:\n",
    "                X = df[feats].fillna(df[feats].median())\n",
    "                df = df[IsolationForest(contamination=.05, random_state=RAND_STATE, n_jobs=N_JOBS).fit_predict(X)==1]\n",
    "        except: pass\n",
    "\n",
    "    if (pct:=(orig-len(df))/orig*100)>0: print(f\"  {name}: {orig:,}→{len(df):,} ({pct:.1f}% filtered)\")\n",
    "    return df\n",
    "\n",
    "def train_model(X, y, q):\n",
    "    return XGBRegressor(objective='reg:quantileerror', quantile_alpha=q, n_estimators=N_EST, learning_rate=.1,\n",
    "                       max_depth=5, min_child_weight=3, subsample=.8, colsample_bytree=.8,\n",
    "                       random_state=RAND_STATE, n_jobs=N_JOBS, tree_method='hist').fit(X, y, verbose=False)\n",
    "\n",
    "def get_feat_importance(model, feats, top_n=20):\n",
    "    scores = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    imp = [(feats[int(k[1:])],v) for k,v in scores.items() if int(k[1:])<len(feats)]\n",
    "    imp.sort(key=lambda x: x[1], reverse=True)\n",
    "    total = sum(v for _,v in imp)\n",
    "    return pd.DataFrame([{'feature':f,'gain':g,'importance':g/total} for f,g in imp[:top_n]]) if total>0 else pd.DataFrame(columns=['feature','gain','importance'])\n",
    "\n",
    "def feature_importance(models, feats, metrics):\n",
    "    rows = []\n",
    "    for seg,mdl in models.items():\n",
    "        scores = mdl['q50'].get_booster().get_score(importance_type=\"gain\")\n",
    "        w = metrics[seg][\"n_test\"]\n",
    "        rows.extend([(feats[int(k[1:])],v,w) for k,v in scores.items() if int(k[1:])<len(feats)])\n",
    "\n",
    "    if not rows: return pd.DataFrame(columns=[\"feature\",\"total_gain\",\"importance\"])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"feature\",\"gain\",\"weight\"])\n",
    "    out = df.assign(wg=df[\"gain\"]*df[\"weight\"]).groupby(\"feature\",as_index=False).agg(\n",
    "        total_gain=(\"gain\",\"sum\"), weighted_gain=(\"wg\",\"sum\")).sort_values(\"weighted_gain\",ascending=False)\n",
    "    out[\"importance\"] = out[\"weighted_gain\"]/out[\"weighted_gain\"].sum()\n",
    "    return out[[\"feature\",\"total_gain\",\"importance\"]].head(100)\n",
    "\n",
    "def prepare_data(df, y_col, id_col, state_col, for_training=True):\n",
    "    print(f\"\\nPreparing data (composite valuation segmentation)...\")\n",
    "    if for_training: df = df[df[y_col]>=MIN_PRICE]\n",
    "\n",
    "    df, kmeans = geo_cluster(engineer(df, y_col))\n",
    "\n",
    "    feat_groups = []\n",
    "    if INCLUDE_MLS: feat_groups.extend([BASE_FEATS,ENG_FEATS,PRIOR_FEATS,CLUSTER_FEATS])\n",
    "    if INCLUDE_CENSUS: feat_groups.append(CENSUS_FEATS)\n",
    "    if INCLUDE_NEIGHBORHOOD: feat_groups.append(ELECT_FEATS)\n",
    "    if INCLUDE_IMAGE: feat_groups.append(IMG_FEATS)\n",
    "\n",
    "    all_feats = [f for g in feat_groups for f in g]\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "    feats.extend([vf for vf in ['prior_appreciated','assessed_per_sqft','assessed_to_census_ratio','land_to_total_ratio'] if vf in df.columns and vf not in feats])\n",
    "\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "\n",
    "    return (df.dropna(subset=[y_col]),feats,kmeans) if for_training else (df,feats,kmeans)\n",
    "\n",
    "def train_segments(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining on {len(df):,} properties (composite valuation)\")\n",
    "\n",
    "    df['seg'], train_q = assign_segments(df)\n",
    "\n",
    "    if '_value_source' in df.columns:\n",
    "        print(f\"\\nValue sources: {dict(df['_value_source'].value_counts())}\")\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    print(f\"\\n{len(seg_cnts)} segments\")\n",
    "\n",
    "    for seg,cnt in seg_cnts.head(MAX_SEGMENTS).items():\n",
    "        if '_value_indicator' in df.columns and seg!='other':\n",
    "            seg_df = df[df['seg']==seg]\n",
    "            if len(seg_df)>0:\n",
    "                print(f\"  {seg}: {cnt:,} (${seg_df['_value_indicator'].min():,.0f}-${seg_df['_value_indicator'].max():,.0f}, med ${seg_df['_value_indicator'].median():,.0f})\")\n",
    "        else:\n",
    "            print(f\"  {seg}: {cnt:,}\")\n",
    "\n",
    "    small = seg_cnts[seg_cnts<50].index\n",
    "    if len(small)>0:\n",
    "        df.loc[df['seg'].isin(small),'seg'] = 'other'\n",
    "        print(f\"\\nMerged {len(small)} small segments\")\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    if len(seg_cnts)>MAX_SEGMENTS:\n",
    "        keep = seg_cnts.head(MAX_SEGMENTS-1).index\n",
    "        df.loc[~df['seg'].isin(keep),'seg'] = 'other'\n",
    "        print(f\"Consolidated to {MAX_SEGMENTS} segments\")\n",
    "\n",
    "    df = df.drop(columns=[c for c in ['_value_indicator','_value_source'] if c in df.columns])\n",
    "\n",
    "    models, metrics, preds_list, seg_imps = {}, {}, [], {}\n",
    "\n",
    "    for seg in df['seg'].unique():\n",
    "        seg_df = filter_outliers(df[df['seg']==seg].copy(), seg, y_col)\n",
    "        if len(seg_df)<50: continue\n",
    "\n",
    "        train_idx = seg_df.sample(frac=1-TEST_SIZE, random_state=RAND_STATE).index\n",
    "        train_df, test_df = seg_df.loc[train_idx].copy(), seg_df.loc[seg_df.index.difference(train_idx)].copy()\n",
    "        train_df, test_df = add_cluster_feats(train_df, test_df, y_col)\n",
    "\n",
    "        X_tr, y_tr = train_df[feats].values, train_df[y_col].values\n",
    "        X_te, y_te = test_df[feats].values, test_df[y_col].values\n",
    "        ids, states = test_df[id_col].values, test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "\n",
    "        seg_models = {f\"q{int(q*100)}\":train_model(X_tr,y_tr,q) for q in QUANTILES}\n",
    "        seg_preds = [seg_models[f\"q{int(q*100)}\"].predict(X_te) for q in QUANTILES]\n",
    "\n",
    "        models[seg] = seg_models\n",
    "        seg_imps[seg] = get_feat_importance(seg_models['q50'], feats)\n",
    "\n",
    "        mae, mape = mean_absolute_error(y_te,seg_preds[1]), np.mean(np.abs((y_te-seg_preds[1])/y_te))*100\n",
    "        r2, cov = r2_score(y_te,seg_preds[1]), np.mean((y_te>=seg_preds[0])&(y_te<=seg_preds[2]))*100\n",
    "        metrics[seg] = {'n_train':len(X_tr),'n_test':len(X_te),'mae':mae,'mape':mape,'r2':r2,'cov':cov,\n",
    "                       'p_min':seg_df[y_col].min(),'p_max':seg_df[y_col].max(),'p_med':seg_df[y_col].median()}\n",
    "        print(f\"  {seg}: {len(test_df):,} test | MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.3f}\")\n",
    "\n",
    "        preds_list.append(pd.DataFrame({\n",
    "            'property_id':ids, 'state':states, 'actual':y_te, 'predicted':seg_preds[1],\n",
    "            'pred_lower':seg_preds[0], 'pred_upper':seg_preds[2], 'segment':seg\n",
    "        }))\n",
    "\n",
    "    return {'models':models, 'metrics':metrics, 'predictions':pd.concat(preds_list),\n",
    "            'feature_importance':feature_importance(models,feats,metrics), 'segment_importances':seg_imps,\n",
    "            'feature_names':feats, 'training_quantiles':train_q}\n",
    "\n",
    "def predict_new(pred_df, models, feats, y_col, id_col, state_col, kmeans, train_stats, train_q):\n",
    "    print(f\"\\n{'='*60}\\nPREDICTING {len(pred_df):,} NEW PROPERTIES\\n{'='*60}\")\n",
    "\n",
    "    pred_df = engineer(pred_df, y_col)\n",
    "    pred_df, _ = geo_cluster(pred_df, kmeans)\n",
    "\n",
    "    if 'geo_cluster' in pred_df.columns and train_stats is not None:\n",
    "        pred_df = pred_df.merge(train_stats, on='geo_cluster', how='left')\n",
    "        med = train_stats['cluster_avg_price'].median()\n",
    "        pred_df['cluster_avg_price'] = pred_df['cluster_avg_price'].fillna(med)\n",
    "        pred_df['cluster_med_price'] = pred_df['cluster_med_price'].fillna(med)\n",
    "\n",
    "    for feat in feats:\n",
    "        if feat not in pred_df.columns: pred_df[feat] = 0\n",
    "        else: pred_df[feat] = pred_df[feat].fillna(pred_df[feat].median() if pred_df[feat].notna().sum()>0 else 0)\n",
    "\n",
    "    pred_df['seg'], _ = assign_segments(pred_df, train_q)\n",
    "\n",
    "    if '_value_source' in pred_df.columns:\n",
    "        print(f\"Value sources: {dict(pred_df['_value_source'].value_counts())}\")\n",
    "\n",
    "    preds_list = []\n",
    "    for seg in pred_df['seg'].unique():\n",
    "        seg_df = pred_df[pred_df['seg']==seg].copy()\n",
    "\n",
    "        if seg not in models:\n",
    "            print(f\"  Warning: '{seg}' not in models, using fallback\")\n",
    "            avail = list(models.keys())\n",
    "            seg = next((s for s in avail if any(x in seg for x in ['ultra','luxury','premium','mid','economy','budget'] if x in s)), avail[0])\n",
    "\n",
    "        X = seg_df[feats].values\n",
    "        ids, states = seg_df[id_col].values, seg_df[state_col].values if state_col and state_col in seg_df.columns else ['Unknown']*len(seg_df)\n",
    "\n",
    "        preds = [models[seg][f\"q{int(q*100)}\"].predict(X) for q in QUANTILES]\n",
    "        actual = seg_df[y_col].values if y_col in seg_df.columns else [np.nan]*len(seg_df)\n",
    "\n",
    "        value_ind = seg_df['_value_indicator'].values if '_value_indicator' in seg_df.columns else [np.nan]*len(seg_df)\n",
    "        value_src = seg_df['_value_source'].values if '_value_source' in seg_df.columns else ['unknown']*len(seg_df)\n",
    "\n",
    "        preds_list.append(pd.DataFrame({\n",
    "            'property_id':ids, 'state':states, 'value_indicator':value_ind, 'value_source':value_src,\n",
    "            'actual':actual, 'predicted':preds[1], 'pred_lower':preds[0], 'pred_upper':preds[2], 'segment':seg,\n",
    "            'error':[actual[i]-preds[1][i] if not np.isnan(actual[i]) else np.nan for i in range(len(actual))],\n",
    "            'pct_error':[100*(actual[i]-preds[1][i])/actual[i] if not np.isnan(actual[i]) and actual[i]!=0 else np.nan for i in range(len(actual))]\n",
    "        }))\n",
    "\n",
    "        print(f\"  {seg}: {len(seg_df):,} predicted\")\n",
    "\n",
    "    result = pd.concat(preds_list, ignore_index=True)\n",
    "    print(f\"\\n✓ Generated {len(result):,} predictions\")\n",
    "\n",
    "    pred_df = pred_df.drop(columns=[c for c in ['_value_indicator','_value_source'] if c in pred_df.columns])\n",
    "\n",
    "    valid = result['actual'].notna().sum()\n",
    "    if valid>0:\n",
    "        valid_df = result[result['actual'].notna()]\n",
    "        mae, mape = mean_absolute_error(valid_df['actual'],valid_df['predicted']), np.mean(np.abs((valid_df['actual']-valid_df['predicted'])/valid_df['actual']))*100\n",
    "        r2 = r2_score(valid_df['actual'],valid_df['predicted'])\n",
    "        print(f\"  Validation ({valid}): MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.4f}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def save_results(results, out_dir, new_preds=None):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi, seg_imps = results['predictions'], results['metrics'], results['feature_importance'], results['segment_importances']\n",
    "\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    # Summary\n",
    "    ws = wb.create_sheet(\"Summary\", 0)\n",
    "    ws['A1'].font, ws['A1'].value = Font(bold=True,size=14), 'IMPROVED SEGMENTED AVM'\n",
    "    ws['A2'].font, ws['A2'].value = Font(italic=True,size=10), 'Composite valuation (assessed + aggressively adjusted census) + 6 tiers'\n",
    "\n",
    "    r2, mae, mape = r2_score(preds['actual'],preds['predicted']), mean_absolute_error(preds['actual'],preds['predicted']), np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "\n",
    "    data = [['Metric','Value'], ['Properties',len(preds)], ['Segments',len(metrics)], ['R²',f'{r2:.4f}'], ['MAE',f'${mae:,.0f}'], ['MAPE%',f'{mape:.2f}%']]\n",
    "    if new_preds is not None: data.append(['New Predictions',len(new_preds)])\n",
    "\n",
    "    for i,(k,v) in enumerate(data,5):\n",
    "        ws[f'A{i}'].font, ws[f'A{i}'].value, ws[f'B{i}'].value = Font(bold=True), k, v\n",
    "\n",
    "    # Segments\n",
    "    ws = wb.create_sheet(\"Segments\")\n",
    "    seg_df = pd.DataFrame([{**{'Segment':s},**m} for s,m in metrics.items()])\n",
    "    for i,h in enumerate(seg_df.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font, c.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(seg_df.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    # Feature Importance\n",
    "    for title,data,name in [('Global Feature Importance',fi,'Global_Feature_Importance')]+[(f'FI: {s}',si,f\"FI_{s}\"[:31]) for s,si in seg_imps.items()]:\n",
    "        ws = wb.create_sheet(name)\n",
    "        ws['A1'].value, ws['A1'].font = title, Font(bold=True,size=12)\n",
    "        for r_idx,row in enumerate(dataframe_to_rows(data,index=False,header=True),2):\n",
    "            for c_idx,value in enumerate(row,1):\n",
    "                cell = ws.cell(row=r_idx,column=c_idx,value=value)\n",
    "                if r_idx==2: cell.font, cell.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "\n",
    "    # Predictions\n",
    "    for sheet_name,data,color in [('Test_Predictions',preds,'366092'), ('New_Predictions',new_preds,'4472C4')]:\n",
    "        if data is None: continue\n",
    "        ws = wb.create_sheet(sheet_name)\n",
    "        for i,h in enumerate(data.columns,1):\n",
    "            c = ws.cell(1,i,h)\n",
    "            c.font, c.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color=color,end_color=color,fill_type='solid')\n",
    "        for i,row in enumerate(data.itertuples(index=False),2):\n",
    "            for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/improved_segmented_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "\n",
    "    for name,data in [('test_predictions',preds), ('segments',seg_df), ('importance',fi)]+[(f'importance_{s}',si) for s,si in seg_imps.items()]+([('new_predictions',new_preds)] if new_preds is not None else []):\n",
    "        data.to_csv(f\"{out_dir}/improved_{name}_{ts}.csv\", index=False)\n",
    "\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved with timestamp: {ts}\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60+\"\\nIMPROVED SEGMENTED AVM\\nComposite Valuation (Assessed + Aggressively Adjusted Census)\\n\"+\"=\"*60)\n",
    "\n",
    "    df, y_col, id_col, state_col = load_data(TRAINING_INPUT_PATH)\n",
    "    df, feats, kmeans = prepare_data(df, y_col, id_col, state_col, for_training=True)\n",
    "    results = train_segments(df, feats, y_col, id_col, state_col)\n",
    "\n",
    "    train_stats = df.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index() if 'geo_cluster' in df.columns else None\n",
    "    if train_stats is not None: train_stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "\n",
    "    new_preds = None\n",
    "    if PREDICTION_INPUT_PATH:\n",
    "        pred_df, _, _, _ = load_data(PREDICTION_INPUT_PATH)\n",
    "        new_preds = predict_new(pred_df, results['models'], feats, y_col, id_col, state_col, kmeans, train_stats, results['training_quantiles'])\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR, new_preds)\n",
    "\n",
    "    preds = results['predictions']\n",
    "    r2, mae, mape = r2_score(preds['actual'],preds['predicted']), mean_absolute_error(preds['actual'],preds['predicted']), np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    print(f\"\\n{'='*60}\\n✓ COMPLETE in {time.time()-t0:.1f}s\\n  Test: {len(preds):,} | {preds['segment'].nunique()} segments\\n  R²:{r2:.4f} | MAE:${mae:,.0f} | MAPE:{mape:.2f}%\")\n",
    "    if new_preds is not None: print(f\"  New predictions: {len(new_preds):,}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__==\"__main__\": main()"
   ],
   "id": "83412ecd08df782f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IMPROVED SEGMENTED AVM\n",
      "Composite Valuation (Assessed + Aggressively Adjusted Census)\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 90.4MB | Price:sale_price ID:property_id\n",
      "\n",
      "Preparing data (composite valuation segmentation)...\n",
      "  ⚠️  Filtered 29,431 bad assessed values\n",
      "51/57 features available\n",
      "\n",
      "Training on 127,258 properties (composite valuation)\n",
      "\n",
      "Value quantiles: Q20:$248,054 Q40:$770,306 Q60:$1,474,192 Q80:$2,762,586 Q95:$6,447,392\n",
      "\n",
      "Value sources: {'census_composite': np.int64(127258)}\n",
      "\n",
      "12 segments\n",
      "  budget_small: 24,857 ($3,519-$248,049, med $99,605)\n",
      "  economy_small: 20,762 ($248,062-$770,303, med $479,515)\n",
      "  premium_large: 20,415 ($1,474,224-$2,762,514, med $2,014,426)\n",
      "  luxury_large: 18,122 ($2,762,634-$6,447,311, med $3,873,393)\n",
      "  mid_large: 13,442 ($770,306-$1,474,184, med $1,151,100)\n",
      "  mid_small: 12,010 ($770,327-$1,474,166, med $1,024,507)\n",
      "  ultra_large: 6,362 ($6,447,852-$41,649,446, med $9,201,787)\n",
      "\n",
      "Merged 1 small segments\n",
      "Consolidated to 7 segments\n",
      "  economy_small: 20,762→16,641 (19.8% filtered)\n",
      "  economy_small: 4,992 test | MAE:$291,313 | MAPE:18.55% | R²:0.362\n",
      "  luxury_large: 18,122→14,844 (18.1% filtered)\n",
      "  luxury_large: 4,453 test | MAE:$349,322 | MAPE:22.76% | R²:0.371\n",
      "  premium_large: 20,415→16,750 (18.0% filtered)\n",
      "  premium_large: 5,025 test | MAE:$230,531 | MAPE:17.45% | R²:0.323\n",
      "  budget_small: 24,857→19,864 (20.1% filtered)\n",
      "  budget_small: 5,959 test | MAE:$1,189,252 | MAPE:42.35% | R²:0.299\n",
      "  other: 17,650→14,250 (19.3% filtered)\n",
      "  other: 4,275 test | MAE:$386,123 | MAPE:23.06% | R²:0.555\n",
      "  mid_large: 13,442→10,788 (19.7% filtered)\n",
      "  mid_large: 3,236 test | MAE:$217,552 | MAPE:16.38% | R²:0.360\n",
      "  mid_small: 12,010→9,863 (17.9% filtered)\n",
      "  mid_small: 2,959 test | MAE:$169,513 | MAPE:13.89% | R²:0.208\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\n",
      "1 records | 0.0MB | Price:sale_price ID:property_id\n",
      "\n",
      "============================================================\n",
      "PREDICTING 1 NEW PROPERTIES\n",
      "============================================================\n",
      "  ⚠️  Filtered 1 bad assessed values\n",
      "Value sources: {'census_composite': np.int64(1)}\n",
      "  economy_small: 1 predicted\n",
      "\n",
      "✓ Generated 1 predictions\n",
      "  Validation (1): MAE:$376,786 | MAPE:71.63% | R²:nan\n",
      "\n",
      "Saving results...\n",
      "✓ Excel: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/improved_segmented_20251224_004759.xlsx\n",
      "✓ CSVs saved with timestamp: 20251224_004759\n",
      "\n",
      "============================================================\n",
      "✓ COMPLETE in 11.4s\n",
      "  Test: 30,899 | 7 segments\n",
      "  R²:0.4129 | MAE:$456,688 | MAPE:23.52%\n",
      "  New predictions: 1\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T05:53:08.393509Z",
     "start_time": "2025-12-24T05:52:57.794315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings, time, os\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Config\n",
    "INCLUDE_MLS, INCLUDE_CENSUS, INCLUDE_NEIGHBORHOOD, INCLUDE_IMAGE = True, True, True, False\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS, N_EST, MAX_SEGMENTS = 20000, 0.3, 42, -1, 8, 100, 7\n",
    "QUANTILES = [0.1, 0.5, 0.9]\n",
    "\n",
    "TRAINING_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "PREDICTION_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "BASE_FEATS = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"effective_year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\"fireplace_code\",\"latitude\",\"longitude\",\"geo_cluster\"]\n",
    "ENG_FEATS = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\"]\n",
    "PRIOR_FEATS = [\"prior_sale_price\",\"prior_price_per_sqft\",\"sqft_per_prior_dollar\",\"years_since_last_sale\",\"expected_appreciation\",\"has_prior_sale\",\"recently_sold\"]\n",
    "CLUSTER_FEATS = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "CENSUS_FEATS = [\"total_population_25plus\",\"male_bachelors_degree\",\"female_bachelors_degree\",\"pct_bachelors_degree\",\"median_earnings_total\",\"median_earnings_male\",\"median_earnings_female\",\"median_household_income\",\"median_home_value\",\"median_gross_rent\",\"owner_occupied_units\",\"renter_occupied_units\",\"pct_owner_occupied\",\"occupied_units\",\"vacant_units\",\"median_age\",\"civilian_employed\",\"civilian_unemployed\",\"unemployment_rate\",\"income_education_score\"]\n",
    "ELECT_FEATS = [\"votes_gop\",\"votes_dem\",\"total_votes\",\"per_gop\",\"per_dem\",\"per_point_diff\",\"dem_margin\",\"rep_margin\"]\n",
    "IMG_FEATS = [\"topic_1\",\"topic_2\",\"topic_3\",\"topic_4\",\"topic_5\",\"topic_6\",\"topic_7\",\"topic_8\",\"topic_9\",\"topic_10\",\"gran_c_in\",\"gran_c_ex\",\"gran_c\",\"high_c_in\",\"high_c_ex\",\"high_c\"]\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8' if set(df[c].dropna().unique()).issubset({0,1}) else 'int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = optimize_dtypes(pd.read_csv(path, low_memory=False))\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return df, y_col, id_col, state_col\n",
    "\n",
    "def filter_bad_assessed(df):\n",
    "    if 'assessed_total_value' not in df.columns:\n",
    "        return pd.Series([False]*len(df), index=df.index)\n",
    "\n",
    "    valid = (df['assessed_total_value']>10000) & (df['assessed_total_value']<100000000)\n",
    "\n",
    "    if 'living_sqft' in df.columns:\n",
    "        has_sqft = (df['living_sqft'].notna()) & (df['living_sqft']>100)\n",
    "        ppsf = df['assessed_total_value']/df['living_sqft']\n",
    "        valid &= ~(has_sqft & ((ppsf<20)|(ppsf>2000)))\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        has_prior = (df['prior_sale_price']>10000) & (df['years_since_last_sale']<=15)\n",
    "        valid &= ~(has_prior & ((df['assessed_total_value']/df['prior_sale_price'])<0.10))\n",
    "\n",
    "    if 'median_home_value' in df.columns:\n",
    "        has_census = (df['median_home_value']>10000)\n",
    "        valid &= ~(has_census & ((df['assessed_total_value']/df['median_home_value'])<0.05))\n",
    "\n",
    "    invalid_cnt = ((df['assessed_total_value'].notna()) & ~valid).sum()\n",
    "    if invalid_cnt>0:\n",
    "        print(f\"  ⚠️  Filtered {invalid_cnt:,} bad assessed values\")\n",
    "\n",
    "    return valid\n",
    "\n",
    "def assign_segments(df, train_q=None):\n",
    "    value_ind = pd.Series([np.nan]*len(df), index=df.index)\n",
    "    source = pd.Series(['none']*len(df), index=df.index)\n",
    "\n",
    "    # Priority 1: Prior sale PPSF × current sqft (property-specific, external - NO LEAKAGE)\n",
    "    if all(c in df.columns for c in ['prior_sale_price','years_since_last_sale','living_sqft']):\n",
    "        has_prior = (df['prior_sale_price']>10000) & (df['years_since_last_sale']<=15) & (df['living_sqft']>400)\n",
    "        if has_prior.sum()>0:\n",
    "            # Calculate historical PPSF from prior sale\n",
    "            prior_ppsf = df.loc[has_prior,'prior_sale_price'] / (df.loc[has_prior,'living_sqft']+1)\n",
    "            # Appreciate the PPSF over time\n",
    "            yrs = df.loc[has_prior,'years_since_last_sale'].fillna(5)\n",
    "            appreciated_ppsf = prior_ppsf * (1.04 ** yrs)\n",
    "            # Apply to current sqft\n",
    "            value_ind[has_prior] = appreciated_ppsf * df.loc[has_prior,'living_sqft']\n",
    "            source[has_prior] = 'prior_ppsf'\n",
    "\n",
    "    # Priority 2: Assessed value (external data - NO LEAKAGE)\n",
    "    if 'assessed_total_value' in df.columns:\n",
    "        valid = filter_bad_assessed(df) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            vals = df.loc[valid,'assessed_total_value']\n",
    "            mult = pd.Series([1.15]*len(vals), index=vals.index)\n",
    "            mult[vals<200000], mult[vals>=500000] = 1.1, 1.2\n",
    "            value_ind[valid], source[valid] = vals*mult, 'assessed'\n",
    "\n",
    "    # Priority 3: Census median (external, neighborhood-level - NO LEAKAGE)\n",
    "    if 'median_home_value' in df.columns:\n",
    "        valid = (df['median_home_value']>10000) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            value_ind[valid], source[valid] = df.loc[valid,'median_home_value'], 'census'\n",
    "\n",
    "    # Priority 4: Geo cluster median price (if available from training)\n",
    "    if 'cluster_med_price' in df.columns:\n",
    "        valid = (df['cluster_med_price']>10000) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            value_ind[valid], source[valid] = df.loc[valid,'cluster_med_price'], 'cluster'\n",
    "\n",
    "    # Fallback: Global median\n",
    "    value_ind = value_ind.fillna(value_ind.median())\n",
    "    source[value_ind.isna()] = 'global_median'\n",
    "\n",
    "    # Segment\n",
    "    if train_q is None:\n",
    "        q20,q40,q60,q80,q95 = value_ind.quantile([0.20,0.40,0.60,0.80,0.95])\n",
    "        train_q = {'value_q20':q20,'value_q40':q40,'value_q60':q60,'value_q80':q80,'value_q95':q95}\n",
    "        print(f\"\\nValue quantiles: Q20:${q20:,.0f} Q40:${q40:,.0f} Q60:${q60:,.0f} Q80:${q80:,.0f} Q95:${q95:,.0f}\")\n",
    "    else:\n",
    "        q20,q40,q60,q80,q95 = train_q['value_q20'],train_q['value_q40'],train_q['value_q60'],train_q['value_q80'],train_q['value_q95']\n",
    "\n",
    "    tier = pd.Series(['mid']*len(df), index=df.index)\n",
    "    tier[value_ind<q20], tier[(value_ind>=q20)&(value_ind<q40)] = 'budget', 'economy'\n",
    "    tier[(value_ind>=q60)&(value_ind<q80)], tier[(value_ind>=q80)&(value_ind<q95)], tier[value_ind>=q95] = 'premium', 'luxury', 'ultra'\n",
    "\n",
    "    size = pd.Series(['small']*len(df), index=df.index)\n",
    "    if 'living_sqft' in df.columns:\n",
    "        sqft = df['living_sqft'].fillna(df['living_sqft'].median())\n",
    "        size[sqft>sqft.median()] = 'large'\n",
    "\n",
    "    df['_value_indicator'], df['_value_source'] = value_ind, source\n",
    "    return tier+'_'+size, train_q\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    if 'living_sqft' in df.columns:\n",
    "        if 'bedrooms' in df.columns: df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "        df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns: df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns: df['lot_acres'] = df['lot_sqft']/43560\n",
    "\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'], df['is_new'], df['age_squared'] = 2024-df['year_built'], ((2024-df['year_built'])<=5).astype('int8'), (2024-df['year_built'])**2\n",
    "\n",
    "    if 'garage_spaces' in df.columns:\n",
    "        df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        df['prior_appreciated'] = df['prior_sale_price']*(1.04**df['years_since_last_sale'].fillna(5))\n",
    "\n",
    "    if 'assessed_total_value' in df.columns:\n",
    "        valid = filter_bad_assessed(df)\n",
    "        if 'living_sqft' in df.columns:\n",
    "            df['assessed_per_sqft'] = 0\n",
    "            df.loc[valid,'assessed_per_sqft'] = df.loc[valid,'assessed_total_value']/(df.loc[valid,'living_sqft']+1)\n",
    "        if 'median_home_value' in df.columns:\n",
    "            reasonable = valid & (df['median_home_value']>10000)\n",
    "            df['assessed_to_census_ratio'] = 1.0\n",
    "            df.loc[reasonable,'assessed_to_census_ratio'] = df.loc[reasonable,'assessed_total_value']/(df.loc[reasonable,'median_home_value']+1)\n",
    "        if 'assessed_land_value' in df.columns:\n",
    "            reasonable = valid & (df['assessed_land_value']>0)\n",
    "            df['land_to_total_ratio'] = 0\n",
    "            df.loc[reasonable,'land_to_total_ratio'] = df.loc[reasonable,'assessed_land_value']/(df.loc[reasonable,'assessed_total_value']+1)\n",
    "\n",
    "    if INCLUDE_CENSUS and all(c in df.columns for c in ['median_household_income','pct_bachelors_degree']):\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'living_sqft' in df.columns:\n",
    "        df['prior_price_per_sqft'], df['sqft_per_prior_dollar'] = df['prior_sale_price']/(df['living_sqft']+1), df['living_sqft']/(df['prior_sale_price']+1)\n",
    "\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        if 'prior_sale_price' in df.columns:\n",
    "            df['expected_appreciation'] = df['prior_sale_price']*(1.04)**df['years_since_last_sale']\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df.loc[missing,'median_home_value'] if 'median_home_value' in df.columns and INCLUDE_CENSUS else df['prior_sale_price'].median()\n",
    "\n",
    "    if 'years_since_last_sale' in df.columns:\n",
    "        df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "\n",
    "    if with_price and y_col in df.columns and 'living_sqft' in df.columns:\n",
    "        df['price_per_sqft'], df['sqft_per_dollar'] = df[y_col]/(df['living_sqft']+1), df['living_sqft']/(df[y_col]+1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df, kmeans=None):\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans\n",
    "\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum()<N_CLUSTERS:\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans\n",
    "\n",
    "    df['geo_cluster'] = 0\n",
    "    if kmeans is None:\n",
    "        kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "        df.loc[valid,'geo_cluster'] = kmeans.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    else:\n",
    "        df.loc[valid,'geo_cluster'] = kmeans.predict(df.loc[valid,['latitude','longitude']])\n",
    "\n",
    "    return df, kmeans\n",
    "\n",
    "def add_cluster_feats(train, test, y_col):\n",
    "    if 'geo_cluster' not in train.columns or y_col not in train.columns:\n",
    "        for d in [train,test]:\n",
    "            d['cluster_avg_price'] = d['cluster_med_price'] = train[y_col].median() if y_col in train.columns else 0\n",
    "        return train, test\n",
    "\n",
    "    stats = train.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "    stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "    med = train[y_col].median()\n",
    "    train = train.merge(stats, on='geo_cluster', how='left').fillna({'cluster_avg_price':med,'cluster_med_price':med})\n",
    "    test = test.merge(stats, on='geo_cluster', how='left').fillna({'cluster_avg_price':med,'cluster_med_price':med})\n",
    "    return train, test\n",
    "\n",
    "def filter_outliers(df, name, y_col):\n",
    "    orig = len(df)\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)].drop(columns=['price_per_sqft','sqft_per_dollar'])\n",
    "\n",
    "    if 'lot_sqft' in df.columns: df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "    if 'year_built' in df.columns: df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "\n",
    "    if len(df)>=100:\n",
    "        try:\n",
    "            feats = [c for c in ['living_sqft','lot_sqft',y_col,'year_built'] if c in df.columns]\n",
    "            if len(feats)>=3:\n",
    "                X = df[feats].fillna(df[feats].median())\n",
    "                df = df[IsolationForest(contamination=.05, random_state=RAND_STATE, n_jobs=N_JOBS).fit_predict(X)==1]\n",
    "        except: pass\n",
    "\n",
    "    if (pct:=(orig-len(df))/orig*100)>0: print(f\"  {name}: {orig:,}→{len(df):,} ({pct:.1f}% filtered)\")\n",
    "    return df\n",
    "\n",
    "def train_model(X, y, q):\n",
    "    return XGBRegressor(objective='reg:quantileerror', quantile_alpha=q, n_estimators=N_EST, learning_rate=.1,\n",
    "                       max_depth=5, min_child_weight=3, subsample=.8, colsample_bytree=.8,\n",
    "                       random_state=RAND_STATE, n_jobs=N_JOBS, tree_method='hist').fit(X, y, verbose=False)\n",
    "\n",
    "def get_feat_importance(model, feats, top_n=20):\n",
    "    scores = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    imp = [(feats[int(k[1:])],v) for k,v in scores.items() if int(k[1:])<len(feats)]\n",
    "    imp.sort(key=lambda x: x[1], reverse=True)\n",
    "    total = sum(v for _,v in imp)\n",
    "    return pd.DataFrame([{'feature':f,'gain':g,'importance':g/total} for f,g in imp[:top_n]]) if total>0 else pd.DataFrame(columns=['feature','gain','importance'])\n",
    "\n",
    "def feature_importance(models, feats, metrics):\n",
    "    rows = []\n",
    "    for seg,mdl in models.items():\n",
    "        scores = mdl['q50'].get_booster().get_score(importance_type=\"gain\")\n",
    "        w = metrics[seg][\"n_test\"]\n",
    "        rows.extend([(feats[int(k[1:])],v,w) for k,v in scores.items() if int(k[1:])<len(feats)])\n",
    "\n",
    "    if not rows: return pd.DataFrame(columns=[\"feature\",\"total_gain\",\"importance\"])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"feature\",\"gain\",\"weight\"])\n",
    "    out = df.assign(wg=df[\"gain\"]*df[\"weight\"]).groupby(\"feature\",as_index=False).agg(\n",
    "        total_gain=(\"gain\",\"sum\"), weighted_gain=(\"wg\",\"sum\")).sort_values(\"weighted_gain\",ascending=False)\n",
    "    out[\"importance\"] = out[\"weighted_gain\"]/out[\"weighted_gain\"].sum()\n",
    "    return out[[\"feature\",\"total_gain\",\"importance\"]].head(100)\n",
    "\n",
    "def prepare_data(df, y_col, id_col, state_col, for_training=True):\n",
    "    print(f\"\\nPreparing data (prior PPSF × sqft segmentation)...\")\n",
    "    if for_training: df = df[df[y_col]>=MIN_PRICE]\n",
    "\n",
    "    df, kmeans = geo_cluster(engineer(df, y_col))\n",
    "\n",
    "    feat_groups = []\n",
    "    if INCLUDE_MLS: feat_groups.extend([BASE_FEATS,ENG_FEATS,PRIOR_FEATS,CLUSTER_FEATS])\n",
    "    if INCLUDE_CENSUS: feat_groups.append(CENSUS_FEATS)\n",
    "    if INCLUDE_NEIGHBORHOOD: feat_groups.append(ELECT_FEATS)\n",
    "    if INCLUDE_IMAGE: feat_groups.append(IMG_FEATS)\n",
    "\n",
    "    all_feats = [f for g in feat_groups for f in g]\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "    feats.extend([vf for vf in ['prior_appreciated','assessed_per_sqft','assessed_to_census_ratio','land_to_total_ratio'] if vf in df.columns and vf not in feats])\n",
    "\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "\n",
    "    return (df.dropna(subset=[y_col]),feats,kmeans) if for_training else (df,feats,kmeans)\n",
    "\n",
    "def train_segments(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining on {len(df):,} properties (prior PPSF × sqft)\")\n",
    "\n",
    "    df['seg'], train_q = assign_segments(df)\n",
    "\n",
    "    if '_value_source' in df.columns:\n",
    "        print(f\"\\nValue sources: {dict(df['_value_source'].value_counts())}\")\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    print(f\"\\n{len(seg_cnts)} segments\")\n",
    "\n",
    "    for seg,cnt in seg_cnts.head(MAX_SEGMENTS).items():\n",
    "        if '_value_indicator' in df.columns and seg!='other':\n",
    "            seg_df = df[df['seg']==seg]\n",
    "            if len(seg_df)>0:\n",
    "                print(f\"  {seg}: {cnt:,} (${seg_df['_value_indicator'].min():,.0f}-${seg_df['_value_indicator'].max():,.0f}, med ${seg_df['_value_indicator'].median():,.0f})\")\n",
    "        else:\n",
    "            print(f\"  {seg}: {cnt:,}\")\n",
    "\n",
    "    small = seg_cnts[seg_cnts<50].index\n",
    "    if len(small)>0:\n",
    "        df.loc[df['seg'].isin(small),'seg'] = 'other'\n",
    "        print(f\"\\nMerged {len(small)} small segments\")\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    if len(seg_cnts)>MAX_SEGMENTS:\n",
    "        keep = seg_cnts.head(MAX_SEGMENTS-1).index\n",
    "        df.loc[~df['seg'].isin(keep),'seg'] = 'other'\n",
    "        print(f\"Consolidated to {MAX_SEGMENTS} segments\")\n",
    "\n",
    "    df = df.drop(columns=[c for c in ['_value_indicator','_value_source'] if c in df.columns])\n",
    "\n",
    "    models, metrics, preds_list, seg_imps = {}, {}, [], {}\n",
    "\n",
    "    for seg in df['seg'].unique():\n",
    "        seg_df = filter_outliers(df[df['seg']==seg].copy(), seg, y_col)\n",
    "        if len(seg_df)<50: continue\n",
    "\n",
    "        train_idx = seg_df.sample(frac=1-TEST_SIZE, random_state=RAND_STATE).index\n",
    "        train_df, test_df = seg_df.loc[train_idx].copy(), seg_df.loc[seg_df.index.difference(train_idx)].copy()\n",
    "        train_df, test_df = add_cluster_feats(train_df, test_df, y_col)\n",
    "\n",
    "        X_tr, y_tr = train_df[feats].values, train_df[y_col].values\n",
    "        X_te, y_te = test_df[feats].values, test_df[y_col].values\n",
    "        ids, states = test_df[id_col].values, test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "\n",
    "        seg_models = {f\"q{int(q*100)}\":train_model(X_tr,y_tr,q) for q in QUANTILES}\n",
    "        seg_preds = [seg_models[f\"q{int(q*100)}\"].predict(X_te) for q in QUANTILES]\n",
    "\n",
    "        models[seg] = seg_models\n",
    "        seg_imps[seg] = get_feat_importance(seg_models['q50'], feats)\n",
    "\n",
    "        mae, mape = mean_absolute_error(y_te,seg_preds[1]), np.mean(np.abs((y_te-seg_preds[1])/y_te))*100\n",
    "        r2, cov = r2_score(y_te,seg_preds[1]), np.mean((y_te>=seg_preds[0])&(y_te<=seg_preds[2]))*100\n",
    "        metrics[seg] = {'n_train':len(X_tr),'n_test':len(X_te),'mae':mae,'mape':mape,'r2':r2,'cov':cov,\n",
    "                       'p_min':seg_df[y_col].min(),'p_max':seg_df[y_col].max(),'p_med':seg_df[y_col].median()}\n",
    "        print(f\"  {seg}: {len(test_df):,} test | MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.3f}\")\n",
    "\n",
    "        preds_list.append(pd.DataFrame({\n",
    "            'property_id':ids, 'state':states, 'actual':y_te, 'predicted':seg_preds[1],\n",
    "            'pred_lower':seg_preds[0], 'pred_upper':seg_preds[2], 'segment':seg\n",
    "        }))\n",
    "\n",
    "    return {'models':models, 'metrics':metrics, 'predictions':pd.concat(preds_list),\n",
    "            'feature_importance':feature_importance(models,feats,metrics), 'segment_importances':seg_imps,\n",
    "            'feature_names':feats, 'training_quantiles':train_q}\n",
    "\n",
    "def predict_new(pred_df, models, feats, y_col, id_col, state_col, kmeans, train_stats, train_q):\n",
    "    print(f\"\\n{'='*60}\\nPREDICTING {len(pred_df):,} NEW PROPERTIES\\n{'='*60}\")\n",
    "\n",
    "    pred_df = engineer(pred_df, y_col)\n",
    "    pred_df, _ = geo_cluster(pred_df, kmeans)\n",
    "\n",
    "    if 'geo_cluster' in pred_df.columns and train_stats is not None:\n",
    "        pred_df = pred_df.merge(train_stats, on='geo_cluster', how='left')\n",
    "        med = train_stats['cluster_avg_price'].median()\n",
    "        pred_df['cluster_avg_price'] = pred_df['cluster_avg_price'].fillna(med)\n",
    "        pred_df['cluster_med_price'] = pred_df['cluster_med_price'].fillna(med)\n",
    "\n",
    "    for feat in feats:\n",
    "        if feat not in pred_df.columns: pred_df[feat] = 0\n",
    "        else: pred_df[feat] = pred_df[feat].fillna(pred_df[feat].median() if pred_df[feat].notna().sum()>0 else 0)\n",
    "\n",
    "    pred_df['seg'], _ = assign_segments(pred_df, train_q)\n",
    "\n",
    "    if '_value_source' in pred_df.columns:\n",
    "        print(f\"Value sources: {dict(pred_df['_value_source'].value_counts())}\")\n",
    "\n",
    "    preds_list = []\n",
    "    for seg in pred_df['seg'].unique():\n",
    "        seg_df = pred_df[pred_df['seg']==seg].copy()\n",
    "\n",
    "        if seg not in models:\n",
    "            print(f\"  Warning: '{seg}' not in models, using fallback\")\n",
    "            avail = list(models.keys())\n",
    "            seg = next((s for s in avail if any(x in seg for x in ['ultra','luxury','premium','mid','economy','budget'] if x in s)), avail[0])\n",
    "\n",
    "        X = seg_df[feats].values\n",
    "        ids, states = seg_df[id_col].values, seg_df[state_col].values if state_col and state_col in seg_df.columns else ['Unknown']*len(seg_df)\n",
    "\n",
    "        preds = [models[seg][f\"q{int(q*100)}\"].predict(X) for q in QUANTILES]\n",
    "        actual = seg_df[y_col].values if y_col in seg_df.columns else [np.nan]*len(seg_df)\n",
    "\n",
    "        value_ind = seg_df['_value_indicator'].values if '_value_indicator' in seg_df.columns else [np.nan]*len(seg_df)\n",
    "        value_src = seg_df['_value_source'].values if '_value_source' in seg_df.columns else ['unknown']*len(seg_df)\n",
    "\n",
    "        preds_list.append(pd.DataFrame({\n",
    "            'property_id':ids, 'state':states, 'value_indicator':value_ind, 'value_source':value_src,\n",
    "            'actual':actual, 'predicted':preds[1], 'pred_lower':preds[0], 'pred_upper':preds[2], 'segment':seg,\n",
    "            'error':[actual[i]-preds[1][i] if not np.isnan(actual[i]) else np.nan for i in range(len(actual))],\n",
    "            'pct_error':[100*(actual[i]-preds[1][i])/actual[i] if not np.isnan(actual[i]) and actual[i]!=0 else np.nan for i in range(len(actual))]\n",
    "        }))\n",
    "\n",
    "        print(f\"  {seg}: {len(seg_df):,} predicted\")\n",
    "\n",
    "    result = pd.concat(preds_list, ignore_index=True)\n",
    "    print(f\"\\n✓ Generated {len(result):,} predictions\")\n",
    "\n",
    "    pred_df = pred_df.drop(columns=[c for c in ['_value_indicator','_value_source'] if c in pred_df.columns])\n",
    "\n",
    "    valid = result['actual'].notna().sum()\n",
    "    if valid>0:\n",
    "        valid_df = result[result['actual'].notna()]\n",
    "        mae, mape = mean_absolute_error(valid_df['actual'],valid_df['predicted']), np.mean(np.abs((valid_df['actual']-valid_df['predicted'])/valid_df['actual']))*100\n",
    "        r2 = r2_score(valid_df['actual'],valid_df['predicted'])\n",
    "        print(f\"  Validation ({valid}): MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.4f}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def save_results(results, out_dir, new_preds=None):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi, seg_imps = results['predictions'], results['metrics'], results['feature_importance'], results['segment_importances']\n",
    "\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    # Summary\n",
    "    ws = wb.create_sheet(\"Summary\", 0)\n",
    "    ws['A1'].font, ws['A1'].value = Font(bold=True,size=14), 'IMPROVED SEGMENTED AVM'\n",
    "    ws['A2'].font, ws['A2'].value = Font(italic=True,size=10), 'Prior PPSF × Current Sqft (property-specific, no leakage) + 6 tiers'\n",
    "\n",
    "    r2, mae, mape = r2_score(preds['actual'],preds['predicted']), mean_absolute_error(preds['actual'],preds['predicted']), np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "\n",
    "    data = [['Metric','Value'], ['Properties',len(preds)], ['Segments',len(metrics)], ['R²',f'{r2:.4f}'], ['MAE',f'${mae:,.0f}'], ['MAPE%',f'{mape:.2f}%']]\n",
    "    if new_preds is not None: data.append(['New Predictions',len(new_preds)])\n",
    "\n",
    "    for i,(k,v) in enumerate(data,5):\n",
    "        ws[f'A{i}'].font, ws[f'A{i}'].value, ws[f'B{i}'].value = Font(bold=True), k, v\n",
    "\n",
    "    # Segments\n",
    "    ws = wb.create_sheet(\"Segments\")\n",
    "    seg_df = pd.DataFrame([{**{'Segment':s},**m} for s,m in metrics.items()])\n",
    "    for i,h in enumerate(seg_df.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font, c.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(seg_df.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    # Feature Importance\n",
    "    for title,data,name in [('Global Feature Importance',fi,'Global_Feature_Importance')]+[(f'FI: {s}',si,f\"FI_{s}\"[:31]) for s,si in seg_imps.items()]:\n",
    "        ws = wb.create_sheet(name)\n",
    "        ws['A1'].value, ws['A1'].font = title, Font(bold=True,size=12)\n",
    "        for r_idx,row in enumerate(dataframe_to_rows(data,index=False,header=True),2):\n",
    "            for c_idx,value in enumerate(row,1):\n",
    "                cell = ws.cell(row=r_idx,column=c_idx,value=value)\n",
    "                if r_idx==2: cell.font, cell.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "\n",
    "    # Predictions\n",
    "    for sheet_name,data,color in [('Test_Predictions',preds,'366092'), ('New_Predictions',new_preds,'4472C4')]:\n",
    "        if data is None: continue\n",
    "        ws = wb.create_sheet(sheet_name)\n",
    "        for i,h in enumerate(data.columns,1):\n",
    "            c = ws.cell(1,i,h)\n",
    "            c.font, c.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color=color,end_color=color,fill_type='solid')\n",
    "        for i,row in enumerate(data.itertuples(index=False),2):\n",
    "            for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/improved_segmented_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "\n",
    "    for name,data in [('test_predictions',preds), ('segments',seg_df), ('importance',fi)]+[(f'importance_{s}',si) for s,si in seg_imps.items()]+([('new_predictions',new_preds)] if new_preds is not None else []):\n",
    "        data.to_csv(f\"{out_dir}/improved_{name}_{ts}.csv\", index=False)\n",
    "\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved with timestamp: {ts}\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60+\"\\nIMPROVED SEGMENTED AVM\\nPrior PPSF × Current Sqft (Property-Specific, No Leakage)\\n\"+\"=\"*60)\n",
    "\n",
    "    df, y_col, id_col, state_col = load_data(TRAINING_INPUT_PATH)\n",
    "    df, feats, kmeans = prepare_data(df, y_col, id_col, state_col, for_training=True)\n",
    "    results = train_segments(df, feats, y_col, id_col, state_col)\n",
    "\n",
    "    train_stats = df.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index() if 'geo_cluster' in df.columns else None\n",
    "    if train_stats is not None: train_stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "\n",
    "    new_preds = None\n",
    "    if PREDICTION_INPUT_PATH:\n",
    "        pred_df, _, _, _ = load_data(PREDICTION_INPUT_PATH)\n",
    "        new_preds = predict_new(pred_df, results['models'], feats, y_col, id_col, state_col, kmeans, train_stats, results['training_quantiles'])\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR, new_preds)\n",
    "\n",
    "    preds = results['predictions']\n",
    "    r2, mae, mape = r2_score(preds['actual'],preds['predicted']), mean_absolute_error(preds['actual'],preds['predicted']), np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    print(f\"\\n{'='*60}\\n✓ COMPLETE in {time.time()-t0:.1f}s\\n  Test: {len(preds):,} | {preds['segment'].nunique()} segments\\n  R²:{r2:.4f} | MAE:${mae:,.0f} | MAPE:{mape:.2f}%\")\n",
    "    if new_preds is not None: print(f\"  New predictions: {len(new_preds):,}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__==\"__main__\": main()"
   ],
   "id": "e612f83e39a457bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IMPROVED SEGMENTED AVM\n",
      "Prior PPSF × Current Sqft (Property-Specific, No Leakage)\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 90.4MB | Price:sale_price ID:property_id\n",
      "\n",
      "Preparing data (prior PPSF × sqft segmentation)...\n",
      "  ⚠️  Filtered 29,431 bad assessed values\n",
      "51/57 features available\n",
      "\n",
      "Training on 127,258 properties (prior PPSF × sqft)\n",
      "\n",
      "Value quantiles: Q20:$225,450 Q40:$434,750 Q60:$434,750 Q80:$741,940 Q95:$1,445,367\n",
      "\n",
      "Value sources: {'census': np.int64(127258)}\n",
      "\n",
      "10 segments\n",
      "  premium_large: 27,925 ($434,750-$732,125, med $434,750)\n",
      "  premium_small: 26,120 ($434,750-$732,125, med $434,750)\n",
      "  budget_small: 19,274 ($12,000-$225,433, med $134,133)\n",
      "  economy_large: 13,496 ($225,450-$433,875, med $335,713)\n",
      "  luxury_large: 11,219 ($741,940-$1,426,334, med $926,975)\n",
      "  economy_small: 8,817 ($225,450-$433,875, med $320,400)\n",
      "  luxury_small: 7,745 ($741,940-$1,426,334, med $923,550)\n",
      "Consolidated to 7 segments\n",
      "  budget_small: 19,274→15,269 (20.8% filtered)\n",
      "  budget_small: 4,581 test | MAE:$1,041,100 | MAPE:45.73% | R²:0.320\n",
      "  premium_large: 27,925→22,872 (18.1% filtered)\n",
      "  premium_large: 6,862 test | MAE:$320,782 | MAPE:20.78% | R²:0.443\n",
      "  premium_small: 26,120→21,295 (18.5% filtered)\n",
      "  premium_small: 6,389 test | MAE:$352,763 | MAPE:20.38% | R²:0.323\n",
      "  other: 20,407→16,679 (18.3% filtered)\n",
      "  other: 5,004 test | MAE:$307,699 | MAPE:20.12% | R²:0.555\n",
      "  economy_small: 8,817→6,985 (20.8% filtered)\n",
      "  economy_small: 2,095 test | MAE:$277,282 | MAPE:17.78% | R²:0.121\n",
      "  economy_large: 13,496→10,785 (20.1% filtered)\n",
      "  economy_large: 3,236 test | MAE:$171,196 | MAPE:13.91% | R²:0.293\n",
      "  luxury_large: 11,219→9,120 (18.7% filtered)\n",
      "  luxury_large: 2,736 test | MAE:$334,876 | MAPE:21.60% | R²:0.475\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\n",
      "1 records | 0.0MB | Price:sale_price ID:property_id\n",
      "\n",
      "============================================================\n",
      "PREDICTING 1 NEW PROPERTIES\n",
      "============================================================\n",
      "  ⚠️  Filtered 1 bad assessed values\n",
      "  ⚠️  Filtered 1 bad assessed values\n",
      "Value sources: {'census': np.int64(1)}\n",
      "  economy_small: 1 predicted\n",
      "\n",
      "✓ Generated 1 predictions\n",
      "  Validation (1): MAE:$414,381 | MAPE:78.78% | R²:nan\n",
      "\n",
      "Saving results...\n",
      "✓ Excel: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/improved_segmented_20251224_005306.xlsx\n",
      "✓ CSVs saved with timestamp: 20251224_005306\n",
      "\n",
      "============================================================\n",
      "✓ COMPLETE in 10.6s\n",
      "  Test: 30,903 | 7 segments\n",
      "  R²:0.4125 | MAE:$414,689 | MAPE:23.44%\n",
      "  New predictions: 1\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T05:58:28.307964Z",
     "start_time": "2025-12-24T05:58:19.112141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings, time, os\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Config\n",
    "INCLUDE_MLS, INCLUDE_CENSUS, INCLUDE_NEIGHBORHOOD, INCLUDE_IMAGE = True, True, True, False\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS, N_EST, MAX_SEGMENTS = 20000, 0.3, 42, -1, 8, 100, 7\n",
    "QUANTILES = [0.1, 0.5, 0.9]\n",
    "\n",
    "TRAINING_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "PREDICTION_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "BASE_FEATS = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"effective_year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\"fireplace_code\",\"latitude\",\"longitude\",\"geo_cluster\",\"value_indicator\"]\n",
    "ENG_FEATS = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\",\"log_value_indicator\"]\n",
    "PRIOR_FEATS = [\"prior_sale_price\",\"prior_price_per_sqft\",\"sqft_per_prior_dollar\",\"years_since_last_sale\",\"expected_appreciation\",\"has_prior_sale\",\"recently_sold\"]\n",
    "CLUSTER_FEATS = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "CENSUS_FEATS = [\"total_population_25plus\",\"male_bachelors_degree\",\"female_bachelors_degree\",\"pct_bachelors_degree\",\"median_earnings_total\",\"median_earnings_male\",\"median_earnings_female\",\"median_household_income\",\"median_home_value\",\"median_gross_rent\",\"owner_occupied_units\",\"renter_occupied_units\",\"pct_owner_occupied\",\"occupied_units\",\"vacant_units\",\"median_age\",\"civilian_employed\",\"civilian_unemployed\",\"unemployment_rate\",\"income_education_score\"]\n",
    "ELECT_FEATS = [\"votes_gop\",\"votes_dem\",\"total_votes\",\"per_gop\",\"per_dem\",\"per_point_diff\",\"dem_margin\",\"rep_margin\"]\n",
    "IMG_FEATS = [\"topic_1\",\"topic_2\",\"topic_3\",\"topic_4\",\"topic_5\",\"topic_6\",\"topic_7\",\"topic_8\",\"topic_9\",\"topic_10\",\"gran_c_in\",\"gran_c_ex\",\"gran_c\",\"high_c_in\",\"high_c_ex\",\"high_c\"]\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8' if set(df[c].dropna().unique()).issubset({0,1}) else 'int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = optimize_dtypes(pd.read_csv(path, low_memory=False))\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return df, y_col, id_col, state_col\n",
    "\n",
    "def filter_bad_assessed(df):\n",
    "    if 'assessed_total_value' not in df.columns:\n",
    "        return pd.Series([False]*len(df), index=df.index)\n",
    "\n",
    "    valid = (df['assessed_total_value']>10000) & (df['assessed_total_value']<100000000)\n",
    "\n",
    "    if 'living_sqft' in df.columns:\n",
    "        has_sqft = (df['living_sqft'].notna()) & (df['living_sqft']>100)\n",
    "        ppsf = df['assessed_total_value']/df['living_sqft']\n",
    "        valid &= ~(has_sqft & ((ppsf<20)|(ppsf>2000)))\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        has_prior = (df['prior_sale_price']>10000) & (df['years_since_last_sale']<=15)\n",
    "        valid &= ~(has_prior & ((df['assessed_total_value']/df['prior_sale_price'])<0.10))\n",
    "\n",
    "    if 'median_home_value' in df.columns:\n",
    "        has_census = (df['median_home_value']>10000)\n",
    "        valid &= ~(has_census & ((df['assessed_total_value']/df['median_home_value'])<0.05))\n",
    "\n",
    "    invalid_cnt = ((df['assessed_total_value'].notna()) & ~valid).sum()\n",
    "    if invalid_cnt>0:\n",
    "        print(f\"  ⚠️  Filtered {invalid_cnt:,} bad assessed values\")\n",
    "\n",
    "    return valid\n",
    "\n",
    "def assign_segments(df, train_q=None):\n",
    "    value_ind = pd.Series([np.nan]*len(df), index=df.index)\n",
    "    source = pd.Series(['none']*len(df), index=df.index)\n",
    "\n",
    "    # Priority 1: Prior sale PPSF × current sqft (property-specific, external - NO LEAKAGE)\n",
    "    if all(c in df.columns for c in ['prior_sale_price','years_since_last_sale','living_sqft']):\n",
    "        has_prior = (df['prior_sale_price']>10000) & (df['years_since_last_sale']<=15) & (df['living_sqft']>400)\n",
    "        if has_prior.sum()>0:\n",
    "            # Calculate historical PPSF from prior sale\n",
    "            prior_ppsf = df.loc[has_prior,'prior_sale_price'] / (df.loc[has_prior,'living_sqft']+1)\n",
    "            # Appreciate the PPSF over time\n",
    "            yrs = df.loc[has_prior,'years_since_last_sale'].fillna(5)\n",
    "            appreciated_ppsf = prior_ppsf * (1.04 ** yrs)\n",
    "            # Apply to current sqft\n",
    "            value_ind[has_prior] = appreciated_ppsf * df.loc[has_prior,'living_sqft']\n",
    "            source[has_prior] = 'prior_ppsf'\n",
    "\n",
    "    # Priority 2: Assessed value (external data - NO LEAKAGE)\n",
    "    if 'assessed_total_value' in df.columns:\n",
    "        valid = filter_bad_assessed(df) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            vals = df.loc[valid,'assessed_total_value']\n",
    "            mult = pd.Series([1.15]*len(vals), index=vals.index)\n",
    "            mult[vals<200000], mult[vals>=500000] = 1.1, 1.2\n",
    "            value_ind[valid], source[valid] = vals*mult, 'assessed'\n",
    "\n",
    "    # Priority 3: Census median (external, neighborhood-level - NO LEAKAGE)\n",
    "    if 'median_home_value' in df.columns:\n",
    "        valid = (df['median_home_value']>10000) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            value_ind[valid], source[valid] = df.loc[valid,'median_home_value'], 'census'\n",
    "\n",
    "    # Priority 4: Geo cluster median price (if available from training)\n",
    "    if 'cluster_med_price' in df.columns:\n",
    "        valid = (df['cluster_med_price']>10000) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            value_ind[valid], source[valid] = df.loc[valid,'cluster_med_price'], 'cluster'\n",
    "\n",
    "    # Fallback: Global median\n",
    "    value_ind = value_ind.fillna(value_ind.median())\n",
    "    source[value_ind.isna()] = 'global_median'\n",
    "\n",
    "    # STORE value_indicator as a feature (for model to use as anchor)\n",
    "    df['value_indicator'] = value_ind\n",
    "    df['log_value_indicator'] = np.log1p(value_ind)  # Also create log transform\n",
    "    df['_value_source'] = source\n",
    "\n",
    "    # Segment based on value indicator\n",
    "    if train_q is None:\n",
    "        q20,q40,q60,q80,q95 = value_ind.quantile([0.20,0.40,0.60,0.80,0.95])\n",
    "        train_q = {'value_q20':q20,'value_q40':q40,'value_q60':q60,'value_q80':q80,'value_q95':q95}\n",
    "        print(f\"\\nValue quantiles: Q20:${q20:,.0f} Q40:${q40:,.0f} Q60:${q60:,.0f} Q80:${q80:,.0f} Q95:${q95:,.0f}\")\n",
    "    else:\n",
    "        q20,q40,q60,q80,q95 = train_q['value_q20'],train_q['value_q40'],train_q['value_q60'],train_q['value_q80'],train_q['value_q95']\n",
    "\n",
    "    tier = pd.Series(['mid']*len(df), index=df.index)\n",
    "    tier[value_ind<q20], tier[(value_ind>=q20)&(value_ind<q40)] = 'budget', 'economy'\n",
    "    tier[(value_ind>=q60)&(value_ind<q80)], tier[(value_ind>=q80)&(value_ind<q95)], tier[value_ind>=q95] = 'premium', 'luxury', 'ultra'\n",
    "\n",
    "    size = pd.Series(['small']*len(df), index=df.index)\n",
    "    if 'living_sqft' in df.columns:\n",
    "        sqft = df['living_sqft'].fillna(df['living_sqft'].median())\n",
    "        size[sqft>sqft.median()] = 'large'\n",
    "\n",
    "    return tier+'_'+size, train_q\n",
    "\n",
    "    # Segment\n",
    "    if train_q is None:\n",
    "        q20,q40,q60,q80,q95 = value_ind.quantile([0.20,0.40,0.60,0.80,0.95])\n",
    "        train_q = {'value_q20':q20,'value_q40':q40,'value_q60':q60,'value_q80':q80,'value_q95':q95}\n",
    "        print(f\"\\nValue quantiles: Q20:${q20:,.0f} Q40:${q40:,.0f} Q60:${q60:,.0f} Q80:${q80:,.0f} Q95:${q95:,.0f}\")\n",
    "    else:\n",
    "        q20,q40,q60,q80,q95 = train_q['value_q20'],train_q['value_q40'],train_q['value_q60'],train_q['value_q80'],train_q['value_q95']\n",
    "\n",
    "    tier = pd.Series(['mid']*len(df), index=df.index)\n",
    "    tier[value_ind<q20], tier[(value_ind>=q20)&(value_ind<q40)] = 'budget', 'economy'\n",
    "    tier[(value_ind>=q60)&(value_ind<q80)], tier[(value_ind>=q80)&(value_ind<q95)], tier[value_ind>=q95] = 'premium', 'luxury', 'ultra'\n",
    "\n",
    "    size = pd.Series(['small']*len(df), index=df.index)\n",
    "    if 'living_sqft' in df.columns:\n",
    "        sqft = df['living_sqft'].fillna(df['living_sqft'].median())\n",
    "        size[sqft>sqft.median()] = 'large'\n",
    "\n",
    "    df['_value_indicator'], df['_value_source'] = value_ind, source\n",
    "    return tier+'_'+size, train_q\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    if 'living_sqft' in df.columns:\n",
    "        if 'bedrooms' in df.columns: df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "        df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns: df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns: df['lot_acres'] = df['lot_sqft']/43560\n",
    "\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'], df['is_new'], df['age_squared'] = 2024-df['year_built'], ((2024-df['year_built'])<=5).astype('int8'), (2024-df['year_built'])**2\n",
    "\n",
    "    if 'garage_spaces' in df.columns:\n",
    "        df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        df['prior_appreciated'] = df['prior_sale_price']*(1.04**df['years_since_last_sale'].fillna(5))\n",
    "\n",
    "    if 'assessed_total_value' in df.columns:\n",
    "        valid = filter_bad_assessed(df)\n",
    "        if 'living_sqft' in df.columns:\n",
    "            df['assessed_per_sqft'] = 0\n",
    "            df.loc[valid,'assessed_per_sqft'] = df.loc[valid,'assessed_total_value']/(df.loc[valid,'living_sqft']+1)\n",
    "        if 'median_home_value' in df.columns:\n",
    "            reasonable = valid & (df['median_home_value']>10000)\n",
    "            df['assessed_to_census_ratio'] = 1.0\n",
    "            df.loc[reasonable,'assessed_to_census_ratio'] = df.loc[reasonable,'assessed_total_value']/(df.loc[reasonable,'median_home_value']+1)\n",
    "        if 'assessed_land_value' in df.columns:\n",
    "            reasonable = valid & (df['assessed_land_value']>0)\n",
    "            df['land_to_total_ratio'] = 0\n",
    "            df.loc[reasonable,'land_to_total_ratio'] = df.loc[reasonable,'assessed_land_value']/(df.loc[reasonable,'assessed_total_value']+1)\n",
    "\n",
    "    if INCLUDE_CENSUS and all(c in df.columns for c in ['median_household_income','pct_bachelors_degree']):\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'living_sqft' in df.columns:\n",
    "        df['prior_price_per_sqft'], df['sqft_per_prior_dollar'] = df['prior_sale_price']/(df['living_sqft']+1), df['living_sqft']/(df['prior_sale_price']+1)\n",
    "\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        if 'prior_sale_price' in df.columns:\n",
    "            df['expected_appreciation'] = df['prior_sale_price']*(1.04)**df['years_since_last_sale']\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df.loc[missing,'median_home_value'] if 'median_home_value' in df.columns and INCLUDE_CENSUS else df['prior_sale_price'].median()\n",
    "\n",
    "    if 'years_since_last_sale' in df.columns:\n",
    "        df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "\n",
    "    if with_price and y_col in df.columns and 'living_sqft' in df.columns:\n",
    "        df['price_per_sqft'], df['sqft_per_dollar'] = df[y_col]/(df['living_sqft']+1), df['living_sqft']/(df[y_col]+1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df, kmeans=None):\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans\n",
    "\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum()<N_CLUSTERS:\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans\n",
    "\n",
    "    df['geo_cluster'] = 0\n",
    "    if kmeans is None:\n",
    "        kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "        df.loc[valid,'geo_cluster'] = kmeans.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    else:\n",
    "        df.loc[valid,'geo_cluster'] = kmeans.predict(df.loc[valid,['latitude','longitude']])\n",
    "\n",
    "    return df, kmeans\n",
    "\n",
    "def add_cluster_feats(train, test, y_col):\n",
    "    if 'geo_cluster' not in train.columns or y_col not in train.columns:\n",
    "        for d in [train,test]:\n",
    "            d['cluster_avg_price'] = d['cluster_med_price'] = train[y_col].median() if y_col in train.columns else 0\n",
    "        return train, test\n",
    "\n",
    "    stats = train.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "    stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "    med = train[y_col].median()\n",
    "    train = train.merge(stats, on='geo_cluster', how='left').fillna({'cluster_avg_price':med,'cluster_med_price':med})\n",
    "    test = test.merge(stats, on='geo_cluster', how='left').fillna({'cluster_avg_price':med,'cluster_med_price':med})\n",
    "    return train, test\n",
    "\n",
    "def filter_outliers(df, name, y_col):\n",
    "    orig = len(df)\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)].drop(columns=['price_per_sqft','sqft_per_dollar'])\n",
    "\n",
    "    if 'lot_sqft' in df.columns: df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "    if 'year_built' in df.columns: df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "\n",
    "    if len(df)>=100:\n",
    "        try:\n",
    "            feats = [c for c in ['living_sqft','lot_sqft',y_col,'year_built'] if c in df.columns]\n",
    "            if len(feats)>=3:\n",
    "                X = df[feats].fillna(df[feats].median())\n",
    "                df = df[IsolationForest(contamination=.05, random_state=RAND_STATE, n_jobs=N_JOBS).fit_predict(X)==1]\n",
    "        except: pass\n",
    "\n",
    "    if (pct:=(orig-len(df))/orig*100)>0: print(f\"  {name}: {orig:,}→{len(df):,} ({pct:.1f}% filtered)\")\n",
    "    return df\n",
    "\n",
    "def train_model(X, y, q):\n",
    "    return XGBRegressor(objective='reg:quantileerror', quantile_alpha=q, n_estimators=N_EST, learning_rate=.1,\n",
    "                       max_depth=5, min_child_weight=3, subsample=.8, colsample_bytree=.8,\n",
    "                       random_state=RAND_STATE, n_jobs=N_JOBS, tree_method='hist').fit(X, y, verbose=False)\n",
    "\n",
    "def get_feat_importance(model, feats, top_n=20):\n",
    "    scores = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    imp = [(feats[int(k[1:])],v) for k,v in scores.items() if int(k[1:])<len(feats)]\n",
    "    imp.sort(key=lambda x: x[1], reverse=True)\n",
    "    total = sum(v for _,v in imp)\n",
    "    return pd.DataFrame([{'feature':f,'gain':g,'importance':g/total} for f,g in imp[:top_n]]) if total>0 else pd.DataFrame(columns=['feature','gain','importance'])\n",
    "\n",
    "def feature_importance(models, feats, metrics):\n",
    "    rows = []\n",
    "    for seg,mdl in models.items():\n",
    "        scores = mdl['q50'].get_booster().get_score(importance_type=\"gain\")\n",
    "        w = metrics[seg][\"n_test\"]\n",
    "        rows.extend([(feats[int(k[1:])],v,w) for k,v in scores.items() if int(k[1:])<len(feats)])\n",
    "\n",
    "    if not rows: return pd.DataFrame(columns=[\"feature\",\"total_gain\",\"importance\"])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"feature\",\"gain\",\"weight\"])\n",
    "    out = df.assign(wg=df[\"gain\"]*df[\"weight\"]).groupby(\"feature\",as_index=False).agg(\n",
    "        total_gain=(\"gain\",\"sum\"), weighted_gain=(\"wg\",\"sum\")).sort_values(\"weighted_gain\",ascending=False)\n",
    "    out[\"importance\"] = out[\"weighted_gain\"]/out[\"weighted_gain\"].sum()\n",
    "    return out[[\"feature\",\"total_gain\",\"importance\"]].head(100)\n",
    "\n",
    "def prepare_data(df, y_col, id_col, state_col, for_training=True):\n",
    "    print(f\"\\nPreparing data (value_indicator as anchor feature)...\")\n",
    "    if for_training: df = df[df[y_col]>=MIN_PRICE]\n",
    "\n",
    "    df, kmeans = geo_cluster(engineer(df, y_col))\n",
    "\n",
    "    feat_groups = []\n",
    "    if INCLUDE_MLS: feat_groups.extend([BASE_FEATS,ENG_FEATS,PRIOR_FEATS,CLUSTER_FEATS])\n",
    "    if INCLUDE_CENSUS: feat_groups.append(CENSUS_FEATS)\n",
    "    if INCLUDE_NEIGHBORHOOD: feat_groups.append(ELECT_FEATS)\n",
    "    if INCLUDE_IMAGE: feat_groups.append(IMG_FEATS)\n",
    "\n",
    "    all_feats = [f for g in feat_groups for f in g]\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "    feats.extend([vf for vf in ['prior_appreciated','assessed_per_sqft','assessed_to_census_ratio','land_to_total_ratio'] if vf in df.columns and vf not in feats])\n",
    "\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "\n",
    "    return (df.dropna(subset=[y_col]),feats,kmeans) if for_training else (df,feats,kmeans)\n",
    "\n",
    "def train_segments(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining on {len(df):,} properties (value_indicator as anchor feature)\")\n",
    "\n",
    "    df['seg'], train_q = assign_segments(df)\n",
    "\n",
    "    # Add value_indicator features to feature list (created by assign_segments)\n",
    "    if 'value_indicator' in df.columns and 'value_indicator' not in feats:\n",
    "        feats = feats + ['value_indicator']\n",
    "    if 'log_value_indicator' in df.columns and 'log_value_indicator' not in feats:\n",
    "        feats = feats + ['log_value_indicator']\n",
    "\n",
    "    if '_value_source' in df.columns:\n",
    "        print(f\"\\nValue sources: {dict(df['_value_source'].value_counts())}\")\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    print(f\"\\n{len(seg_cnts)} segments\")\n",
    "\n",
    "    for seg,cnt in seg_cnts.head(MAX_SEGMENTS).items():\n",
    "        if 'value_indicator' in df.columns and seg!='other':\n",
    "            seg_df = df[df['seg']==seg]\n",
    "            if len(seg_df)>0:\n",
    "                print(f\"  {seg}: {cnt:,} (${seg_df['value_indicator'].min():,.0f}-${seg_df['value_indicator'].max():,.0f}, med ${seg_df['value_indicator'].median():,.0f})\")\n",
    "        else:\n",
    "            print(f\"  {seg}: {cnt:,}\")\n",
    "\n",
    "    small = seg_cnts[seg_cnts<50].index\n",
    "    if len(small)>0:\n",
    "        df.loc[df['seg'].isin(small),'seg'] = 'other'\n",
    "        print(f\"\\nMerged {len(small)} small segments\")\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    if len(seg_cnts)>MAX_SEGMENTS:\n",
    "        keep = seg_cnts.head(MAX_SEGMENTS-1).index\n",
    "        df.loc[~df['seg'].isin(keep),'seg'] = 'other'\n",
    "        print(f\"Consolidated to {MAX_SEGMENTS} segments\")\n",
    "\n",
    "    df = df.drop(columns=[c for c in ['_value_indicator','_value_source'] if c in df.columns])\n",
    "\n",
    "    models, metrics, preds_list, seg_imps = {}, {}, [], {}\n",
    "\n",
    "    for seg in df['seg'].unique():\n",
    "        seg_df = filter_outliers(df[df['seg']==seg].copy(), seg, y_col)\n",
    "        if len(seg_df)<50: continue\n",
    "\n",
    "        train_idx = seg_df.sample(frac=1-TEST_SIZE, random_state=RAND_STATE).index\n",
    "        train_df, test_df = seg_df.loc[train_idx].copy(), seg_df.loc[seg_df.index.difference(train_idx)].copy()\n",
    "        train_df, test_df = add_cluster_feats(train_df, test_df, y_col)\n",
    "\n",
    "        X_tr, y_tr = train_df[feats].values, train_df[y_col].values\n",
    "        X_te, y_te = test_df[feats].values, test_df[y_col].values\n",
    "        ids, states = test_df[id_col].values, test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "\n",
    "        seg_models = {f\"q{int(q*100)}\":train_model(X_tr,y_tr,q) for q in QUANTILES}\n",
    "        seg_preds = [seg_models[f\"q{int(q*100)}\"].predict(X_te) for q in QUANTILES]\n",
    "\n",
    "        models[seg] = seg_models\n",
    "        seg_imps[seg] = get_feat_importance(seg_models['q50'], feats)\n",
    "\n",
    "        mae, mape = mean_absolute_error(y_te,seg_preds[1]), np.mean(np.abs((y_te-seg_preds[1])/y_te))*100\n",
    "        r2, cov = r2_score(y_te,seg_preds[1]), np.mean((y_te>=seg_preds[0])&(y_te<=seg_preds[2]))*100\n",
    "        metrics[seg] = {'n_train':len(X_tr),'n_test':len(X_te),'mae':mae,'mape':mape,'r2':r2,'cov':cov,\n",
    "                       'p_min':seg_df[y_col].min(),'p_max':seg_df[y_col].max(),'p_med':seg_df[y_col].median()}\n",
    "        print(f\"  {seg}: {len(test_df):,} test | MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.3f}\")\n",
    "\n",
    "        preds_list.append(pd.DataFrame({\n",
    "            'property_id':ids, 'state':states, 'actual':y_te, 'predicted':seg_preds[1],\n",
    "            'pred_lower':seg_preds[0], 'pred_upper':seg_preds[2], 'segment':seg\n",
    "        }))\n",
    "\n",
    "    return {'models':models, 'metrics':metrics, 'predictions':pd.concat(preds_list),\n",
    "            'feature_importance':feature_importance(models,feats,metrics), 'segment_importances':seg_imps,\n",
    "            'feature_names':feats, 'training_quantiles':train_q}  # feats now includes value_indicator\n",
    "\n",
    "def predict_new(pred_df, models, feats, y_col, id_col, state_col, kmeans, train_stats, train_q):\n",
    "    print(f\"\\n{'='*60}\\nPREDICTING {len(pred_df):,} NEW PROPERTIES\\n{'='*60}\")\n",
    "\n",
    "    pred_df = engineer(pred_df, y_col)\n",
    "    pred_df, _ = geo_cluster(pred_df, kmeans)\n",
    "\n",
    "    if 'geo_cluster' in pred_df.columns and train_stats is not None:\n",
    "        pred_df = pred_df.merge(train_stats, on='geo_cluster', how='left')\n",
    "        med = train_stats['cluster_avg_price'].median()\n",
    "        pred_df['cluster_avg_price'] = pred_df['cluster_avg_price'].fillna(med)\n",
    "        pred_df['cluster_med_price'] = pred_df['cluster_med_price'].fillna(med)\n",
    "\n",
    "    for feat in feats:\n",
    "        if feat not in pred_df.columns: pred_df[feat] = 0\n",
    "        else: pred_df[feat] = pred_df[feat].fillna(pred_df[feat].median() if pred_df[feat].notna().sum()>0 else 0)\n",
    "\n",
    "    pred_df['seg'], _ = assign_segments(pred_df, train_q)\n",
    "\n",
    "    if '_value_source' in pred_df.columns:\n",
    "        print(f\"Value sources: {dict(pred_df['_value_source'].value_counts())}\")\n",
    "\n",
    "    preds_list = []\n",
    "    for seg in pred_df['seg'].unique():\n",
    "        seg_df = pred_df[pred_df['seg']==seg].copy()\n",
    "\n",
    "        if seg not in models:\n",
    "            print(f\"  Warning: '{seg}' not in models, using fallback\")\n",
    "            avail = list(models.keys())\n",
    "            seg = next((s for s in avail if any(x in seg for x in ['ultra','luxury','premium','mid','economy','budget'] if x in s)), avail[0])\n",
    "\n",
    "        final_feats = list(feats)\n",
    "        for vf in ['value_indicator', 'log_value_indicator']:\n",
    "            if vf in pred_df.columns and vf not in final_feats:\n",
    "                final_feats.append(vf)\n",
    "\n",
    "        # Fill missing\n",
    "        for f in final_feats:\n",
    "            if f not in pred_df.columns: pred_df[f] = 0\n",
    "            else: pred_df[f] = pred_df[f].fillna(pred_df[f].median() if pred_df[f].notna().sum()>0 else 0)\n",
    "\n",
    "        X = seg_df[final_feats].values\n",
    "        ids, states = seg_df[id_col].values, seg_df[state_col].values if state_col and state_col in seg_df.columns else ['Unknown']*len(seg_df)\n",
    "\n",
    "        preds = [models[seg][f\"q{int(q*100)}\"].predict(X) for q in QUANTILES]\n",
    "        actual = seg_df[y_col].values if y_col in seg_df.columns else [np.nan]*len(seg_df)\n",
    "\n",
    "        value_ind = seg_df['_value_indicator'].values if '_value_indicator' in seg_df.columns else [np.nan]*len(seg_df)\n",
    "        value_src = seg_df['_value_source'].values if '_value_source' in seg_df.columns else ['unknown']*len(seg_df)\n",
    "\n",
    "        preds_list.append(pd.DataFrame({\n",
    "            'property_id':ids, 'state':states, 'value_indicator':value_ind, 'value_source':value_src,\n",
    "            'actual':actual, 'predicted':preds[1], 'pred_lower':preds[0], 'pred_upper':preds[2], 'segment':seg,\n",
    "            'error':[actual[i]-preds[1][i] if not np.isnan(actual[i]) else np.nan for i in range(len(actual))],\n",
    "            'pct_error':[100*(actual[i]-preds[1][i])/actual[i] if not np.isnan(actual[i]) and actual[i]!=0 else np.nan for i in range(len(actual))]\n",
    "        }))\n",
    "\n",
    "        print(f\"  {seg}: {len(seg_df):,} predicted\")\n",
    "\n",
    "    result = pd.concat(preds_list, ignore_index=True)\n",
    "    print(f\"\\n✓ Generated {len(result):,} predictions\")\n",
    "\n",
    "    pred_df = pred_df.drop(columns=[c for c in ['_value_indicator','_value_source'] if c in pred_df.columns])\n",
    "\n",
    "    valid = result['actual'].notna().sum()\n",
    "    if valid>0:\n",
    "        valid_df = result[result['actual'].notna()]\n",
    "        mae, mape = mean_absolute_error(valid_df['actual'],valid_df['predicted']), np.mean(np.abs((valid_df['actual']-valid_df['predicted'])/valid_df['actual']))*100\n",
    "        r2 = r2_score(valid_df['actual'],valid_df['predicted'])\n",
    "        print(f\"  Validation ({valid}): MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.4f}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def save_results(results, out_dir, new_preds=None):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi, seg_imps = results['predictions'], results['metrics'], results['feature_importance'], results['segment_importances']\n",
    "\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    # Summary\n",
    "    ws = wb.create_sheet(\"Summary\", 0)\n",
    "    ws['A1'].font, ws['A1'].value = Font(bold=True,size=14), 'IMPROVED SEGMENTED AVM'\n",
    "    ws['A2'].font, ws['A2'].value = Font(italic=True,size=10), 'Value indicator AS MODEL FEATURE (anchors predictions) + 6 tiers'\n",
    "\n",
    "    r2, mae, mape = r2_score(preds['actual'],preds['predicted']), mean_absolute_error(preds['actual'],preds['predicted']), np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "\n",
    "    data = [['Metric','Value'], ['Properties',len(preds)], ['Segments',len(metrics)], ['R²',f'{r2:.4f}'], ['MAE',f'${mae:,.0f}'], ['MAPE%',f'{mape:.2f}%']]\n",
    "    if new_preds is not None: data.append(['New Predictions',len(new_preds)])\n",
    "\n",
    "    for i,(k,v) in enumerate(data,5):\n",
    "        ws[f'A{i}'].font, ws[f'A{i}'].value, ws[f'B{i}'].value = Font(bold=True), k, v\n",
    "\n",
    "    # Segments\n",
    "    ws = wb.create_sheet(\"Segments\")\n",
    "    seg_df = pd.DataFrame([{**{'Segment':s},**m} for s,m in metrics.items()])\n",
    "    for i,h in enumerate(seg_df.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font, c.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(seg_df.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    # Feature Importance\n",
    "    for title,data,name in [('Global Feature Importance',fi,'Global_Feature_Importance')]+[(f'FI: {s}',si,f\"FI_{s}\"[:31]) for s,si in seg_imps.items()]:\n",
    "        ws = wb.create_sheet(name)\n",
    "        ws['A1'].value, ws['A1'].font = title, Font(bold=True,size=12)\n",
    "        for r_idx,row in enumerate(dataframe_to_rows(data,index=False,header=True),2):\n",
    "            for c_idx,value in enumerate(row,1):\n",
    "                cell = ws.cell(row=r_idx,column=c_idx,value=value)\n",
    "                if r_idx==2: cell.font, cell.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "\n",
    "    # Predictions\n",
    "    for sheet_name,data,color in [('Test_Predictions',preds,'366092'), ('New_Predictions',new_preds,'4472C4')]:\n",
    "        if data is None: continue\n",
    "        ws = wb.create_sheet(sheet_name)\n",
    "        for i,h in enumerate(data.columns,1):\n",
    "            c = ws.cell(1,i,h)\n",
    "            c.font, c.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color=color,end_color=color,fill_type='solid')\n",
    "        for i,row in enumerate(data.itertuples(index=False),2):\n",
    "            for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/improved_segmented_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "\n",
    "    for name,data in [('test_predictions',preds), ('segments',seg_df), ('importance',fi)]+[(f'importance_{s}',si) for s,si in seg_imps.items()]+([('new_predictions',new_preds)] if new_preds is not None else []):\n",
    "        data.to_csv(f\"{out_dir}/improved_{name}_{ts}.csv\", index=False)\n",
    "\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved with timestamp: {ts}\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60+\"\\nIMPROVED SEGMENTED AVM\\nValue Indicator AS MODEL FEATURE (Anchors Predictions)\\n\"+\"=\"*60)\n",
    "\n",
    "    df, y_col, id_col, state_col = load_data(TRAINING_INPUT_PATH)\n",
    "    df, feats, kmeans = prepare_data(df, y_col, id_col, state_col, for_training=True)\n",
    "    results = train_segments(df, feats, y_col, id_col, state_col)\n",
    "\n",
    "    train_stats = df.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index() if 'geo_cluster' in df.columns else None\n",
    "    if train_stats is not None: train_stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "\n",
    "    new_preds = None\n",
    "    if PREDICTION_INPUT_PATH:\n",
    "        pred_df, _, _, _ = load_data(PREDICTION_INPUT_PATH)\n",
    "        new_preds = predict_new(pred_df, results['models'], feats, y_col, id_col, state_col, kmeans, train_stats, results['training_quantiles'])\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR, new_preds)\n",
    "\n",
    "    preds = results['predictions']\n",
    "    r2, mae, mape = r2_score(preds['actual'],preds['predicted']), mean_absolute_error(preds['actual'],preds['predicted']), np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    print(f\"\\n{'='*60}\\n✓ COMPLETE in {time.time()-t0:.1f}s\\n  Test: {len(preds):,} | {preds['segment'].nunique()} segments\\n  R²:{r2:.4f} | MAE:${mae:,.0f} | MAPE:{mape:.2f}%\")\n",
    "    if new_preds is not None: print(f\"  New predictions: {len(new_preds):,}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__==\"__main__\": main()"
   ],
   "id": "588e148a420f25b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IMPROVED SEGMENTED AVM\n",
      "Value Indicator AS MODEL FEATURE (Anchors Predictions)\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 90.4MB | Price:sale_price ID:property_id\n",
      "\n",
      "Preparing data (value_indicator as anchor feature)...\n",
      "  ⚠️  Filtered 29,431 bad assessed values\n",
      "51/59 features available\n",
      "\n",
      "Training on 127,258 properties (value_indicator as anchor feature)\n",
      "\n",
      "Value quantiles: Q20:$225,450 Q40:$434,750 Q60:$434,750 Q80:$741,940 Q95:$1,445,367\n",
      "\n",
      "Value sources: {'census': np.int64(127258)}\n",
      "\n",
      "10 segments\n",
      "  premium_large: 27,925 ($434,750-$732,125, med $434,750)\n",
      "  premium_small: 26,120 ($434,750-$732,125, med $434,750)\n",
      "  budget_small: 19,274 ($12,000-$225,433, med $134,133)\n",
      "  economy_large: 13,496 ($225,450-$433,875, med $335,713)\n",
      "  luxury_large: 11,219 ($741,940-$1,426,334, med $926,975)\n",
      "  economy_small: 8,817 ($225,450-$433,875, med $320,400)\n",
      "  luxury_small: 7,745 ($741,940-$1,426,334, med $923,550)\n",
      "Consolidated to 7 segments\n",
      "  budget_small: 19,274→15,269 (20.8% filtered)\n",
      "  budget_small: 4,581 test | MAE:$1,030,426 | MAPE:45.06% | R²:0.331\n",
      "  premium_large: 27,925→22,872 (18.1% filtered)\n",
      "  premium_large: 6,862 test | MAE:$318,889 | MAPE:20.77% | R²:0.446\n",
      "  premium_small: 26,120→21,295 (18.5% filtered)\n",
      "  premium_small: 6,389 test | MAE:$352,094 | MAPE:20.49% | R²:0.332\n",
      "  other: 20,407→16,679 (18.3% filtered)\n",
      "  other: 5,004 test | MAE:$306,546 | MAPE:20.07% | R²:0.556\n",
      "  economy_small: 8,817→6,985 (20.8% filtered)\n",
      "  economy_small: 2,095 test | MAE:$277,879 | MAPE:17.69% | R²:0.120\n",
      "  economy_large: 13,496→10,785 (20.1% filtered)\n",
      "  economy_large: 3,236 test | MAE:$171,488 | MAPE:13.96% | R²:0.294\n",
      "  luxury_large: 11,219→9,120 (18.7% filtered)\n",
      "  luxury_large: 2,736 test | MAE:$335,211 | MAPE:21.61% | R²:0.475\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\n",
      "1 records | 0.0MB | Price:sale_price ID:property_id\n",
      "\n",
      "============================================================\n",
      "PREDICTING 1 NEW PROPERTIES\n",
      "============================================================\n",
      "  ⚠️  Filtered 1 bad assessed values\n",
      "  ⚠️  Filtered 1 bad assessed values\n",
      "Value sources: {'census': np.int64(1)}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feats_with_value' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[29]\u001B[39m\u001B[32m, line 544\u001B[39m\n\u001B[32m    541\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m new_preds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m: \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m  New predictions: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(new_preds)\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    542\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m*\u001B[32m60\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m544\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m==\u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m: main()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[29]\u001B[39m\u001B[32m, line 533\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    531\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m PREDICTION_INPUT_PATH:\n\u001B[32m    532\u001B[39m     pred_df, _, _, _ = load_data(PREDICTION_INPUT_PATH)\n\u001B[32m--> \u001B[39m\u001B[32m533\u001B[39m     new_preds = \u001B[43mpredict_new\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresults\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mmodels\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeats\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_col\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mid_col\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_col\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkmeans\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_stats\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresults\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mtraining_quantiles\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    535\u001B[39m os.makedirs(OUTPUT_DIR, exist_ok=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    536\u001B[39m save_results(results, OUTPUT_DIR, new_preds)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[29]\u001B[39m\u001B[32m, line 429\u001B[39m, in \u001B[36mpredict_new\u001B[39m\u001B[34m(pred_df, models, feats, y_col, id_col, state_col, kmeans, train_stats, train_q)\u001B[39m\n\u001B[32m    426\u001B[39m     avail = \u001B[38;5;28mlist\u001B[39m(models.keys())\n\u001B[32m    427\u001B[39m     seg = \u001B[38;5;28mnext\u001B[39m((s \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m avail \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(x \u001B[38;5;129;01min\u001B[39;00m seg \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m'\u001B[39m\u001B[33multra\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mluxury\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mpremium\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mmid\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33meconomy\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mbudget\u001B[39m\u001B[33m'\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m s)), avail[\u001B[32m0\u001B[39m])\n\u001B[32m--> \u001B[39m\u001B[32m429\u001B[39m X = seg_df[\u001B[43mfeats_with_value\u001B[49m].values\n\u001B[32m    430\u001B[39m ids, states = seg_df[id_col].values, seg_df[state_col].values \u001B[38;5;28;01mif\u001B[39;00m state_col \u001B[38;5;129;01mand\u001B[39;00m state_col \u001B[38;5;129;01min\u001B[39;00m seg_df.columns \u001B[38;5;28;01melse\u001B[39;00m [\u001B[33m'\u001B[39m\u001B[33mUnknown\u001B[39m\u001B[33m'\u001B[39m]*\u001B[38;5;28mlen\u001B[39m(seg_df)\n\u001B[32m    432\u001B[39m preds = [models[seg][\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mq\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mint\u001B[39m(q*\u001B[32m100\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m].predict(X) \u001B[38;5;28;01mfor\u001B[39;00m q \u001B[38;5;129;01min\u001B[39;00m QUANTILES]\n",
      "\u001B[31mNameError\u001B[39m: name 'feats_with_value' is not defined"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T06:10:39.587585Z",
     "start_time": "2025-12-24T06:10:26.306016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings, time, os\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Config\n",
    "INCLUDE_MLS, INCLUDE_CENSUS, INCLUDE_NEIGHBORHOOD, INCLUDE_IMAGE = True, True, True, False\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS, N_EST, MAX_SEGMENTS = 20000, 0.3, 42, -1, 8, 100, 7\n",
    "QUANTILES = [0.1, 0.5, 0.9]\n",
    "\n",
    "TRAINING_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "PREDICTION_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "BASE_FEATS = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"effective_year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\"fireplace_code\",\"latitude\",\"longitude\",\"geo_cluster\",\"value_indicator\"]\n",
    "ENG_FEATS = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\",\"log_value_indicator\"]\n",
    "PRIOR_FEATS = [\"prior_sale_price\",\"prior_price_per_sqft\",\"sqft_per_prior_dollar\",\"years_since_last_sale\",\"expected_appreciation\",\"has_prior_sale\",\"recently_sold\"]\n",
    "CLUSTER_FEATS = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "CENSUS_FEATS = [\"total_population_25plus\",\"male_bachelors_degree\",\"female_bachelors_degree\",\"pct_bachelors_degree\",\"median_earnings_total\",\"median_earnings_male\",\"median_earnings_female\",\"median_household_income\",\"median_home_value\",\"median_gross_rent\",\"owner_occupied_units\",\"renter_occupied_units\",\"pct_owner_occupied\",\"occupied_units\",\"vacant_units\",\"median_age\",\"civilian_employed\",\"civilian_unemployed\",\"unemployment_rate\",\"income_education_score\"]\n",
    "ELECT_FEATS = [\"votes_gop\",\"votes_dem\",\"total_votes\",\"per_gop\",\"per_dem\",\"per_point_diff\",\"dem_margin\",\"rep_margin\"]\n",
    "IMG_FEATS = [\"topic_1\",\"topic_2\",\"topic_3\",\"topic_4\",\"topic_5\",\"topic_6\",\"topic_7\",\"topic_8\",\"topic_9\",\"topic_10\",\"gran_c_in\",\"gran_c_ex\",\"gran_c\",\"high_c_in\",\"high_c_ex\",\"high_c\"]\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8' if set(df[c].dropna().unique()).issubset({0,1}) else 'int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = optimize_dtypes(pd.read_csv(path, low_memory=False))\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return df, y_col, id_col, state_col\n",
    "\n",
    "def filter_bad_assessed(df):\n",
    "    if 'assessed_total_value' not in df.columns:\n",
    "        return pd.Series([False]*len(df), index=df.index)\n",
    "    \n",
    "    valid = (df['assessed_total_value']>10000) & (df['assessed_total_value']<100000000)\n",
    "    \n",
    "    if 'living_sqft' in df.columns:\n",
    "        has_sqft = (df['living_sqft'].notna()) & (df['living_sqft']>100)\n",
    "        ppsf = df['assessed_total_value']/df['living_sqft']\n",
    "        valid &= ~(has_sqft & ((ppsf<20)|(ppsf>2000)))\n",
    "    \n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        has_prior = (df['prior_sale_price']>10000) & (df['years_since_last_sale']<=15)\n",
    "        valid &= ~(has_prior & ((df['assessed_total_value']/df['prior_sale_price'])<0.10))\n",
    "    \n",
    "    if 'median_home_value' in df.columns:\n",
    "        has_census = (df['median_home_value']>10000)\n",
    "        valid &= ~(has_census & ((df['assessed_total_value']/df['median_home_value'])<0.05))\n",
    "    \n",
    "    invalid_cnt = ((df['assessed_total_value'].notna()) & ~valid).sum()\n",
    "    if invalid_cnt>0:\n",
    "        print(f\"  ⚠️  Filtered {invalid_cnt:,} bad assessed values\")\n",
    "    \n",
    "    return valid\n",
    "\n",
    "def assign_segments(df, train_q=None):\n",
    "    value_ind = pd.Series([np.nan]*len(df), index=df.index)\n",
    "    source = pd.Series(['none']*len(df), index=df.index)\n",
    "    \n",
    "    # Priority 1: Prior sale PPSF × current sqft (property-specific, external - NO LEAKAGE)\n",
    "    if all(c in df.columns for c in ['prior_sale_price','years_since_last_sale','living_sqft']):\n",
    "        has_prior = (df['prior_sale_price']>10000) & (df['years_since_last_sale']<=15) & (df['living_sqft']>400)\n",
    "        if has_prior.sum()>0:\n",
    "            prior_ppsf = df.loc[has_prior,'prior_sale_price'] / (df.loc[has_prior,'living_sqft']+1)\n",
    "            yrs = df.loc[has_prior,'years_since_last_sale'].fillna(5)\n",
    "            appreciated_ppsf = prior_ppsf * (1.04 ** yrs)\n",
    "            value_ind[has_prior] = appreciated_ppsf * df.loc[has_prior,'living_sqft']\n",
    "            source[has_prior] = 'prior_ppsf'\n",
    "    \n",
    "    # Priority 2: Assessed value (external data - NO LEAKAGE)\n",
    "    if 'assessed_total_value' in df.columns:\n",
    "        valid = filter_bad_assessed(df) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            vals = df.loc[valid,'assessed_total_value']\n",
    "            mult = pd.Series([1.15]*len(vals), index=vals.index)\n",
    "            mult[vals<200000], mult[vals>=500000] = 1.1, 1.2\n",
    "            value_ind[valid], source[valid] = vals*mult, 'assessed'\n",
    "    \n",
    "    # Priority 3: Census median (external, neighborhood-level - NO LEAKAGE)\n",
    "    if 'median_home_value' in df.columns:\n",
    "        valid = (df['median_home_value']>10000) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            value_ind[valid], source[valid] = df.loc[valid,'median_home_value'], 'census'\n",
    "    \n",
    "    # Priority 4: Geo cluster median price (if available from training)\n",
    "    if 'cluster_med_price' in df.columns:\n",
    "        valid = (df['cluster_med_price']>10000) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            value_ind[valid], source[valid] = df.loc[valid,'cluster_med_price'], 'cluster'\n",
    "    \n",
    "    # Fallback: Global median\n",
    "    value_ind = value_ind.fillna(value_ind.median())\n",
    "    source[value_ind.isna()] = 'global_median'\n",
    "    \n",
    "    # STORE value_indicator as a feature (for model to use as anchor)\n",
    "    df['value_indicator'] = value_ind\n",
    "    df['log_value_indicator'] = np.log1p(value_ind)\n",
    "    df['_value_source'] = source\n",
    "    \n",
    "    # Segment based on value indicator\n",
    "    if train_q is None:\n",
    "        q20,q40,q60,q80,q95 = value_ind.quantile([0.20,0.40,0.60,0.80,0.95])\n",
    "        train_q = {'value_q20':q20,'value_q40':q40,'value_q60':q60,'value_q80':q80,'value_q95':q95}\n",
    "        print(f\"\\nValue quantiles: Q20:${q20:,.0f} Q40:${q40:,.0f} Q60:${q60:,.0f} Q80:${q80:,.0f} Q95:${q95:,.0f}\")\n",
    "    else:\n",
    "        q20,q40,q60,q80,q95 = train_q['value_q20'],train_q['value_q40'],train_q['value_q60'],train_q['value_q80'],train_q['value_q95']\n",
    "    \n",
    "    tier = pd.Series(['mid']*len(df), index=df.index)\n",
    "    tier[value_ind<q20], tier[(value_ind>=q20)&(value_ind<q40)] = 'budget', 'economy'\n",
    "    tier[(value_ind>=q60)&(value_ind<q80)], tier[(value_ind>=q80)&(value_ind<q95)], tier[value_ind>=q95] = 'premium', 'luxury', 'ultra'\n",
    "    \n",
    "    size = pd.Series(['small']*len(df), index=df.index)\n",
    "    if 'living_sqft' in df.columns:\n",
    "        sqft = df['living_sqft'].fillna(df['living_sqft'].median())\n",
    "        size[sqft>sqft.median()] = 'large'\n",
    "    \n",
    "    return tier+'_'+size, train_q\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    if 'living_sqft' in df.columns:\n",
    "        if 'bedrooms' in df.columns: df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "        df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "    \n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns: df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns: df['lot_acres'] = df['lot_sqft']/43560\n",
    "\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'], df['is_new'], df['age_squared'] = 2024-df['year_built'], ((2024-df['year_built'])<=5).astype('int8'), (2024-df['year_built'])**2\n",
    "\n",
    "    if 'garage_spaces' in df.columns:\n",
    "        df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        df['prior_appreciated'] = df['prior_sale_price']*(1.04**df['years_since_last_sale'].fillna(5))\n",
    "\n",
    "    if 'assessed_total_value' in df.columns:\n",
    "        valid = filter_bad_assessed(df)\n",
    "        if 'living_sqft' in df.columns:\n",
    "            df['assessed_per_sqft'] = 0\n",
    "            df.loc[valid,'assessed_per_sqft'] = df.loc[valid,'assessed_total_value']/(df.loc[valid,'living_sqft']+1)\n",
    "        if 'median_home_value' in df.columns:\n",
    "            reasonable = valid & (df['median_home_value']>10000)\n",
    "            df['assessed_to_census_ratio'] = 1.0\n",
    "            df.loc[reasonable,'assessed_to_census_ratio'] = df.loc[reasonable,'assessed_total_value']/(df.loc[reasonable,'median_home_value']+1)\n",
    "        if 'assessed_land_value' in df.columns:\n",
    "            reasonable = valid & (df['assessed_land_value']>0)\n",
    "            df['land_to_total_ratio'] = 0\n",
    "            df.loc[reasonable,'land_to_total_ratio'] = df.loc[reasonable,'assessed_land_value']/(df.loc[reasonable,'assessed_total_value']+1)\n",
    "\n",
    "    if INCLUDE_CENSUS and all(c in df.columns for c in ['median_household_income','pct_bachelors_degree']):\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'living_sqft' in df.columns:\n",
    "        df['prior_price_per_sqft'], df['sqft_per_prior_dollar'] = df['prior_sale_price']/(df['living_sqft']+1), df['living_sqft']/(df['prior_sale_price']+1)\n",
    "\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        if 'prior_sale_price' in df.columns:\n",
    "            df['expected_appreciation'] = df['prior_sale_price']*(1.04)**df['years_since_last_sale']\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df.loc[missing,'median_home_value'] if 'median_home_value' in df.columns and INCLUDE_CENSUS else df['prior_sale_price'].median()\n",
    "\n",
    "    if 'years_since_last_sale' in df.columns:\n",
    "        df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "\n",
    "    if with_price and y_col in df.columns and 'living_sqft' in df.columns:\n",
    "        df['price_per_sqft'], df['sqft_per_dollar'] = df[y_col]/(df['living_sqft']+1), df['living_sqft']/(df[y_col]+1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df, kmeans=None):\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans\n",
    "\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum()<N_CLUSTERS:\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans\n",
    "\n",
    "    df['geo_cluster'] = 0\n",
    "    if kmeans is None:\n",
    "        kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "        df.loc[valid,'geo_cluster'] = kmeans.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    else:\n",
    "        df.loc[valid,'geo_cluster'] = kmeans.predict(df.loc[valid,['latitude','longitude']])\n",
    "\n",
    "    return df, kmeans\n",
    "\n",
    "def add_cluster_feats(train, test, y_col):\n",
    "    if 'geo_cluster' not in train.columns or y_col not in train.columns:\n",
    "        for d in [train,test]:\n",
    "            d['cluster_avg_price'] = d['cluster_med_price'] = train[y_col].median() if y_col in train.columns else 0\n",
    "        return train, test\n",
    "\n",
    "    stats = train.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "    stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "    med = train[y_col].median()\n",
    "    train = train.merge(stats, on='geo_cluster', how='left').fillna({'cluster_avg_price':med,'cluster_med_price':med})\n",
    "    test = test.merge(stats, on='geo_cluster', how='left').fillna({'cluster_avg_price':med,'cluster_med_price':med})\n",
    "    return train, test\n",
    "\n",
    "def filter_outliers(df, name, y_col):\n",
    "    orig = len(df)\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)].drop(columns=['price_per_sqft','sqft_per_dollar'])\n",
    "\n",
    "    if 'lot_sqft' in df.columns: df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "    if 'year_built' in df.columns: df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "\n",
    "    if len(df)>=100:\n",
    "        try:\n",
    "            feats = [c for c in ['living_sqft','lot_sqft',y_col,'year_built'] if c in df.columns]\n",
    "            if len(feats)>=3:\n",
    "                X = df[feats].fillna(df[feats].median())\n",
    "                df = df[IsolationForest(contamination=.05, random_state=RAND_STATE, n_jobs=N_JOBS).fit_predict(X)==1]\n",
    "        except: pass\n",
    "\n",
    "    if (pct:=(orig-len(df))/orig*100)>0: print(f\"  {name}: {orig:,}→{len(df):,} ({pct:.1f}% filtered)\")\n",
    "    return df\n",
    "\n",
    "def train_model(X, y, q):\n",
    "    return XGBRegressor(objective='reg:quantileerror', quantile_alpha=q, n_estimators=N_EST, learning_rate=.1,\n",
    "                       max_depth=5, min_child_weight=3, subsample=.8, colsample_bytree=.8,\n",
    "                       random_state=RAND_STATE, n_jobs=N_JOBS, tree_method='hist').fit(X, y, verbose=False)\n",
    "\n",
    "def get_feat_importance(model, feats, top_n=20):\n",
    "    scores = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    imp = [(feats[int(k[1:])],v) for k,v in scores.items() if int(k[1:])<len(feats)]\n",
    "    imp.sort(key=lambda x: x[1], reverse=True)\n",
    "    total = sum(v for _,v in imp)\n",
    "    return pd.DataFrame([{'feature':f,'gain':g,'importance':g/total} for f,g in imp[:top_n]]) if total>0 else pd.DataFrame(columns=['feature','gain','importance'])\n",
    "\n",
    "def feature_importance(models, feats, metrics):\n",
    "    rows = []\n",
    "    for seg,mdl in models.items():\n",
    "        scores = mdl['q50'].get_booster().get_score(importance_type=\"gain\")\n",
    "        w = metrics[seg][\"n_test\"]\n",
    "        rows.extend([(feats[int(k[1:])],v,w) for k,v in scores.items() if int(k[1:])<len(feats)])\n",
    "\n",
    "    if not rows: return pd.DataFrame(columns=[\"feature\",\"total_gain\",\"importance\"])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"feature\",\"gain\",\"weight\"])\n",
    "    out = df.assign(wg=df[\"gain\"]*df[\"weight\"]).groupby(\"feature\",as_index=False).agg(\n",
    "        total_gain=(\"gain\",\"sum\"), weighted_gain=(\"wg\",\"sum\")).sort_values(\"weighted_gain\",ascending=False)\n",
    "    out[\"importance\"] = out[\"weighted_gain\"]/out[\"weighted_gain\"].sum()\n",
    "    return out[[\"feature\",\"total_gain\",\"importance\"]].head(100)\n",
    "\n",
    "def prepare_data(df, y_col, id_col, state_col, for_training=True):\n",
    "    print(f\"\\nPreparing data (value_indicator as anchor feature)...\")\n",
    "    if for_training: df = df[df[y_col]>=MIN_PRICE]\n",
    "\n",
    "    df, kmeans = geo_cluster(engineer(df, y_col))\n",
    "\n",
    "    feat_groups = []\n",
    "    if INCLUDE_MLS: feat_groups.extend([BASE_FEATS,ENG_FEATS,PRIOR_FEATS,CLUSTER_FEATS])\n",
    "    if INCLUDE_CENSUS: feat_groups.append(CENSUS_FEATS)\n",
    "    if INCLUDE_NEIGHBORHOOD: feat_groups.append(ELECT_FEATS)\n",
    "    if INCLUDE_IMAGE: feat_groups.append(IMG_FEATS)\n",
    "\n",
    "    all_feats = [f for g in feat_groups for f in g]\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "    feats.extend([vf for vf in ['prior_appreciated','assessed_per_sqft','assessed_to_census_ratio','land_to_total_ratio'] if vf in df.columns and vf not in feats])\n",
    "\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "\n",
    "    return (df.dropna(subset=[y_col]),feats,kmeans) if for_training else (df,feats,kmeans)\n",
    "\n",
    "def train_segments(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining on {len(df):,} properties (value_indicator as anchor feature)\")\n",
    "\n",
    "    df['seg'], train_q = assign_segments(df)\n",
    "\n",
    "    # Add value_indicator features to feature list (created by assign_segments)\n",
    "    if 'value_indicator' in df.columns and 'value_indicator' not in feats:\n",
    "        feats = feats + ['value_indicator']\n",
    "    if 'log_value_indicator' in df.columns and 'log_value_indicator' not in feats:\n",
    "        feats = feats + ['log_value_indicator']\n",
    "\n",
    "    if '_value_source' in df.columns:\n",
    "        print(f\"\\nValue sources: {dict(df['_value_source'].value_counts())}\")\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    print(f\"\\n{len(seg_cnts)} segments\")\n",
    "\n",
    "    for seg,cnt in seg_cnts.head(MAX_SEGMENTS).items():\n",
    "        if 'value_indicator' in df.columns and seg!='other':\n",
    "            seg_df = df[df['seg']==seg]\n",
    "            if len(seg_df)>0:\n",
    "                print(f\"  {seg}: {cnt:,} (${seg_df['value_indicator'].min():,.0f}-${seg_df['value_indicator'].max():,.0f}, med ${seg_df['value_indicator'].median():,.0f})\")\n",
    "        else:\n",
    "            print(f\"  {seg}: {cnt:,}\")\n",
    "\n",
    "    small = seg_cnts[seg_cnts<50].index\n",
    "    if len(small)>0:\n",
    "        df.loc[df['seg'].isin(small),'seg'] = 'other'\n",
    "        print(f\"Consolidated to {MAX_SEGMENTS} segments\")\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    if len(seg_cnts)>MAX_SEGMENTS:\n",
    "        keep = seg_cnts.head(MAX_SEGMENTS-1).index\n",
    "        df.loc[~df['seg'].isin(keep),'seg'] = 'other'\n",
    "        print(f\"Consolidated to {MAX_SEGMENTS} segments\")\n",
    "\n",
    "    if '_value_source' in df.columns:\n",
    "        df = df.drop(columns=['_value_source'])\n",
    "\n",
    "    models, metrics, preds_list, seg_imps = {}, {}, [], {}\n",
    "\n",
    "    for seg in df['seg'].unique():\n",
    "        seg_df = filter_outliers(df[df['seg']==seg].copy(), seg, y_col)\n",
    "        if len(seg_df)<50: continue\n",
    "\n",
    "        train_idx = seg_df.sample(frac=1-TEST_SIZE, random_state=RAND_STATE).index\n",
    "        train_df, test_df = seg_df.loc[train_idx].copy(), seg_df.loc[seg_df.index.difference(train_idx)].copy()\n",
    "        train_df, test_df = add_cluster_feats(train_df, test_df, y_col)\n",
    "\n",
    "        X_tr, y_tr = train_df[feats].values, train_df[y_col].values\n",
    "        X_te, y_te = test_df[feats].values, test_df[y_col].values\n",
    "        ids = test_df[id_col].values\n",
    "        states = test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "\n",
    "        seg_models = {f\"q{int(q*100)}\":train_model(X_tr,y_tr,q) for q in QUANTILES}\n",
    "        seg_preds = [seg_models[f\"q{int(q*100)}\"].predict(X_te) for q in QUANTILES]\n",
    "\n",
    "        models[seg] = seg_models\n",
    "        seg_imps[seg] = get_feat_importance(seg_models['q50'], feats)\n",
    "\n",
    "        mae, mape = mean_absolute_error(y_te,seg_preds[1]), np.mean(np.abs((y_te-seg_preds[1])/y_te))*100\n",
    "        r2, cov = r2_score(y_te,seg_preds[1]), np.mean((y_te>=seg_preds[0])&(y_te<=seg_preds[2]))*100\n",
    "        metrics[seg] = {'n_train':len(X_tr),'n_test':len(X_te),'mae':mae,'mape':mape,'r2':r2,'cov':cov,\n",
    "                       'p_min':seg_df[y_col].min(),'p_max':seg_df[y_col].max(),'p_med':seg_df[y_col].median()}\n",
    "        print(f\"  {seg}: {len(test_df):,} test | MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.3f}\")\n",
    "\n",
    "        preds_list.append(pd.DataFrame({\n",
    "            'property_id':ids, 'state':states, 'actual':y_te, 'predicted':seg_preds[1],\n",
    "            'pred_lower':seg_preds[0], 'pred_upper':seg_preds[2], 'segment':seg\n",
    "        }))\n",
    "\n",
    "    return {'models':models, 'metrics':metrics, 'predictions':pd.concat(preds_list),\n",
    "            'feature_importance':feature_importance(models,feats,metrics), 'segment_importances':seg_imps,\n",
    "            'feature_names':feats, 'training_quantiles':train_q}\n",
    "\n",
    "def predict_new(pred_df, models, feats, y_col, id_col, state_col, kmeans, train_stats, train_q):\n",
    "    print(f\"\\n{'='*60}\\nPREDICTING {len(pred_df):,} NEW PROPERTIES\\n{'='*60}\")\n",
    "\n",
    "    pred_df = engineer(pred_df, y_col)\n",
    "    pred_df, _ = geo_cluster(pred_df, kmeans)\n",
    "\n",
    "    if 'geo_cluster' in pred_df.columns and train_stats is not None:\n",
    "        pred_df = pred_df.merge(train_stats, on='geo_cluster', how='left')\n",
    "        med = train_stats['cluster_avg_price'].median()\n",
    "        pred_df['cluster_avg_price'], pred_df['cluster_med_price'] = pred_df['cluster_avg_price'].fillna(med), pred_df['cluster_med_price'].fillna(med)\n",
    "\n",
    "    pred_df['seg'], _ = assign_segments(pred_df, train_q)\n",
    "\n",
    "    # Build complete feature list including value_indicator\n",
    "    use_feats = list(feats)\n",
    "    for vf in ['value_indicator', 'log_value_indicator']:\n",
    "        if vf in pred_df.columns and vf not in use_feats:\n",
    "            use_feats.append(vf)\n",
    "\n",
    "    for f in use_feats:\n",
    "        if f not in pred_df.columns: pred_df[f] = 0\n",
    "        else: pred_df[f] = pred_df[f].fillna(pred_df[f].median() if pred_df[f].notna().sum()>0 else 0)\n",
    "\n",
    "    if '_value_source' in pred_df.columns:\n",
    "        print(f\"Value sources: {dict(pred_df['_value_source'].value_counts())}\")\n",
    "\n",
    "    preds_list = []\n",
    "    for seg in pred_df['seg'].unique():\n",
    "        seg_df = pred_df[pred_df['seg']==seg].copy()\n",
    "\n",
    "        if seg not in models:\n",
    "            print(f\"  Warning: '{seg}' not in models, using fallback\")\n",
    "            avail = list(models.keys())\n",
    "            seg = next((s for s in avail if any(x in seg for x in ['ultra','luxury','premium','mid','economy','budget'] if x in s)), avail[0])\n",
    "\n",
    "        X = seg_df[use_feats].values\n",
    "        ids = seg_df[id_col].values\n",
    "        states = seg_df[state_col].values if state_col and state_col in seg_df.columns else ['Unknown']*len(seg_df)\n",
    "        actual = seg_df[y_col].values if y_col in seg_df.columns else [np.nan]*len(seg_df)\n",
    "        value_ind = seg_df['value_indicator'].values if 'value_indicator' in seg_df.columns else [np.nan]*len(seg_df)\n",
    "        value_src = seg_df['_value_source'].values if '_value_source' in seg_df.columns else ['unknown']*len(seg_df)\n",
    "\n",
    "        preds = [models[seg][f\"q{int(q*100)}\"].predict(X) for q in QUANTILES]\n",
    "\n",
    "        preds_list.append(pd.DataFrame({\n",
    "            'property_id':ids, 'state':states, 'value_indicator':value_ind, 'value_source':value_src,\n",
    "            'actual':actual, 'predicted':preds[1], 'pred_lower':preds[0], 'pred_upper':preds[2], 'segment':seg,\n",
    "            'error':[actual[i]-preds[1][i] if not np.isnan(actual[i]) else np.nan for i in range(len(actual))],\n",
    "            'pct_error':[100*(actual[i]-preds[1][i])/actual[i] if not np.isnan(actual[i]) and actual[i]!=0 else np.nan for i in range(len(actual))]\n",
    "        }))\n",
    "\n",
    "        print(f\"  {seg}: {len(seg_df):,} predicted\")\n",
    "\n",
    "    result = pd.concat(preds_list, ignore_index=True)\n",
    "    print(f\"\\n✓ Generated {len(result):,} predictions\")\n",
    "\n",
    "    if '_value_source' in pred_df.columns: pred_df = pred_df.drop(columns=['_value_source'])\n",
    "\n",
    "    valid = result['actual'].notna().sum()\n",
    "    if valid>0:\n",
    "        valid_df = result[result['actual'].notna()]\n",
    "        mae, mape, r2 = mean_absolute_error(valid_df['actual'],valid_df['predicted']), np.mean(np.abs((valid_df['actual']-valid_df['predicted'])/valid_df['actual']))*100, r2_score(valid_df['actual'],valid_df['predicted'])\n",
    "        print(f\"  Validation ({valid}): MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.4f}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def save_results(results, out_dir, new_preds=None):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi, seg_imps = results['predictions'], results['metrics'], results['feature_importance'], results['segment_importances']\n",
    "\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    ws = wb.create_sheet(\"Summary\", 0)\n",
    "    ws['A1'].font, ws['A1'].value = Font(bold=True,size=14), 'IMPROVED SEGMENTED AVM'\n",
    "    ws['A2'].font, ws['A2'].value = Font(italic=True,size=10), 'Value indicator AS MODEL FEATURE (anchors predictions)'\n",
    "\n",
    "    r2, mae, mape = r2_score(preds['actual'],preds['predicted']), mean_absolute_error(preds['actual'],preds['predicted']), np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "\n",
    "    data = [['Metric','Value'], ['Properties',len(preds)], ['Segments',len(metrics)], ['R²',f'{r2:.4f}'], ['MAE',f'${mae:,.0f}'], ['MAPE%',f'{mape:.2f}%']]\n",
    "    if new_preds is not None: data.append(['New Predictions',len(new_preds)])\n",
    "\n",
    "    for i,(k,v) in enumerate(data,5):\n",
    "        ws[f'A{i}'].font, ws[f'A{i}'].value, ws[f'B{i}'].value = Font(bold=True), k, v\n",
    "\n",
    "    ws = wb.create_sheet(\"Segments\")\n",
    "    seg_df = pd.DataFrame([{**{'Segment':s},**m} for s,m in metrics.items()])\n",
    "    for i,h in enumerate(seg_df.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font, c.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(seg_df.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    for title,data,name in [('Global Feature Importance',fi,'Global_Feature_Importance')]+[(f'FI: {s}',si,f\"FI_{s}\"[:31]) for s,si in seg_imps.items()]:\n",
    "        ws = wb.create_sheet(name)\n",
    "        ws['A1'].value, ws['A1'].font = title, Font(bold=True,size=12)\n",
    "        for r_idx,row in enumerate(dataframe_to_rows(data,index=False,header=True),2):\n",
    "            for c_idx,value in enumerate(row,1):\n",
    "                cell = ws.cell(row=r_idx,column=c_idx,value=value)\n",
    "                if r_idx==2: cell.font, cell.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "\n",
    "    for sheet_name,data,color in [('Test_Predictions',preds,'366092'), ('New_Predictions',new_preds,'4472C4')]:\n",
    "        if data is None: continue\n",
    "        ws = wb.create_sheet(sheet_name)\n",
    "        for i,h in enumerate(data.columns,1):\n",
    "            c = ws.cell(1,i,h)\n",
    "            c.font, c.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color=color,end_color=color,fill_type='solid')\n",
    "        for i,row in enumerate(data.itertuples(index=False),2):\n",
    "            for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/improved_segmented_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "\n",
    "    for name,data in [('test_predictions',preds), ('segments',seg_df), ('importance',fi)]+[(f'importance_{s}',si) for s,si in seg_imps.items()]+([('new_predictions',new_preds)] if new_preds is not None else []):\n",
    "        data.to_csv(f\"{out_dir}/improved_{name}_{ts}.csv\", index=False)\n",
    "\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved with timestamp: {ts}\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60+\"\\nIMPROVED SEGMENTED AVM\\nValue Indicator AS MODEL FEATURE (Anchors Predictions)\\n\"+\"=\"*60)\n",
    "\n",
    "    df, y_col, id_col, state_col = load_data(TRAINING_INPUT_PATH)\n",
    "    df, feats, kmeans = prepare_data(df, y_col, id_col, state_col, for_training=True)\n",
    "    results = train_segments(df, feats, y_col, id_col, state_col)\n",
    "\n",
    "    train_stats = df.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index() if 'geo_cluster' in df.columns else None\n",
    "    if train_stats is not None: train_stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "\n",
    "    new_preds = None\n",
    "    if PREDICTION_INPUT_PATH:\n",
    "        pred_df, _, _, _ = load_data(PREDICTION_INPUT_PATH)\n",
    "        new_preds = predict_new(pred_df, results['models'], results['feature_names'], y_col, id_col, state_col, kmeans, train_stats, results['training_quantiles'])\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR, new_preds)\n",
    "\n",
    "    preds = results['predictions']\n",
    "    r2, mae, mape = r2_score(preds['actual'],preds['predicted']), mean_absolute_error(preds['actual'],preds['predicted']), np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    print(f\"\\n{'='*60}\\n✓ COMPLETE in {time.time()-t0:.1f}s\\n  Test: {len(preds):,} | {preds['segment'].nunique()} segments\\n  R²:{r2:.4f} | MAE:${mae:,.0f} | MAPE:{mape:.2f}%\")\n",
    "    if new_preds is not None: print(f\"  New predictions: {len(new_preds):,}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__==\"__main__\": main()"
   ],
   "id": "12484b93e52c2417",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IMPROVED SEGMENTED AVM\n",
      "Value Indicator AS MODEL FEATURE (Anchors Predictions)\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 90.4MB | Price:sale_price ID:property_id\n",
      "\n",
      "Preparing data (value_indicator as anchor feature)...\n",
      "  ⚠️  Filtered 29,431 bad assessed values\n",
      "51/59 features available\n",
      "\n",
      "Training on 127,258 properties (value_indicator as anchor feature)\n",
      "\n",
      "Value quantiles: Q20:$225,450 Q40:$434,750 Q60:$434,750 Q80:$741,940 Q95:$1,445,367\n",
      "\n",
      "Value sources: {'census': np.int64(127258)}\n",
      "\n",
      "10 segments\n",
      "  premium_large: 27,925 ($434,750-$732,125, med $434,750)\n",
      "  premium_small: 26,120 ($434,750-$732,125, med $434,750)\n",
      "  budget_small: 19,274 ($12,000-$225,433, med $134,133)\n",
      "  economy_large: 13,496 ($225,450-$433,875, med $335,713)\n",
      "  luxury_large: 11,219 ($741,940-$1,426,334, med $926,975)\n",
      "  economy_small: 8,817 ($225,450-$433,875, med $320,400)\n",
      "  luxury_small: 7,745 ($741,940-$1,426,334, med $923,550)\n",
      "Consolidated to 7 segments\n",
      "  budget_small: 19,274→15,269 (20.8% filtered)\n",
      "  budget_small: 4,581 test | MAE:$1,030,426 | MAPE:45.06% | R²:0.331\n",
      "  premium_large: 27,925→22,872 (18.1% filtered)\n",
      "  premium_large: 6,862 test | MAE:$318,889 | MAPE:20.77% | R²:0.446\n",
      "  premium_small: 26,120→21,295 (18.5% filtered)\n",
      "  premium_small: 6,389 test | MAE:$352,094 | MAPE:20.49% | R²:0.332\n",
      "  other: 20,407→16,679 (18.3% filtered)\n",
      "  other: 5,004 test | MAE:$306,546 | MAPE:20.07% | R²:0.556\n",
      "  economy_small: 8,817→6,985 (20.8% filtered)\n",
      "  economy_small: 2,095 test | MAE:$277,879 | MAPE:17.69% | R²:0.120\n",
      "  economy_large: 13,496→10,785 (20.1% filtered)\n",
      "  economy_large: 3,236 test | MAE:$171,488 | MAPE:13.96% | R²:0.294\n",
      "  luxury_large: 11,219→9,120 (18.7% filtered)\n",
      "  luxury_large: 2,736 test | MAE:$335,211 | MAPE:21.61% | R²:0.475\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\n",
      "1 records | 0.0MB | Price:sale_price ID:property_id\n",
      "\n",
      "============================================================\n",
      "PREDICTING 1 NEW PROPERTIES\n",
      "============================================================\n",
      "  ⚠️  Filtered 1 bad assessed values\n",
      "  ⚠️  Filtered 1 bad assessed values\n",
      "Value sources: {'census': np.int64(1)}\n",
      "  economy_small: 1 predicted\n",
      "\n",
      "✓ Generated 1 predictions\n",
      "  Validation (1): MAE:$423,214 | MAPE:80.46% | R²:nan\n",
      "\n",
      "Saving results...\n",
      "✓ Excel: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/improved_segmented_20251224_011038.xlsx\n",
      "✓ CSVs saved with timestamp: 20251224_011038\n",
      "\n",
      "============================================================\n",
      "✓ COMPLETE in 10.5s\n",
      "  Test: 30,903 | 7 segments\n",
      "  R²:0.4204 | MAE:$412,462 | MAPE:23.35%\n",
      "  New predictions: 1\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T06:14:21.625241Z",
     "start_time": "2025-12-24T06:14:11.805574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings, time, os\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Config\n",
    "INCLUDE_MLS, INCLUDE_CENSUS, INCLUDE_NEIGHBORHOOD, INCLUDE_IMAGE = True, True, True, False\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS, N_EST, MAX_SEGMENTS = 20000, 0.3, 42, -1, 8, 100, 7\n",
    "QUANTILES = [0.1, 0.5, 0.9]\n",
    "\n",
    "TRAINING_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "PREDICTION_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "BASE_FEATS = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"effective_year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\"fireplace_code\",\"latitude\",\"longitude\",\"geo_cluster\",\"value_indicator\"]\n",
    "ENG_FEATS = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\",\"log_value_indicator\"]\n",
    "PRIOR_FEATS = [\"prior_sale_price\",\"prior_price_per_sqft\",\"sqft_per_prior_dollar\",\"years_since_last_sale\",\"expected_appreciation\",\"has_prior_sale\",\"recently_sold\"]\n",
    "CLUSTER_FEATS = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "CENSUS_FEATS = [\"total_population_25plus\",\"male_bachelors_degree\",\"female_bachelors_degree\",\"pct_bachelors_degree\",\"median_earnings_total\",\"median_earnings_male\",\"median_earnings_female\",\"median_household_income\",\"median_home_value\",\"median_gross_rent\",\"owner_occupied_units\",\"renter_occupied_units\",\"pct_owner_occupied\",\"occupied_units\",\"vacant_units\",\"median_age\",\"civilian_employed\",\"civilian_unemployed\",\"unemployment_rate\",\"income_education_score\"]\n",
    "ELECT_FEATS = [\"votes_gop\",\"votes_dem\",\"total_votes\",\"per_gop\",\"per_dem\",\"per_point_diff\",\"dem_margin\",\"rep_margin\"]\n",
    "IMG_FEATS = [\"topic_1\",\"topic_2\",\"topic_3\",\"topic_4\",\"topic_5\",\"topic_6\",\"topic_7\",\"topic_8\",\"topic_9\",\"topic_10\",\"gran_c_in\",\"gran_c_ex\",\"gran_c\",\"high_c_in\",\"high_c_ex\",\"high_c\"]\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8' if set(df[c].dropna().unique()).issubset({0,1}) else 'int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = optimize_dtypes(pd.read_csv(path, low_memory=False))\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return df, y_col, id_col, state_col\n",
    "\n",
    "def filter_bad_assessed(df):\n",
    "    if 'assessed_total_value' not in df.columns:\n",
    "        return pd.Series([False]*len(df), index=df.index)\n",
    "\n",
    "    valid = (df['assessed_total_value']>10000) & (df['assessed_total_value']<100000000)\n",
    "\n",
    "    if 'living_sqft' in df.columns:\n",
    "        has_sqft = (df['living_sqft'].notna()) & (df['living_sqft']>100)\n",
    "        ppsf = df['assessed_total_value']/df['living_sqft']\n",
    "        valid &= ~(has_sqft & ((ppsf<20)|(ppsf>2000)))\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        has_prior = (df['prior_sale_price']>10000) & (df['years_since_last_sale']<=15)\n",
    "        valid &= ~(has_prior & ((df['assessed_total_value']/df['prior_sale_price'])<0.10))\n",
    "\n",
    "    if 'median_home_value' in df.columns:\n",
    "        has_census = (df['median_home_value']>10000)\n",
    "        valid &= ~(has_census & ((df['assessed_total_value']/df['median_home_value'])<0.05))\n",
    "\n",
    "    invalid_cnt = ((df['assessed_total_value'].notna()) & ~valid).sum()\n",
    "    if invalid_cnt>0:\n",
    "        print(f\"  ⚠️  Filtered {invalid_cnt:,} bad assessed values\")\n",
    "\n",
    "    return valid\n",
    "\n",
    "def assign_segments(df, train_q=None):\n",
    "    value_ind = pd.Series([np.nan]*len(df), index=df.index)\n",
    "    source = pd.Series(['none']*len(df), index=df.index)\n",
    "\n",
    "    # Priority 1: Prior sale PPSF × current sqft (property-specific, external - NO LEAKAGE)\n",
    "    if all(c in df.columns for c in ['prior_sale_price','years_since_last_sale','living_sqft']):\n",
    "        has_prior = (df['prior_sale_price']>10000) & (df['years_since_last_sale']<=15) & (df['living_sqft']>400)\n",
    "        if has_prior.sum()>0:\n",
    "            prior_ppsf = df.loc[has_prior,'prior_sale_price'] / (df.loc[has_prior,'living_sqft']+1)\n",
    "            yrs = df.loc[has_prior,'years_since_last_sale'].fillna(5)\n",
    "            appreciated_ppsf = prior_ppsf * (1.04 ** yrs)\n",
    "            value_ind[has_prior] = appreciated_ppsf * df.loc[has_prior,'living_sqft']\n",
    "            source[has_prior] = 'prior_ppsf'\n",
    "\n",
    "    # Priority 2: Assessed value (external data - NO LEAKAGE)\n",
    "    if 'assessed_total_value' in df.columns:\n",
    "        valid = filter_bad_assessed(df) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            vals = df.loc[valid,'assessed_total_value']\n",
    "            mult = pd.Series([1.15]*len(vals), index=vals.index)\n",
    "            mult[vals<200000], mult[vals>=500000] = 1.1, 1.2\n",
    "            value_ind[valid], source[valid] = vals*mult, 'assessed'\n",
    "\n",
    "    # Priority 3: Census median (external, neighborhood-level - NO LEAKAGE)\n",
    "    if 'median_home_value' in df.columns:\n",
    "        valid = (df['median_home_value']>10000) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            value_ind[valid], source[valid] = df.loc[valid,'median_home_value'], 'census'\n",
    "\n",
    "    # Priority 4: Geo cluster median price (if available from training)\n",
    "    if 'cluster_med_price' in df.columns:\n",
    "        valid = (df['cluster_med_price']>10000) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            value_ind[valid], source[valid] = df.loc[valid,'cluster_med_price'], 'cluster'\n",
    "\n",
    "    # Fallback: Global median\n",
    "    value_ind = value_ind.fillna(value_ind.median())\n",
    "    source[value_ind.isna()] = 'global_median'\n",
    "\n",
    "    # STORE value_indicator as a feature (for model to use as anchor)\n",
    "    df['value_indicator'] = value_ind\n",
    "    df['log_value_indicator'] = np.log1p(value_ind)\n",
    "    df['_value_source'] = source\n",
    "\n",
    "    # Segment based on value indicator\n",
    "    if train_q is None:\n",
    "        q20,q40,q60,q80,q95 = value_ind.quantile([0.20,0.40,0.60,0.80,0.95])\n",
    "        train_q = {'value_q20':q20,'value_q40':q40,'value_q60':q60,'value_q80':q80,'value_q95':q95}\n",
    "        print(f\"\\nValue quantiles: Q20:${q20:,.0f} Q40:${q40:,.0f} Q60:${q60:,.0f} Q80:${q80:,.0f} Q95:${q95:,.0f}\")\n",
    "    else:\n",
    "        q20,q40,q60,q80,q95 = train_q['value_q20'],train_q['value_q40'],train_q['value_q60'],train_q['value_q80'],train_q['value_q95']\n",
    "\n",
    "    tier = pd.Series(['mid']*len(df), index=df.index)\n",
    "    tier[value_ind<q20], tier[(value_ind>=q20)&(value_ind<q40)] = 'budget', 'economy'\n",
    "    tier[(value_ind>=q60)&(value_ind<q80)], tier[(value_ind>=q80)&(value_ind<q95)], tier[value_ind>=q95] = 'premium', 'luxury', 'ultra'\n",
    "\n",
    "    size = pd.Series(['small']*len(df), index=df.index)\n",
    "    if 'living_sqft' in df.columns:\n",
    "        sqft = df['living_sqft'].fillna(df['living_sqft'].median())\n",
    "        size[sqft>sqft.median()] = 'large'\n",
    "\n",
    "    return tier+'_'+size, train_q\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    if 'living_sqft' in df.columns:\n",
    "        if 'bedrooms' in df.columns: df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "        df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns: df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns: df['lot_acres'] = df['lot_sqft']/43560\n",
    "\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'], df['is_new'], df['age_squared'] = 2024-df['year_built'], ((2024-df['year_built'])<=5).astype('int8'), (2024-df['year_built'])**2\n",
    "\n",
    "    if 'garage_spaces' in df.columns:\n",
    "        df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        df['prior_appreciated'] = df['prior_sale_price']*(1.04**df['years_since_last_sale'].fillna(5))\n",
    "\n",
    "    if 'assessed_total_value' in df.columns:\n",
    "        valid = filter_bad_assessed(df)\n",
    "        if 'living_sqft' in df.columns:\n",
    "            df['assessed_per_sqft'] = 0\n",
    "            df.loc[valid,'assessed_per_sqft'] = df.loc[valid,'assessed_total_value']/(df.loc[valid,'living_sqft']+1)\n",
    "        if 'median_home_value' in df.columns:\n",
    "            reasonable = valid & (df['median_home_value']>10000)\n",
    "            df['assessed_to_census_ratio'] = 1.0\n",
    "            df.loc[reasonable,'assessed_to_census_ratio'] = df.loc[reasonable,'assessed_total_value']/(df.loc[reasonable,'median_home_value']+1)\n",
    "        if 'assessed_land_value' in df.columns:\n",
    "            reasonable = valid & (df['assessed_land_value']>0)\n",
    "            df['land_to_total_ratio'] = 0\n",
    "            df.loc[reasonable,'land_to_total_ratio'] = df.loc[reasonable,'assessed_land_value']/(df.loc[reasonable,'assessed_total_value']+1)\n",
    "\n",
    "    if INCLUDE_CENSUS and all(c in df.columns for c in ['median_household_income','pct_bachelors_degree']):\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'living_sqft' in df.columns:\n",
    "        df['prior_price_per_sqft'], df['sqft_per_prior_dollar'] = df['prior_sale_price']/(df['living_sqft']+1), df['living_sqft']/(df['prior_sale_price']+1)\n",
    "\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        if 'prior_sale_price' in df.columns:\n",
    "            df['expected_appreciation'] = df['prior_sale_price']*(1.04)**df['years_since_last_sale']\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df.loc[missing,'median_home_value'] if 'median_home_value' in df.columns and INCLUDE_CENSUS else df['prior_sale_price'].median()\n",
    "\n",
    "    if 'years_since_last_sale' in df.columns:\n",
    "        df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "\n",
    "    if with_price and y_col in df.columns and 'living_sqft' in df.columns:\n",
    "        df['price_per_sqft'], df['sqft_per_dollar'] = df[y_col]/(df['living_sqft']+1), df['living_sqft']/(df[y_col]+1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df, kmeans=None):\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans\n",
    "\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum()<N_CLUSTERS:\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans\n",
    "\n",
    "    df['geo_cluster'] = 0\n",
    "    if kmeans is None:\n",
    "        kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "        df.loc[valid,'geo_cluster'] = kmeans.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    else:\n",
    "        df.loc[valid,'geo_cluster'] = kmeans.predict(df.loc[valid,['latitude','longitude']])\n",
    "\n",
    "    return df, kmeans\n",
    "\n",
    "def add_cluster_feats(train, test, y_col):\n",
    "    if 'geo_cluster' not in train.columns or y_col not in train.columns:\n",
    "        for d in [train,test]:\n",
    "            d['cluster_avg_price'] = d['cluster_med_price'] = train[y_col].median() if y_col in train.columns else 0\n",
    "        return train, test\n",
    "\n",
    "    stats = train.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "    stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "    med = train[y_col].median()\n",
    "    train = train.merge(stats, on='geo_cluster', how='left').fillna({'cluster_avg_price':med,'cluster_med_price':med})\n",
    "    test = test.merge(stats, on='geo_cluster', how='left').fillna({'cluster_avg_price':med,'cluster_med_price':med})\n",
    "    return train, test\n",
    "\n",
    "def filter_outliers(df, name, y_col):\n",
    "    orig = len(df)\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)].drop(columns=['price_per_sqft','sqft_per_dollar'])\n",
    "\n",
    "    if 'lot_sqft' in df.columns: df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "    if 'year_built' in df.columns: df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "\n",
    "    if len(df)>=100:\n",
    "        try:\n",
    "            feats = [c for c in ['living_sqft','lot_sqft',y_col,'year_built'] if c in df.columns]\n",
    "            if len(feats)>=3:\n",
    "                X = df[feats].fillna(df[feats].median())\n",
    "                df = df[IsolationForest(contamination=.05, random_state=RAND_STATE, n_jobs=N_JOBS).fit_predict(X)==1]\n",
    "        except: pass\n",
    "\n",
    "    if (pct:=(orig-len(df))/orig*100)>0: print(f\"  {name}: {orig:,}→{len(df):,} ({pct:.1f}% filtered)\")\n",
    "    return df\n",
    "\n",
    "def train_model(X, y, q):\n",
    "    return XGBRegressor(objective='reg:quantileerror', quantile_alpha=q, n_estimators=N_EST, learning_rate=.1,\n",
    "                       max_depth=5, min_child_weight=3, subsample=.8, colsample_bytree=.8,\n",
    "                       random_state=RAND_STATE, n_jobs=N_JOBS, tree_method='hist').fit(X, y, verbose=False)\n",
    "\n",
    "def get_feat_importance(model, feats, top_n=20):\n",
    "    scores = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    imp = [(feats[int(k[1:])],v) for k,v in scores.items() if int(k[1:])<len(feats)]\n",
    "    imp.sort(key=lambda x: x[1], reverse=True)\n",
    "    total = sum(v for _,v in imp)\n",
    "    return pd.DataFrame([{'feature':f,'gain':g,'importance':g/total} for f,g in imp[:top_n]]) if total>0 else pd.DataFrame(columns=['feature','gain','importance'])\n",
    "\n",
    "def feature_importance(models, feats, metrics):\n",
    "    rows = []\n",
    "    for seg,mdl in models.items():\n",
    "        scores = mdl['q50'].get_booster().get_score(importance_type=\"gain\")\n",
    "        w = metrics[seg][\"n_test\"]\n",
    "        rows.extend([(feats[int(k[1:])],v,w) for k,v in scores.items() if int(k[1:])<len(feats)])\n",
    "\n",
    "    if not rows: return pd.DataFrame(columns=[\"feature\",\"total_gain\",\"importance\"])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"feature\",\"gain\",\"weight\"])\n",
    "    out = df.assign(wg=df[\"gain\"]*df[\"weight\"]).groupby(\"feature\",as_index=False).agg(\n",
    "        total_gain=(\"gain\",\"sum\"), weighted_gain=(\"wg\",\"sum\")).sort_values(\"weighted_gain\",ascending=False)\n",
    "    out[\"importance\"] = out[\"weighted_gain\"]/out[\"weighted_gain\"].sum()\n",
    "    return out[[\"feature\",\"total_gain\",\"importance\"]].head(100)\n",
    "\n",
    "def prepare_data(df, y_col, id_col, state_col, for_training=True):\n",
    "    print(f\"\\nPreparing data (ratio model approach)...\")\n",
    "    if for_training: df = df[df[y_col]>=MIN_PRICE]\n",
    "\n",
    "    df, kmeans = geo_cluster(engineer(df, y_col))\n",
    "\n",
    "    feat_groups = []\n",
    "    if INCLUDE_MLS: feat_groups.extend([BASE_FEATS,ENG_FEATS,PRIOR_FEATS,CLUSTER_FEATS])\n",
    "    if INCLUDE_CENSUS: feat_groups.append(CENSUS_FEATS)\n",
    "    if INCLUDE_NEIGHBORHOOD: feat_groups.append(ELECT_FEATS)\n",
    "    if INCLUDE_IMAGE: feat_groups.append(IMG_FEATS)\n",
    "\n",
    "    all_feats = [f for g in feat_groups for f in g]\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "    feats.extend([vf for vf in ['prior_appreciated','assessed_per_sqft','assessed_to_census_ratio','land_to_total_ratio'] if vf in df.columns and vf not in feats])\n",
    "\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "\n",
    "    return (df.dropna(subset=[y_col]),feats,kmeans) if for_training else (df,feats,kmeans)\n",
    "\n",
    "def train_segments(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining on {len(df):,} properties (RATIO MODEL - predicts actual/value_indicator)\")\n",
    "\n",
    "    df['seg'], train_q = assign_segments(df)\n",
    "\n",
    "    # Add value_indicator features to feature list\n",
    "    if 'value_indicator' in df.columns and 'value_indicator' not in feats:\n",
    "        feats = feats + ['value_indicator']\n",
    "    if 'log_value_indicator' in df.columns and 'log_value_indicator' not in feats:\n",
    "        feats = feats + ['log_value_indicator']\n",
    "\n",
    "    # CRITICAL: Create ratio target (actual price / value_indicator)\n",
    "    df['price_ratio'] = df[y_col] / (df['value_indicator'] + 1)\n",
    "\n",
    "    if '_value_source' in df.columns:\n",
    "        print(f\"\\nValue sources: {dict(df['_value_source'].value_counts())}\")\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    print(f\"\\n{len(seg_cnts)} segments\")\n",
    "\n",
    "    for seg,cnt in seg_cnts.head(MAX_SEGMENTS).items():\n",
    "        if 'value_indicator' in df.columns and seg!='other':\n",
    "            seg_df = df[df['seg']==seg]\n",
    "            if len(seg_df)>0:\n",
    "                avg_ratio = seg_df['price_ratio'].mean()\n",
    "                print(f\"  {seg}: {cnt:,} (${seg_df['value_indicator'].min():,.0f}-${seg_df['value_indicator'].max():,.0f}, avg ratio: {avg_ratio:.2f}x)\")\n",
    "        else:\n",
    "            print(f\"  {seg}: {cnt:,}\")\n",
    "\n",
    "    small = seg_cnts[seg_cnts<50].index\n",
    "    if len(small)>0:\n",
    "        df.loc[df['seg'].isin(small),'seg'] = 'other'\n",
    "        print(f\"Consolidated to {MAX_SEGMENTS} segments\")\n",
    "\n",
    "    seg_cnts = df['seg'].value_counts()\n",
    "    if len(seg_cnts)>MAX_SEGMENTS:\n",
    "        keep = seg_cnts.head(MAX_SEGMENTS-1).index\n",
    "        df.loc[~df['seg'].isin(keep),'seg'] = 'other'\n",
    "        print(f\"Consolidated to {MAX_SEGMENTS} segments\")\n",
    "\n",
    "    if '_value_source' in df.columns:\n",
    "        df = df.drop(columns=['_value_source'])\n",
    "\n",
    "    models, metrics, preds_list, seg_imps = {}, {}, [], {}\n",
    "\n",
    "    for seg in df['seg'].unique():\n",
    "        seg_df = filter_outliers(df[df['seg']==seg].copy(), seg, y_col)\n",
    "        if len(seg_df)<50: continue\n",
    "\n",
    "        train_idx = seg_df.sample(frac=1-TEST_SIZE, random_state=RAND_STATE).index\n",
    "        train_df, test_df = seg_df.loc[train_idx].copy(), seg_df.loc[seg_df.index.difference(train_idx)].copy()\n",
    "        train_df, test_df = add_cluster_feats(train_df, test_df, y_col)\n",
    "\n",
    "        # Train on RATIO, not absolute price\n",
    "        X_tr, y_tr = train_df[feats].values, train_df['price_ratio'].values\n",
    "        X_te, y_te_ratio = test_df[feats].values, test_df['price_ratio'].values\n",
    "        y_te_actual = test_df[y_col].values\n",
    "        value_indicators = test_df['value_indicator'].values\n",
    "\n",
    "        ids = test_df[id_col].values\n",
    "        states = test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "\n",
    "        seg_models = {f\"q{int(q*100)}\":train_model(X_tr,y_tr,q) for q in QUANTILES}\n",
    "        seg_preds_ratio = [seg_models[f\"q{int(q*100)}\"].predict(X_te) for q in QUANTILES]\n",
    "\n",
    "        # Convert ratio predictions back to prices\n",
    "        seg_preds = [seg_preds_ratio[i] * value_indicators for i in range(len(QUANTILES))]\n",
    "\n",
    "        models[seg] = seg_models\n",
    "        seg_imps[seg] = get_feat_importance(seg_models['q50'], feats)\n",
    "\n",
    "        mae, mape = mean_absolute_error(y_te_actual,seg_preds[1]), np.mean(np.abs((y_te_actual-seg_preds[1])/y_te_actual))*100\n",
    "        r2, cov = r2_score(y_te_actual,seg_preds[1]), np.mean((y_te_actual>=seg_preds[0])&(y_te_actual<=seg_preds[2]))*100\n",
    "        metrics[seg] = {'n_train':len(X_tr),'n_test':len(X_te),'mae':mae,'mape':mape,'r2':r2,'cov':cov,\n",
    "                       'p_min':seg_df[y_col].min(),'p_max':seg_df[y_col].max(),'p_med':seg_df[y_col].median()}\n",
    "        print(f\"  {seg}: {len(test_df):,} test | MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.3f}\")\n",
    "\n",
    "        preds_list.append(pd.DataFrame({\n",
    "            'property_id':ids, 'state':states, 'actual':y_te_actual, 'predicted':seg_preds[1],\n",
    "            'pred_lower':seg_preds[0], 'pred_upper':seg_preds[2], 'segment':seg\n",
    "        }))\n",
    "\n",
    "    return {'models':models, 'metrics':metrics, 'predictions':pd.concat(preds_list),\n",
    "            'feature_importance':feature_importance(models,feats,metrics), 'segment_importances':seg_imps,\n",
    "            'feature_names':feats, 'training_quantiles':train_q}\n",
    "\n",
    "def predict_new(pred_df, models, feats, y_col, id_col, state_col, kmeans, train_stats, train_q):\n",
    "    print(f\"\\n{'='*60}\\nPREDICTING {len(pred_df):,} NEW PROPERTIES\\n{'='*60}\")\n",
    "\n",
    "    pred_df = engineer(pred_df, y_col)\n",
    "    pred_df, _ = geo_cluster(pred_df, kmeans)\n",
    "\n",
    "    if 'geo_cluster' in pred_df.columns and train_stats is not None:\n",
    "        pred_df = pred_df.merge(train_stats, on='geo_cluster', how='left')\n",
    "        med = train_stats['cluster_avg_price'].median()\n",
    "        pred_df['cluster_avg_price'], pred_df['cluster_med_price'] = pred_df['cluster_avg_price'].fillna(med), pred_df['cluster_med_price'].fillna(med)\n",
    "\n",
    "    pred_df['seg'], _ = assign_segments(pred_df, train_q)\n",
    "\n",
    "    # Build complete feature list including value_indicator\n",
    "    use_feats = list(feats)\n",
    "    for vf in ['value_indicator', 'log_value_indicator']:\n",
    "        if vf in pred_df.columns and vf not in use_feats:\n",
    "            use_feats.append(vf)\n",
    "\n",
    "    for f in use_feats:\n",
    "        if f not in pred_df.columns: pred_df[f] = 0\n",
    "        else: pred_df[f] = pred_df[f].fillna(pred_df[f].median() if pred_df[f].notna().sum()>0 else 0)\n",
    "\n",
    "    if '_value_source' in pred_df.columns:\n",
    "        print(f\"Value sources: {dict(pred_df['_value_source'].value_counts())}\")\n",
    "\n",
    "    preds_list = []\n",
    "    for seg in pred_df['seg'].unique():\n",
    "        seg_df = pred_df[pred_df['seg']==seg].copy()\n",
    "\n",
    "        if seg not in models:\n",
    "            print(f\"  Warning: '{seg}' not in models, using fallback\")\n",
    "            avail = list(models.keys())\n",
    "            seg = next((s for s in avail if any(x in seg for x in ['ultra','luxury','premium','mid','economy','budget'] if x in s)), avail[0])\n",
    "\n",
    "        X = seg_df[use_feats].values\n",
    "        ids = seg_df[id_col].values\n",
    "        states = seg_df[state_col].values if state_col and state_col in seg_df.columns else ['Unknown']*len(seg_df)\n",
    "        actual = seg_df[y_col].values if y_col in seg_df.columns else [np.nan]*len(seg_df)\n",
    "        value_ind = seg_df['value_indicator'].values if 'value_indicator' in seg_df.columns else [np.nan]*len(seg_df)\n",
    "        value_src = seg_df['_value_source'].values if '_value_source' in seg_df.columns else ['unknown']*len(seg_df)\n",
    "\n",
    "        # Model predicts RATIO, convert to price\n",
    "        preds_ratio = [models[seg][f\"q{int(q*100)}\"].predict(X) for q in QUANTILES]\n",
    "        preds = [preds_ratio[i] * value_ind for i in range(len(QUANTILES))]\n",
    "\n",
    "        preds_list.append(pd.DataFrame({\n",
    "            'property_id':ids, 'state':states, 'value_indicator':value_ind, 'value_source':value_src,\n",
    "            'predicted_ratio':preds_ratio[1], 'actual':actual, 'predicted':preds[1],\n",
    "            'pred_lower':preds[0], 'pred_upper':preds[2], 'segment':seg,\n",
    "            'error':[actual[i]-preds[1][i] if not np.isnan(actual[i]) else np.nan for i in range(len(actual))],\n",
    "            'pct_error':[100*(actual[i]-preds[1][i])/actual[i] if not np.isnan(actual[i]) and actual[i]!=0 else np.nan for i in range(len(actual))]\n",
    "        }))\n",
    "\n",
    "        print(f\"  {seg}: {len(seg_df):,} predicted\")\n",
    "\n",
    "    result = pd.concat(preds_list, ignore_index=True)\n",
    "    print(f\"\\n✓ Generated {len(result):,} predictions\")\n",
    "\n",
    "    if '_value_source' in pred_df.columns: pred_df = pred_df.drop(columns=['_value_source'])\n",
    "\n",
    "    valid = result['actual'].notna().sum()\n",
    "    if valid>0:\n",
    "        valid_df = result[result['actual'].notna()]\n",
    "        mae, mape, r2 = mean_absolute_error(valid_df['actual'],valid_df['predicted']), np.mean(np.abs((valid_df['actual']-valid_df['predicted'])/valid_df['actual']))*100, r2_score(valid_df['actual'],valid_df['predicted'])\n",
    "        print(f\"  Validation ({valid}): MAE:${mae:,.0f} | MAPE:{mape:.2f}% | R²:{r2:.4f}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def save_results(results, out_dir, new_preds=None):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi, seg_imps = results['predictions'], results['metrics'], results['feature_importance'], results['segment_importances']\n",
    "\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    ws = wb.create_sheet(\"Summary\", 0)\n",
    "    ws['A1'].font, ws['A1'].value = Font(bold=True,size=14), 'IMPROVED SEGMENTED AVM'\n",
    "    ws['A2'].font, ws['A2'].value = Font(italic=True,size=10), 'RATIO MODEL: predicts (actual/value_indicator) then multiplies back'\n",
    "\n",
    "    r2, mae, mape = r2_score(preds['actual'],preds['predicted']), mean_absolute_error(preds['actual'],preds['predicted']), np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "\n",
    "    data = [['Metric','Value'], ['Properties',len(preds)], ['Segments',len(metrics)], ['R²',f'{r2:.4f}'], ['MAE',f'${mae:,.0f}'], ['MAPE%',f'{mape:.2f}%']]\n",
    "    if new_preds is not None: data.append(['New Predictions',len(new_preds)])\n",
    "\n",
    "    for i,(k,v) in enumerate(data,5):\n",
    "        ws[f'A{i}'].font, ws[f'A{i}'].value, ws[f'B{i}'].value = Font(bold=True), k, v\n",
    "\n",
    "    ws = wb.create_sheet(\"Segments\")\n",
    "    seg_df = pd.DataFrame([{**{'Segment':s},**m} for s,m in metrics.items()])\n",
    "    for i,h in enumerate(seg_df.columns,1):\n",
    "        c = ws.cell(1,i,h)\n",
    "        c.font, c.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "    for i,row in enumerate(seg_df.itertuples(index=False),2):\n",
    "        for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    for title,data,name in [('Global Feature Importance',fi,'Global_Feature_Importance')]+[(f'FI: {s}',si,f\"FI_{s}\"[:31]) for s,si in seg_imps.items()]:\n",
    "        ws = wb.create_sheet(name)\n",
    "        ws['A1'].value, ws['A1'].font = title, Font(bold=True,size=12)\n",
    "        for r_idx,row in enumerate(dataframe_to_rows(data,index=False,header=True),2):\n",
    "            for c_idx,value in enumerate(row,1):\n",
    "                cell = ws.cell(row=r_idx,column=c_idx,value=value)\n",
    "                if r_idx==2: cell.font, cell.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "\n",
    "    for sheet_name,data,color in [('Test_Predictions',preds,'366092'), ('New_Predictions',new_preds,'4472C4')]:\n",
    "        if data is None: continue\n",
    "        ws = wb.create_sheet(sheet_name)\n",
    "        for i,h in enumerate(data.columns,1):\n",
    "            c = ws.cell(1,i,h)\n",
    "            c.font, c.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color=color,end_color=color,fill_type='solid')\n",
    "        for i,row in enumerate(data.itertuples(index=False),2):\n",
    "            for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/improved_segmented_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "\n",
    "    for name,data in [('test_predictions',preds), ('segments',seg_df), ('importance',fi)]+[(f'importance_{s}',si) for s,si in seg_imps.items()]+([('new_predictions',new_preds)] if new_preds is not None else []):\n",
    "        data.to_csv(f\"{out_dir}/improved_{name}_{ts}.csv\", index=False)\n",
    "\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved with timestamp: {ts}\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60+\"\\nIMPROVED SEGMENTED AVM\\nRATIO MODEL (predicts actual/value_indicator)\\n\"+\"=\"*60)\n",
    "\n",
    "    df, y_col, id_col, state_col = load_data(TRAINING_INPUT_PATH)\n",
    "    df, feats, kmeans = prepare_data(df, y_col, id_col, state_col, for_training=True)\n",
    "    results = train_segments(df, feats, y_col, id_col, state_col)\n",
    "\n",
    "    train_stats = df.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index() if 'geo_cluster' in df.columns else None\n",
    "    if train_stats is not None: train_stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "\n",
    "    new_preds = None\n",
    "    if PREDICTION_INPUT_PATH:\n",
    "        pred_df, _, _, _ = load_data(PREDICTION_INPUT_PATH)\n",
    "        new_preds = predict_new(pred_df, results['models'], results['feature_names'], y_col, id_col, state_col, kmeans, train_stats, results['training_quantiles'])\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR, new_preds)\n",
    "\n",
    "    preds = results['predictions']\n",
    "    r2, mae, mape = r2_score(preds['actual'],preds['predicted']), mean_absolute_error(preds['actual'],preds['predicted']), np.mean(np.abs((preds['actual']-preds['predicted'])/preds['actual']))*100\n",
    "    print(f\"\\n{'='*60}\\n✓ COMPLETE in {time.time()-t0:.1f}s\\n  Test: {len(preds):,} | {preds['segment'].nunique()} segments\\n  R²:{r2:.4f} | MAE:${mae:,.0f} | MAPE:{mape:.2f}%\")\n",
    "    if new_preds is not None: print(f\"  New predictions: {len(new_preds):,}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__==\"__main__\": main()"
   ],
   "id": "d1f55b08b8485029",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IMPROVED SEGMENTED AVM\n",
      "RATIO MODEL (predicts actual/value_indicator)\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 90.4MB | Price:sale_price ID:property_id\n",
      "\n",
      "Preparing data (ratio model approach)...\n",
      "  ⚠️  Filtered 29,431 bad assessed values\n",
      "51/59 features available\n",
      "\n",
      "Training on 127,258 properties (RATIO MODEL - predicts actual/value_indicator)\n",
      "\n",
      "Value quantiles: Q20:$225,450 Q40:$434,750 Q60:$434,750 Q80:$741,940 Q95:$1,445,367\n",
      "\n",
      "Value sources: {'census': np.int64(127258)}\n",
      "\n",
      "10 segments\n",
      "  premium_large: 27,925 ($434,750-$732,125, avg ratio: 4.25x)\n",
      "  premium_small: 26,120 ($434,750-$732,125, avg ratio: 5.21x)\n",
      "  budget_small: 19,274 ($12,000-$225,433, avg ratio: 35.52x)\n",
      "  economy_large: 13,496 ($225,450-$433,875, avg ratio: 4.57x)\n",
      "  luxury_large: 11,219 ($741,940-$1,426,334, avg ratio: 1.95x)\n",
      "  economy_small: 8,817 ($225,450-$433,875, avg ratio: 6.83x)\n",
      "  luxury_small: 7,745 ($741,940-$1,426,334, avg ratio: 1.32x)\n",
      "Consolidated to 7 segments\n",
      "  budget_small: 19,274→15,269 (20.8% filtered)\n",
      "  budget_small: 4,581 test | MAE:$1,026,460 | MAPE:46.45% | R²:0.341\n",
      "  premium_large: 27,925→22,872 (18.1% filtered)\n",
      "  premium_large: 6,862 test | MAE:$321,974 | MAPE:20.90% | R²:0.440\n",
      "  premium_small: 26,120→21,295 (18.5% filtered)\n",
      "  premium_small: 6,389 test | MAE:$355,598 | MAPE:20.50% | R²:0.317\n",
      "  other: 20,407→16,679 (18.3% filtered)\n",
      "  other: 5,004 test | MAE:$307,936 | MAPE:20.35% | R²:0.549\n",
      "  economy_small: 8,817→6,985 (20.8% filtered)\n",
      "  economy_small: 2,095 test | MAE:$281,009 | MAPE:18.33% | R²:0.134\n",
      "  economy_large: 13,496→10,785 (20.1% filtered)\n",
      "  economy_large: 3,236 test | MAE:$171,895 | MAPE:14.02% | R²:0.284\n",
      "  luxury_large: 11,219→9,120 (18.7% filtered)\n",
      "  luxury_large: 2,736 test | MAE:$333,169 | MAPE:21.57% | R²:0.475\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\n",
      "1 records | 0.0MB | Price:sale_price ID:property_id\n",
      "\n",
      "============================================================\n",
      "PREDICTING 1 NEW PROPERTIES\n",
      "============================================================\n",
      "  ⚠️  Filtered 1 bad assessed values\n",
      "  ⚠️  Filtered 1 bad assessed values\n",
      "Value sources: {'census': np.int64(1)}\n",
      "  economy_small: 1 predicted\n",
      "\n",
      "✓ Generated 1 predictions\n",
      "  Validation (1): MAE:$576,709 | MAPE:109.64% | R²:nan\n",
      "\n",
      "Saving results...\n",
      "✓ Excel: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/improved_segmented_20251224_011420.xlsx\n",
      "✓ CSVs saved with timestamp: 20251224_011420\n",
      "\n",
      "============================================================\n",
      "✓ COMPLETE in 9.6s\n",
      "  Test: 30,903 | 7 segments\n",
      "  R²:0.4251 | MAE:$413,582 | MAPE:23.68%\n",
      "  New predictions: 1\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T06:21:10.057520Z",
     "start_time": "2025-12-24T06:21:05.186543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings, time, os\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Config\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS = 20000, 0.3, 42, -1, 8\n",
    "N_EST = 200  # More trees for single model\n",
    "\n",
    "TRAINING_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "PREDICTION_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8' if set(df[c].dropna().unique()).issubset({0,1}) else 'int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = optimize_dtypes(pd.read_csv(path, low_memory=False))\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return df, y_col, id_col, state_col\n",
    "\n",
    "def filter_bad_assessed(df):\n",
    "    if 'assessed_total_value' not in df.columns:\n",
    "        return pd.Series([False]*len(df), index=df.index)\n",
    "\n",
    "    valid = (df['assessed_total_value']>10000) & (df['assessed_total_value']<100000000)\n",
    "\n",
    "    if 'living_sqft' in df.columns:\n",
    "        has_sqft = (df['living_sqft'].notna()) & (df['living_sqft']>100)\n",
    "        ppsf = df['assessed_total_value']/df['living_sqft']\n",
    "        valid &= ~(has_sqft & ((ppsf<20)|(ppsf>2000)))\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        has_prior = (df['prior_sale_price']>10000) & (df['years_since_last_sale']<=15)\n",
    "        valid &= ~(has_prior & ((df['assessed_total_value']/df['prior_sale_price'])<0.10))\n",
    "\n",
    "    if 'median_home_value' in df.columns:\n",
    "        has_census = (df['median_home_value']>10000)\n",
    "        valid &= ~(has_census & ((df['assessed_total_value']/df['median_home_value'])<0.05))\n",
    "\n",
    "    invalid_cnt = ((df['assessed_total_value'].notna()) & ~valid).sum()\n",
    "    if invalid_cnt>0: print(f\"  ⚠️  Filtered {invalid_cnt:,} bad assessed values\")\n",
    "\n",
    "    return valid\n",
    "\n",
    "def create_value_indicator(df):\n",
    "    \"\"\"Create value_indicator from multiple sources - NO LEAKAGE\"\"\"\n",
    "    value_ind = pd.Series([np.nan]*len(df), index=df.index)\n",
    "    source = pd.Series(['none']*len(df), index=df.index)\n",
    "\n",
    "    # Priority 1: Prior sale PPSF × sqft\n",
    "    if all(c in df.columns for c in ['prior_sale_price','years_since_last_sale','living_sqft']):\n",
    "        has_prior = (df['prior_sale_price']>10000) & (df['years_since_last_sale']<=15) & (df['living_sqft']>400)\n",
    "        if has_prior.sum()>0:\n",
    "            prior_ppsf = df.loc[has_prior,'prior_sale_price'] / (df.loc[has_prior,'living_sqft']+1)\n",
    "            yrs = df.loc[has_prior,'years_since_last_sale'].fillna(5)\n",
    "            appreciated_ppsf = prior_ppsf * (1.04 ** yrs)\n",
    "            value_ind[has_prior] = appreciated_ppsf * df.loc[has_prior,'living_sqft']\n",
    "            source[has_prior] = 'prior_ppsf'\n",
    "\n",
    "    # Priority 2: Assessed value\n",
    "    if 'assessed_total_value' in df.columns:\n",
    "        valid = filter_bad_assessed(df) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            vals = df.loc[valid,'assessed_total_value']\n",
    "            mult = pd.Series([1.15]*len(vals), index=vals.index)\n",
    "            mult[vals<200000], mult[vals>=500000] = 1.1, 1.2\n",
    "            value_ind[valid], source[valid] = vals*mult, 'assessed'\n",
    "\n",
    "    # Priority 3: Census median\n",
    "    if 'median_home_value' in df.columns:\n",
    "        valid = (df['median_home_value']>10000) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            value_ind[valid], source[valid] = df.loc[valid,'median_home_value'], 'census'\n",
    "\n",
    "    # Priority 4: Geo cluster median\n",
    "    if 'cluster_med_price' in df.columns:\n",
    "        valid = (df['cluster_med_price']>10000) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            value_ind[valid], source[valid] = df.loc[valid,'cluster_med_price'], 'cluster'\n",
    "\n",
    "    # Fallback\n",
    "    value_ind = value_ind.fillna(value_ind.median())\n",
    "    source[value_ind.isna()] = 'global_median'\n",
    "\n",
    "    df['value_indicator'] = value_ind\n",
    "    df['log_value_indicator'] = np.log1p(value_ind)\n",
    "    df['_value_source'] = source\n",
    "\n",
    "    return df\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    if 'living_sqft' in df.columns:\n",
    "        if 'bedrooms' in df.columns: df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "        df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns: df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns: df['lot_acres'] = df['lot_sqft']/43560\n",
    "\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'] = 2024-df['year_built']\n",
    "        df['is_new'] = ((2024-df['year_built'])<=5).astype('int8')\n",
    "        df['age_squared'] = (2024-df['year_built'])**2\n",
    "\n",
    "    if 'garage_spaces' in df.columns:\n",
    "        df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        if 'years_since_last_sale' in df.columns:\n",
    "            df['prior_appreciated'] = df['prior_sale_price']*(1.04**df['years_since_last_sale'].fillna(5))\n",
    "        if 'living_sqft' in df.columns:\n",
    "            df['prior_price_per_sqft'] = df['prior_sale_price']/(df['living_sqft']+1)\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "\n",
    "    if 'assessed_total_value' in df.columns and 'living_sqft' in df.columns:\n",
    "        valid = filter_bad_assessed(df)\n",
    "        df['assessed_per_sqft'] = 0\n",
    "        df.loc[valid,'assessed_per_sqft'] = df.loc[valid,'assessed_total_value']/(df.loc[valid,'living_sqft']+1)\n",
    "\n",
    "    if 'median_household_income' in df.columns and 'pct_bachelors_degree' in df.columns:\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "\n",
    "    if 'years_since_last_sale' in df.columns:\n",
    "        df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df['prior_sale_price'].median()\n",
    "\n",
    "    if with_price and y_col in df.columns and 'living_sqft' in df.columns:\n",
    "        df['price_per_sqft'] = df[y_col]/(df['living_sqft']+1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df, kmeans=None):\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans\n",
    "\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum()<N_CLUSTERS:\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans\n",
    "\n",
    "    df['geo_cluster'] = 0\n",
    "    if kmeans is None:\n",
    "        kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "        df.loc[valid,'geo_cluster'] = kmeans.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    else:\n",
    "        df.loc[valid,'geo_cluster'] = kmeans.predict(df.loc[valid,['latitude','longitude']])\n",
    "\n",
    "    return df, kmeans\n",
    "\n",
    "def add_cluster_feats(df, cluster_stats):\n",
    "    if cluster_stats is None or 'geo_cluster' not in df.columns:\n",
    "        df['cluster_avg_price'] = df['cluster_med_price'] = 0\n",
    "        return df\n",
    "\n",
    "    df = df.merge(cluster_stats, on='geo_cluster', how='left')\n",
    "    med = cluster_stats['cluster_avg_price'].median()\n",
    "    df['cluster_avg_price'] = df['cluster_avg_price'].fillna(med)\n",
    "    df['cluster_med_price'] = df['cluster_med_price'].fillna(med)\n",
    "    return df\n",
    "\n",
    "def filter_outliers(df, y_col):\n",
    "    orig = len(df)\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)].drop(columns=['price_per_sqft'])\n",
    "\n",
    "    if 'lot_sqft' in df.columns: df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "    if 'year_built' in df.columns: df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "\n",
    "    if len(df)>=100:\n",
    "        try:\n",
    "            feats = [c for c in ['living_sqft','lot_sqft',y_col,'year_built'] if c in df.columns]\n",
    "            if len(feats)>=3:\n",
    "                X = df[feats].fillna(df[feats].median())\n",
    "                df = df[IsolationForest(contamination=.05, random_state=RAND_STATE, n_jobs=N_JOBS).fit_predict(X)==1]\n",
    "        except: pass\n",
    "\n",
    "    pct = (orig-len(df))/orig*100 if orig>0 else 0\n",
    "    if pct>0: print(f\"  Filtered: {orig:,}→{len(df):,} ({pct:.1f}%)\")\n",
    "    return df\n",
    "\n",
    "def train_single_model(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining SINGLE GLOBAL MODEL on {len(df):,} properties\")\n",
    "    print(\"Approach: XGBoost learns when to trust value_indicator vs other features\")\n",
    "\n",
    "    # Filter outliers\n",
    "    df = filter_outliers(df, y_col)\n",
    "\n",
    "    # Split train/test\n",
    "    train_idx = df.sample(frac=1-TEST_SIZE, random_state=RAND_STATE).index\n",
    "    train_df = df.loc[train_idx].copy()\n",
    "    test_df = df.loc[df.index.difference(train_idx)].copy()\n",
    "\n",
    "    # Add cluster stats from training data\n",
    "    cluster_stats = None\n",
    "    if 'geo_cluster' in train_df.columns:\n",
    "        cluster_stats = train_df.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "        cluster_stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "        train_df = add_cluster_feats(train_df, cluster_stats)\n",
    "        test_df = add_cluster_feats(test_df, cluster_stats)\n",
    "\n",
    "    X_tr, y_tr = train_df[feats].values, train_df[y_col].values\n",
    "    X_te, y_te = test_df[feats].values, test_df[y_col].values\n",
    "\n",
    "    print(f\"  Training: {len(X_tr):,} | Test: {len(X_te):,}\")\n",
    "\n",
    "    # Train single model\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=N_EST,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RAND_STATE,\n",
    "        n_jobs=N_JOBS,\n",
    "        tree_method='hist'\n",
    "    ).fit(X_tr, y_tr, verbose=False)\n",
    "\n",
    "    y_pred = model.predict(X_te)\n",
    "\n",
    "    mae = mean_absolute_error(y_te, y_pred)\n",
    "    mape = np.mean(np.abs((y_te-y_pred)/y_te))*100\n",
    "    r2 = r2_score(y_te, y_pred)\n",
    "\n",
    "    print(f\"  Results: MAE=${mae:,.0f} | MAPE={mape:.2f}% | R²={r2:.4f}\")\n",
    "\n",
    "    # Feature importance\n",
    "    scores = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    imp = [(feats[int(k[1:])],v) for k,v in scores.items() if int(k[1:])<len(feats)]\n",
    "    imp.sort(key=lambda x: x[1], reverse=True)\n",
    "    total = sum(v for _,v in imp)\n",
    "    importance_df = pd.DataFrame([\n",
    "        {'feature':f,'gain':g,'importance':g/total} for f,g in imp[:20]\n",
    "    ]) if total>0 else pd.DataFrame(columns=['feature','gain','importance'])\n",
    "\n",
    "    # Predictions\n",
    "    ids = test_df[id_col].values\n",
    "    states = test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "    value_ind = test_df['value_indicator'].values if 'value_indicator' in test_df.columns else [np.nan]*len(test_df)\n",
    "    value_src = test_df['_value_source'].values if '_value_source' in test_df.columns else ['unknown']*len(test_df)\n",
    "\n",
    "    preds_df = pd.DataFrame({\n",
    "        'property_id':ids, 'state':states, 'value_indicator':value_ind, 'value_source':value_src,\n",
    "        'actual':y_te, 'predicted':y_pred,\n",
    "        'error':y_te-y_pred,\n",
    "        'pct_error':100*(y_te-y_pred)/y_te\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'metrics': {'mae':mae, 'mape':mape, 'r2':r2, 'n_train':len(X_tr), 'n_test':len(X_te)},\n",
    "        'predictions': preds_df,\n",
    "        'feature_importance': importance_df,\n",
    "        'cluster_stats': cluster_stats\n",
    "    }\n",
    "\n",
    "def predict_new(pred_df, model, feats, y_col, id_col, state_col, kmeans, cluster_stats):\n",
    "    print(f\"\\n{'='*60}\\nPREDICTING {len(pred_df):,} NEW PROPERTIES\\n{'='*60}\")\n",
    "\n",
    "    pred_df = engineer(pred_df, y_col)\n",
    "    pred_df, _ = geo_cluster(pred_df, kmeans)\n",
    "    pred_df = create_value_indicator(pred_df)\n",
    "    pred_df = add_cluster_feats(pred_df, cluster_stats)\n",
    "\n",
    "    # Fill missing features\n",
    "    for f in feats:\n",
    "        if f not in pred_df.columns: pred_df[f] = 0\n",
    "        else: pred_df[f] = pred_df[f].fillna(pred_df[f].median() if pred_df[f].notna().sum()>0 else 0)\n",
    "\n",
    "    if '_value_source' in pred_df.columns:\n",
    "        print(f\"Value sources: {dict(pred_df['_value_source'].value_counts())}\")\n",
    "\n",
    "    X = pred_df[feats].values\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    ids = pred_df[id_col].values\n",
    "    states = pred_df[state_col].values if state_col and state_col in pred_df.columns else ['Unknown']*len(pred_df)\n",
    "    actual = pred_df[y_col].values if y_col in pred_df.columns else [np.nan]*len(pred_df)\n",
    "    value_ind = pred_df['value_indicator'].values\n",
    "    value_src = pred_df['_value_source'].values\n",
    "\n",
    "    result = pd.DataFrame({\n",
    "        'property_id':ids, 'state':states, 'value_indicator':value_ind, 'value_source':value_src,\n",
    "        'actual':actual, 'predicted':y_pred,\n",
    "        'error':[actual[i]-y_pred[i] if not np.isnan(actual[i]) else np.nan for i in range(len(actual))],\n",
    "        'pct_error':[100*(actual[i]-y_pred[i])/actual[i] if not np.isnan(actual[i]) and actual[i]!=0 else np.nan for i in range(len(actual))]\n",
    "    })\n",
    "\n",
    "    print(f\"✓ Generated {len(result):,} predictions\")\n",
    "\n",
    "    valid = result['actual'].notna().sum()\n",
    "    if valid>0:\n",
    "        valid_df = result[result['actual'].notna()]\n",
    "        mae = mean_absolute_error(valid_df['actual'], valid_df['predicted'])\n",
    "        mape = np.mean(np.abs((valid_df['actual']-valid_df['predicted'])/valid_df['actual']))*100\n",
    "        r2 = r2_score(valid_df['actual'], valid_df['predicted'])\n",
    "        print(f\"Validation ({valid}): MAE=${mae:,.0f} | MAPE={mape:.2f}% | R²={r2:.4f}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def save_results(results, out_dir, new_preds=None):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi = results['predictions'], results['metrics'], results['feature_importance']\n",
    "\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    ws = wb.create_sheet(\"Summary\", 0)\n",
    "    ws['A1'].font, ws['A1'].value = Font(bold=True,size=14), 'SINGLE GLOBAL MODEL AVM'\n",
    "    ws['A2'].font, ws['A2'].value = Font(italic=True,size=10), 'XGBoost learns when to trust value_indicator'\n",
    "\n",
    "    data = [\n",
    "        ['Metric','Value'],\n",
    "        ['Train Properties',metrics['n_train']],\n",
    "        ['Test Properties',metrics['n_test']],\n",
    "        ['R²',f\"{metrics['r2']:.4f}\"],\n",
    "        ['MAE',f\"${metrics['mae']:,.0f}\"],\n",
    "        ['MAPE%',f\"{metrics['mape']:.2f}%\"]\n",
    "    ]\n",
    "    if new_preds is not None: data.append(['New Predictions',len(new_preds)])\n",
    "\n",
    "    for i,(k,v) in enumerate(data,5):\n",
    "        ws[f'A{i}'].font, ws[f'A{i}'].value, ws[f'B{i}'].value = Font(bold=True), k, v\n",
    "\n",
    "    ws = wb.create_sheet(\"Feature_Importance\")\n",
    "    for r_idx,row in enumerate(dataframe_to_rows(fi,index=False,header=True),1):\n",
    "        for c_idx,value in enumerate(row,1):\n",
    "            cell = ws.cell(row=r_idx,column=c_idx,value=value)\n",
    "            if r_idx==1:\n",
    "                cell.font = Font(bold=True,color='FFFFFF')\n",
    "                cell.fill = PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "\n",
    "    for sheet_name,data,color in [('Test_Predictions',preds,'366092'),('New_Predictions',new_preds,'4472C4')]:\n",
    "        if data is None: continue\n",
    "        ws = wb.create_sheet(sheet_name)\n",
    "        for i,h in enumerate(data.columns,1):\n",
    "            c = ws.cell(1,i,h)\n",
    "            c.font, c.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color=color,end_color=color,fill_type='solid')\n",
    "        for i,row in enumerate(data.itertuples(index=False),2):\n",
    "            for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/single_model_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "\n",
    "    preds.to_csv(f\"{out_dir}/single_test_predictions_{ts}.csv\", index=False)\n",
    "    fi.to_csv(f\"{out_dir}/single_importance_{ts}.csv\", index=False)\n",
    "    if new_preds is not None:\n",
    "        new_preds.to_csv(f\"{out_dir}/single_new_predictions_{ts}.csv\", index=False)\n",
    "\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60+\"\\nSINGLE GLOBAL MODEL AVM\\nNo Segmentation - Let XGBoost Learn\\n\"+\"=\"*60)\n",
    "\n",
    "    # Load and prepare training data\n",
    "    df, y_col, id_col, state_col = load_data(TRAINING_INPUT_PATH)\n",
    "    df = df[df[y_col]>=MIN_PRICE]\n",
    "    df = engineer(df, y_col)\n",
    "    df, kmeans = geo_cluster(df)\n",
    "    df = create_value_indicator(df)\n",
    "\n",
    "    # Define features\n",
    "    base = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\n",
    "            \"latitude\",\"longitude\",\"geo_cluster\",\"value_indicator\",\"log_value_indicator\"]\n",
    "    eng = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\"]\n",
    "    prior = [\"prior_sale_price\",\"prior_price_per_sqft\",\"prior_appreciated\",\"years_since_last_sale\",\"has_prior_sale\",\"recently_sold\"]\n",
    "    cluster = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "    census = [\"median_household_income\",\"median_home_value\",\"pct_bachelors_degree\",\"income_education_score\"]\n",
    "\n",
    "    all_feats = base + eng + prior + cluster + census\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "    df = df.dropna(subset=[y_col])\n",
    "\n",
    "    # Train model\n",
    "    results = train_single_model(df, feats, y_col, id_col, state_col)\n",
    "\n",
    "    # Predict new properties\n",
    "    new_preds = None\n",
    "    if PREDICTION_INPUT_PATH:\n",
    "        pred_df, _, _, _ = load_data(PREDICTION_INPUT_PATH)\n",
    "        new_preds = predict_new(pred_df, results['model'], feats, y_col, id_col, state_col,\n",
    "                               kmeans, results['cluster_stats'])\n",
    "\n",
    "    # Save results\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR, new_preds)\n",
    "\n",
    "    print(f\"\\n{'='*60}\\n✓ COMPLETE in {time.time()-t0:.1f}s\")\n",
    "    if new_preds is not None: print(f\"  New predictions: {len(new_preds):,}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__==\"__main__\": main()"
   ],
   "id": "84479e85ed3fbbe0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SINGLE GLOBAL MODEL AVM\n",
      "No Segmentation - Let XGBoost Learn\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 90.4MB | Price:sale_price ID:property_id\n",
      "  ⚠️  Filtered 29,431 bad assessed values\n",
      "  ⚠️  Filtered 29,431 bad assessed values\n",
      "24/32 features available\n",
      "\n",
      "Training SINGLE GLOBAL MODEL on 127,258 properties\n",
      "Approach: XGBoost learns when to trust value_indicator vs other features\n",
      "  Filtered: 127,258→103,005 (19.1%)\n",
      "  Training: 72,104 | Test: 30,901\n",
      "  Results: MAE=$397,134 | MAPE=28.56% | R²=0.4193\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\n",
      "1 records | 0.0MB | Price:sale_price ID:property_id\n",
      "\n",
      "============================================================\n",
      "PREDICTING 1 NEW PROPERTIES\n",
      "============================================================\n",
      "  ⚠️  Filtered 1 bad assessed values\n",
      "  ⚠️  Filtered 1 bad assessed values\n",
      "Value sources: {'census': np.int64(1)}\n",
      "✓ Generated 1 predictions\n",
      "Validation (1): MAE=$921,190 | MAPE=175.13% | R²=nan\n",
      "\n",
      "Saving results...\n",
      "✓ Excel: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/single_model_20251224_012108.xlsx\n",
      "✓ CSVs saved\n",
      "\n",
      "============================================================\n",
      "✓ COMPLETE in 4.7s\n",
      "  New predictions: 1\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T06:46:36.838560Z",
     "start_time": "2025-12-24T06:46:33.676975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings, time, os\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Config\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS = 20000, 0.3, 42, -1, 8\n",
    "N_EST = 200  # More trees for single model\n",
    "\n",
    "TRAINING_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "PREDICTION_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8' if set(df[c].dropna().unique()).issubset({0,1}) else 'int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = optimize_dtypes(pd.read_csv(path, low_memory=False))\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return df, y_col, id_col, state_col\n",
    "\n",
    "def filter_bad_assessed(df):\n",
    "    if 'assessed_total_value' not in df.columns:\n",
    "        return pd.Series([False]*len(df), index=df.index)\n",
    "\n",
    "    valid = (df['assessed_total_value']>10000) & (df['assessed_total_value']<100000000)\n",
    "\n",
    "    if 'living_sqft' in df.columns:\n",
    "        has_sqft = (df['living_sqft'].notna()) & (df['living_sqft']>100)\n",
    "        ppsf = df['assessed_total_value']/df['living_sqft']\n",
    "        valid &= ~(has_sqft & ((ppsf<20)|(ppsf>2000)))\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        has_prior = (df['prior_sale_price']>10000) & (df['years_since_last_sale']<=15)\n",
    "        valid &= ~(has_prior & ((df['assessed_total_value']/df['prior_sale_price'])<0.10))\n",
    "\n",
    "    if 'median_home_value' in df.columns:\n",
    "        has_census = (df['median_home_value']>10000)\n",
    "        valid &= ~(has_census & ((df['assessed_total_value']/df['median_home_value'])<0.05))\n",
    "\n",
    "    invalid_cnt = ((df['assessed_total_value'].notna()) & ~valid).sum()\n",
    "    if invalid_cnt>0: print(f\"  ⚠️  Filtered {invalid_cnt:,} bad assessed values\")\n",
    "\n",
    "    return valid\n",
    "\n",
    "def create_value_indicator(df):\n",
    "    \"\"\"Create value_indicator from multiple sources - NO LEAKAGE\"\"\"\n",
    "    value_ind = pd.Series([np.nan]*len(df), index=df.index)\n",
    "    source = pd.Series(['none']*len(df), index=df.index)\n",
    "\n",
    "    # Priority 1: Prior sale PPSF × sqft\n",
    "    if all(c in df.columns for c in ['prior_sale_price','years_since_last_sale','living_sqft']):\n",
    "        has_prior = (df['prior_sale_price']>10000) & (df['years_since_last_sale']<=15) & (df['living_sqft']>400)\n",
    "        if has_prior.sum()>0:\n",
    "            prior_ppsf = df.loc[has_prior,'prior_sale_price'] / (df.loc[has_prior,'living_sqft']+1)\n",
    "            yrs = df.loc[has_prior,'years_since_last_sale'].fillna(5)\n",
    "            appreciated_ppsf = prior_ppsf * (1.04 ** yrs)\n",
    "            value_ind[has_prior] = appreciated_ppsf * df.loc[has_prior,'living_sqft']\n",
    "            source[has_prior] = 'prior_ppsf'\n",
    "\n",
    "    # Priority 2: Assessed value\n",
    "    if 'assessed_total_value' in df.columns:\n",
    "        valid = filter_bad_assessed(df) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            vals = df.loc[valid,'assessed_total_value']\n",
    "            mult = pd.Series([1.15]*len(vals), index=vals.index)\n",
    "            mult[vals<200000], mult[vals>=500000] = 1.1, 1.2\n",
    "            value_ind[valid], source[valid] = vals*mult, 'assessed'\n",
    "\n",
    "    # Priority 3: Census median\n",
    "    if 'median_home_value' in df.columns:\n",
    "        valid = (df['median_home_value']>10000) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            value_ind[valid], source[valid] = df.loc[valid,'median_home_value'], 'census'\n",
    "\n",
    "    # Priority 4: Geo cluster median\n",
    "    if 'cluster_med_price' in df.columns:\n",
    "        valid = (df['cluster_med_price']>10000) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            value_ind[valid], source[valid] = df.loc[valid,'cluster_med_price'], 'cluster'\n",
    "\n",
    "    # Fallback\n",
    "    value_ind = value_ind.fillna(value_ind.median())\n",
    "    source[value_ind.isna()] = 'global_median'\n",
    "\n",
    "    df['value_indicator'] = value_ind\n",
    "    df['log_value_indicator'] = np.log1p(value_ind)\n",
    "    df['_value_source'] = source\n",
    "\n",
    "    return df\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    if 'living_sqft' in df.columns:\n",
    "        if 'bedrooms' in df.columns: df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "        df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns: df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns: df['lot_acres'] = df['lot_sqft']/43560\n",
    "\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'] = 2024-df['year_built']\n",
    "        df['is_new'] = ((2024-df['year_built'])<=5).astype('int8')\n",
    "        df['age_squared'] = (2024-df['year_built'])**2\n",
    "\n",
    "    if 'garage_spaces' in df.columns:\n",
    "        df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        if 'years_since_last_sale' in df.columns:\n",
    "            df['prior_appreciated'] = df['prior_sale_price']*(1.04**df['years_since_last_sale'].fillna(5))\n",
    "        if 'living_sqft' in df.columns:\n",
    "            df['prior_price_per_sqft'] = df['prior_sale_price']/(df['living_sqft']+1)\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "\n",
    "    if 'assessed_total_value' in df.columns and 'living_sqft' in df.columns:\n",
    "        valid = filter_bad_assessed(df)\n",
    "        df['assessed_per_sqft'] = 0\n",
    "        df.loc[valid,'assessed_per_sqft'] = df.loc[valid,'assessed_total_value']/(df.loc[valid,'living_sqft']+1)\n",
    "\n",
    "    if 'median_household_income' in df.columns and 'pct_bachelors_degree' in df.columns:\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "\n",
    "    if 'years_since_last_sale' in df.columns:\n",
    "        df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df['prior_sale_price'].median()\n",
    "\n",
    "    if with_price and y_col in df.columns:\n",
    "        if 'living_sqft' in df.columns:\n",
    "            df['price_per_sqft'] = df[y_col]/(df['living_sqft']+1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df, kmeans=None):\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans\n",
    "\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum()<N_CLUSTERS:\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans\n",
    "\n",
    "    df['geo_cluster'] = 0\n",
    "    if kmeans is None:\n",
    "        kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "        df.loc[valid,'geo_cluster'] = kmeans.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    else:\n",
    "        df.loc[valid,'geo_cluster'] = kmeans.predict(df.loc[valid,['latitude','longitude']])\n",
    "\n",
    "    return df, kmeans\n",
    "\n",
    "def add_cluster_feats(df, cluster_stats):\n",
    "    if cluster_stats is None or 'geo_cluster' not in df.columns:\n",
    "        df['cluster_avg_price'] = df['cluster_med_price'] = 0\n",
    "        return df\n",
    "\n",
    "    df = df.merge(cluster_stats, on='geo_cluster', how='left')\n",
    "    med = cluster_stats['cluster_avg_price'].median()\n",
    "    df['cluster_avg_price'] = df['cluster_avg_price'].fillna(med)\n",
    "    df['cluster_med_price'] = df['cluster_med_price'].fillna(med)\n",
    "    return df\n",
    "\n",
    "def detect_and_normalize_anomalies(df, y_col, for_training=True):\n",
    "    \"\"\"\n",
    "    AGGRESSIVE neighborhood-level anomaly detection and normalization.\n",
    "    Detects properties with extreme feature combinations relative to their cohort.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PREPROCESSING: Aggressive Anomaly Detection\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    df['anomaly_flag'] = 0\n",
    "    df['anomaly_reason'] = 'none'\n",
    "\n",
    "    # Create cohorts: geo_cluster + value_indicator quartile\n",
    "    if 'geo_cluster' not in df.columns or 'value_indicator' not in df.columns:\n",
    "        print(\"⚠️  Skipping anomaly detection - missing clustering features\")\n",
    "        return df\n",
    "\n",
    "    # Skip for very small datasets\n",
    "    if len(df) < 10:\n",
    "        print(f\"⚠️  Skipping cohort-based anomaly detection - only {len(df)} properties (need 10+)\")\n",
    "        return df\n",
    "\n",
    "    # Handle quartile binning errors gracefully\n",
    "    try:\n",
    "        df['value_quartile'] = pd.qcut(\n",
    "            df['value_indicator'],\n",
    "            q=4,\n",
    "            labels=False,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        df['value_quartile'] = df['value_quartile'].map({\n",
    "            0: 'Q1', 1: 'Q2', 2: 'Q3', 3: 'Q4'\n",
    "        }).fillna('Q1')\n",
    "\n",
    "    except (ValueError, TypeError) as e:\n",
    "        print(f\"⚠️  Could not create quartiles ({str(e)}), using single cohort\")\n",
    "        df['value_quartile'] = 'Q1'\n",
    "\n",
    "    df['cohort'] = df['geo_cluster'].astype(str) + '_' + df['value_quartile'].astype(str)\n",
    "\n",
    "    # AGGRESSIVE feature checks - MUCH TIGHTER thresholds\n",
    "    feature_checks = {\n",
    "        'living_sqft': ('sqft', 0.6, 1.8),      # Was 0.5-2.5, now 0.6-1.8\n",
    "        'full_baths': ('baths', 0, 8),           # Was 0-10, now 0-8\n",
    "        'luxury_score': ('luxury', 0.4, 2.5),    # Was 0.3-3.0, now 0.4-2.5\n",
    "        'property_age': ('age', -10, 120),       # Was -20-150, now -10-120\n",
    "        'bedrooms': ('beds', 0, 10),             # NEW: cap bedrooms\n",
    "        'lot_sqft': ('lot', 0.3, 3.0),           # NEW: lot size relative to cohort\n",
    "    }\n",
    "\n",
    "    anomalies_found = 0\n",
    "\n",
    "    for cohort in df['cohort'].unique():\n",
    "        cohort_df = df[df['cohort']==cohort]\n",
    "        if len(cohort_df) < 10:\n",
    "            continue\n",
    "\n",
    "        cohort_idx = cohort_df.index\n",
    "\n",
    "        # Check each feature for anomalies\n",
    "        for feat, (name, min_mult, max_mult) in feature_checks.items():\n",
    "            if feat not in df.columns:\n",
    "                continue\n",
    "\n",
    "            cohort_vals = cohort_df[feat].dropna()\n",
    "            if len(cohort_vals) < 5:\n",
    "                continue\n",
    "\n",
    "            cohort_median = cohort_vals.median()\n",
    "            cohort_std = cohort_vals.std()\n",
    "\n",
    "            if cohort_median == 0 or cohort_std == 0:\n",
    "                continue\n",
    "\n",
    "            # Method 1: Ratio-based detection (TIGHTER thresholds)\n",
    "            if feat in ['living_sqft', 'luxury_score', 'lot_sqft']:\n",
    "                ratios = df.loc[cohort_idx, feat] / cohort_median\n",
    "                anomaly_mask = (ratios < min_mult) | (ratios > max_mult)\n",
    "\n",
    "                if anomaly_mask.sum() > 0:\n",
    "                    df.loc[cohort_idx[anomaly_mask], feat] = np.clip(\n",
    "                        df.loc[cohort_idx[anomaly_mask], feat],\n",
    "                        cohort_median * min_mult,\n",
    "                        cohort_median * max_mult\n",
    "                    )\n",
    "                    df.loc[cohort_idx[anomaly_mask], 'anomaly_flag'] = 1\n",
    "                    df.loc[cohort_idx[anomaly_mask], 'anomaly_reason'] = f'{name}_extreme'\n",
    "                    anomalies_found += anomaly_mask.sum()\n",
    "\n",
    "            # Method 2: Z-score detection (TIGHTER - 2.5 std instead of 3)\n",
    "            elif feat in ['property_age', 'full_baths', 'bedrooms']:\n",
    "                z_scores = np.abs((df.loc[cohort_idx, feat] - cohort_median) / (cohort_std + 1))\n",
    "                anomaly_mask = z_scores > 2.5  # Was 3, now 2.5\n",
    "\n",
    "                if anomaly_mask.sum() > 0:\n",
    "                    df.loc[cohort_idx[anomaly_mask], feat] = np.clip(\n",
    "                        df.loc[cohort_idx[anomaly_mask], feat],\n",
    "                        cohort_median - 2.5*cohort_std,\n",
    "                        cohort_median + 2.5*cohort_std\n",
    "                    )\n",
    "                    df.loc[cohort_idx[anomaly_mask], 'anomaly_flag'] = 1\n",
    "                    df.loc[cohort_idx[anomaly_mask], 'anomaly_reason'] = f'{name}_outlier'\n",
    "                    anomalies_found += anomaly_mask.sum()\n",
    "\n",
    "    # NEW: Check living_sqft vs bedrooms ratio\n",
    "    if 'living_sqft' in df.columns and 'bedrooms' in df.columns:\n",
    "        sqft_per_bed = df['living_sqft'] / (df['bedrooms'] + 1)\n",
    "        # Typical range: 400-1500 sqft/bedroom\n",
    "        extreme_ratio = (sqft_per_bed < 250) | (sqft_per_bed > 2000)\n",
    "        if extreme_ratio.sum() > 0:\n",
    "            df.loc[extreme_ratio, 'anomaly_flag'] = 1\n",
    "            df.loc[extreme_ratio, 'anomaly_reason'] = 'sqft_bed_mismatch'\n",
    "            anomalies_found += extreme_ratio.sum()\n",
    "\n",
    "    # EXISTING: Cross-feature consistency (TIGHTER)\n",
    "    if 'living_sqft' in df.columns and 'luxury_score' in df.columns:\n",
    "        sqft_low = df['living_sqft'] < df['living_sqft'].quantile(0.30)  # Was 0.25\n",
    "        luxury_high = df['luxury_score'] > df['luxury_score'].quantile(0.70)  # Was 0.75\n",
    "        inconsistent = sqft_low & luxury_high\n",
    "\n",
    "        if inconsistent.sum() > 0:\n",
    "            df.loc[inconsistent, 'luxury_score'] = df.loc[inconsistent, 'luxury_score'] * 0.5  # Was 0.6\n",
    "            df.loc[inconsistent, 'anomaly_flag'] = 1\n",
    "            df.loc[inconsistent, 'anomaly_reason'] = 'sqft_luxury_mismatch'\n",
    "            anomalies_found += inconsistent.sum()\n",
    "\n",
    "    # NEW: Penalize census-based value indicators (they're often unreliable)\n",
    "    if '_value_source' in df.columns:\n",
    "        census_mask = df['_value_source'] == 'census'\n",
    "        if census_mask.sum() > 0:\n",
    "            # Reduce impact of census values by dampening extreme features\n",
    "            if 'luxury_score' in df.columns:\n",
    "                df.loc[census_mask, 'luxury_score'] = df.loc[census_mask, 'luxury_score'] * 0.85\n",
    "            if 'living_sqft' in df.columns:\n",
    "                median_sqft = df['living_sqft'].median()\n",
    "                extreme_sqft = df.loc[census_mask, 'living_sqft'] > median_sqft * 1.5\n",
    "                if extreme_sqft.sum() > 0:\n",
    "                    df.loc[census_mask & extreme_sqft, 'living_sqft'] = df.loc[census_mask & extreme_sqft, 'living_sqft'] * 0.9\n",
    "\n",
    "            df.loc[census_mask, 'anomaly_flag'] = 1\n",
    "            df.loc[census_mask, 'anomaly_reason'] = 'census_unreliable'\n",
    "            anomalies_found += census_mask.sum()\n",
    "\n",
    "    # TRAINING ONLY: Check price vs value_indicator ratio (TIGHTER)\n",
    "    if for_training and y_col in df.columns:\n",
    "        df['price_to_value_ratio'] = df[y_col] / (df['value_indicator'] + 1)\n",
    "\n",
    "        # MUCH TIGHTER: ratio >2.5x or <0.5x (was 3.0x and 0.4x)\n",
    "        extreme_ratio = (df['price_to_value_ratio'] > 2.5) | (df['price_to_value_ratio'] < 0.5)\n",
    "\n",
    "        if extreme_ratio.sum() > 0:\n",
    "            print(f\"  ⚠️  Found {extreme_ratio.sum()} properties with extreme price/value ratios\")\n",
    "            print(f\"      These will be EXCLUDED from training (likely data errors)\")\n",
    "            df.loc[extreme_ratio, 'anomaly_flag'] = 2  # Flag 2 = exclude\n",
    "            df.loc[extreme_ratio, 'anomaly_reason'] = 'extreme_price_ratio'\n",
    "            anomalies_found += extreme_ratio.sum()\n",
    "\n",
    "    if anomalies_found > 0:\n",
    "        cohort_count = df['cohort'].nunique() if 'cohort' in df.columns else 0\n",
    "        print(f\"  ✓ Detected {anomalies_found:,} anomalies across {cohort_count} cohorts\")\n",
    "    else:\n",
    "        print(f\"  ✓ No anomalies detected\")\n",
    "\n",
    "    # For training: exclude extreme anomalies (flag=2)\n",
    "    if for_training:\n",
    "        before = len(df)\n",
    "        df = df[df['anomaly_flag'] != 2].copy()\n",
    "        if len(df) < before:\n",
    "            print(f\"  ✓ Excluded {before-len(df):,} extreme anomalies from training\")\n",
    "\n",
    "    # Clean up temporary columns\n",
    "    cols_to_drop = ['value_quartile', 'cohort']\n",
    "    if 'price_to_value_ratio' in df.columns:\n",
    "        cols_to_drop.append('price_to_value_ratio')\n",
    "    df = df.drop(columns=[c for c in cols_to_drop if c in df.columns])\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_prediction_confidence(pred_df):\n",
    "    \"\"\"\n",
    "    Add a confidence score and warning flags for predictions.\n",
    "    Helps identify predictions that are likely to be unreliable.\n",
    "    \"\"\"\n",
    "    pred_df['confidence_score'] = 100  # Start at 100%\n",
    "    pred_df['warning_flags'] = ''\n",
    "\n",
    "    warnings = []\n",
    "\n",
    "    # Check 1: Predicted vs value_indicator ratio\n",
    "    if 'predicted' in pred_df.columns and 'value_indicator' in pred_df.columns:\n",
    "        ratio = pred_df['predicted'] / (pred_df['value_indicator'] + 1)\n",
    "\n",
    "        # Extreme ratios indicate unreliable predictions\n",
    "        extreme_high = ratio > 2.0\n",
    "        extreme_low = ratio < 0.6\n",
    "\n",
    "        if extreme_high.sum() > 0:\n",
    "            pred_df.loc[extreme_high, 'confidence_score'] -= 40\n",
    "            pred_df.loc[extreme_high, 'warning_flags'] = pred_df.loc[extreme_high, 'warning_flags'] + 'PRED_MUCH_HIGHER_THAN_VALUE|'\n",
    "\n",
    "        if extreme_low.sum() > 0:\n",
    "            pred_df.loc[extreme_low, 'confidence_score'] -= 30\n",
    "            pred_df.loc[extreme_low, 'warning_flags'] = pred_df.loc[extreme_low, 'warning_flags'] + 'PRED_MUCH_LOWER_THAN_VALUE|'\n",
    "\n",
    "    # Check 2: Census-based value indicators are unreliable\n",
    "    if 'value_source' in pred_df.columns:\n",
    "        census_mask = pred_df['value_source'] == 'census'\n",
    "        if census_mask.sum() > 0:\n",
    "            pred_df.loc[census_mask, 'confidence_score'] -= 25\n",
    "            pred_df.loc[census_mask, 'warning_flags'] = pred_df.loc[census_mask, 'warning_flags'] + 'CENSUS_VALUE_UNRELIABLE|'\n",
    "\n",
    "    # Check 3: Properties with anomaly flags\n",
    "    if 'anomaly_flag' in pred_df.columns:\n",
    "        anomaly_mask = pred_df['anomaly_flag'] > 0\n",
    "        if anomaly_mask.sum() > 0:\n",
    "            pred_df.loc[anomaly_mask, 'confidence_score'] -= 20\n",
    "            pred_df.loc[anomaly_mask, 'warning_flags'] = pred_df.loc[anomaly_mask, 'warning_flags'] + 'FEATURES_NORMALIZED|'\n",
    "\n",
    "    # Check 4: Missing key features (if we had them in training)\n",
    "    if 'prior_sale_price' in pred_df.columns:\n",
    "        missing_prior = pred_df['prior_sale_price'].isna() | (pred_df['prior_sale_price'] == pred_df['prior_sale_price'].median())\n",
    "        if missing_prior.sum() > 0:\n",
    "            pred_df.loc[missing_prior, 'confidence_score'] -= 15\n",
    "            pred_df.loc[missing_prior, 'warning_flags'] = pred_df.loc[missing_prior, 'warning_flags'] + 'NO_PRIOR_SALE|'\n",
    "\n",
    "    # Clean up warning flags\n",
    "    pred_df['warning_flags'] = pred_df['warning_flags'].str.rstrip('|')\n",
    "    pred_df['warning_flags'] = pred_df['warning_flags'].replace('', 'NONE')\n",
    "\n",
    "    # Ensure confidence doesn't go below 0\n",
    "    pred_df['confidence_score'] = pred_df['confidence_score'].clip(lower=0)\n",
    "\n",
    "    # Add recommendation\n",
    "    pred_df['recommendation'] = 'USE'\n",
    "    pred_df.loc[pred_df['confidence_score'] < 40, 'recommendation'] = 'CAUTION'\n",
    "    pred_df.loc[pred_df['confidence_score'] < 20, 'recommendation'] = 'DO_NOT_USE'\n",
    "\n",
    "    return pred_df\n",
    "\n",
    "def train_single_model(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining SINGLE GLOBAL MODEL on {len(df):,} properties\")\n",
    "\n",
    "    # Engineer price features for anomaly detection\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "\n",
    "    # PREPROCESSING: Detect and normalize anomalies\n",
    "    df = detect_and_normalize_anomalies(df, y_col, for_training=True)\n",
    "\n",
    "    # Basic filtering\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)].drop(columns=['price_per_sqft'])\n",
    "    if 'lot_sqft' in df.columns: df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "    if 'year_built' in df.columns: df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "\n",
    "    print(f\"  After filtering: {len(df):,} properties\")\n",
    "\n",
    "    # Split train/test\n",
    "    train_idx = df.sample(frac=1-TEST_SIZE, random_state=RAND_STATE).index\n",
    "    train_df = df.loc[train_idx].copy()\n",
    "    test_df = df.loc[df.index.difference(train_idx)].copy()\n",
    "\n",
    "    # Add cluster stats from training data\n",
    "    cluster_stats = None\n",
    "    if 'geo_cluster' in train_df.columns:\n",
    "        cluster_stats = train_df.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "        cluster_stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "        train_df = add_cluster_feats(train_df, cluster_stats)\n",
    "        test_df = add_cluster_feats(test_df, cluster_stats)\n",
    "\n",
    "    X_tr, y_tr = train_df[feats].values, train_df[y_col].values\n",
    "    X_te, y_te = test_df[feats].values, test_df[y_col].values\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MODEL TRAINING\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Training: {len(X_tr):,} | Test: {len(X_te):,}\")\n",
    "\n",
    "    # Train single model\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=N_EST,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RAND_STATE,\n",
    "        n_jobs=N_JOBS,\n",
    "        tree_method='hist'\n",
    "    ).fit(X_tr, y_tr, verbose=False)\n",
    "\n",
    "    y_pred = model.predict(X_te)\n",
    "\n",
    "    mae = mean_absolute_error(y_te, y_pred)\n",
    "    mape = np.mean(np.abs((y_te-y_pred)/y_te))*100\n",
    "    r2 = r2_score(y_te, y_pred)\n",
    "\n",
    "    print(f\"  Results: MAE=${mae:,.0f} | MAPE={mape:.2f}% | R²={r2:.4f}\")\n",
    "\n",
    "    # Feature importance\n",
    "    scores = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    imp = [(feats[int(k[1:])],v) for k,v in scores.items() if int(k[1:])<len(feats)]\n",
    "    imp.sort(key=lambda x: x[1], reverse=True)\n",
    "    total = sum(v for _,v in imp)\n",
    "    importance_df = pd.DataFrame([\n",
    "        {'feature':f,'gain':g,'importance':g/total} for f,g in imp[:20]\n",
    "    ]) if total>0 else pd.DataFrame(columns=['feature','gain','importance'])\n",
    "\n",
    "    # Predictions\n",
    "    ids = test_df[id_col].values\n",
    "    states = test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "    value_ind = test_df['value_indicator'].values if 'value_indicator' in test_df.columns else [np.nan]*len(test_df)\n",
    "    value_src = test_df['_value_source'].values if '_value_source' in test_df.columns else ['unknown']*len(test_df)\n",
    "    anomaly_flag = test_df['anomaly_flag'].values if 'anomaly_flag' in test_df.columns else [0]*len(test_df)\n",
    "\n",
    "    preds_df = pd.DataFrame({\n",
    "        'property_id':ids, 'state':states, 'value_indicator':value_ind, 'value_source':value_src,\n",
    "        'anomaly_flag':anomaly_flag,\n",
    "        'actual':y_te, 'predicted':y_pred,\n",
    "        'error':y_te-y_pred,\n",
    "        'pct_error':100*(y_te-y_pred)/y_te\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'metrics': {'mae':mae, 'mape':mape, 'r2':r2, 'n_train':len(X_tr), 'n_test':len(X_te)},\n",
    "        'predictions': preds_df,\n",
    "        'feature_importance': importance_df,\n",
    "        'cluster_stats': cluster_stats\n",
    "    }\n",
    "\n",
    "def predict_new(pred_df, model, feats, y_col, id_col, state_col, kmeans, cluster_stats):\n",
    "    print(f\"\\n{'='*60}\\nPREDICTING {len(pred_df):,} NEW PROPERTIES\\n{'='*60}\")\n",
    "\n",
    "    pred_df = engineer(pred_df, y_col)\n",
    "    pred_df, _ = geo_cluster(pred_df, kmeans)\n",
    "    pred_df = create_value_indicator(pred_df)\n",
    "    pred_df = add_cluster_feats(pred_df, cluster_stats)\n",
    "\n",
    "    # AGGRESSIVE anomaly detection\n",
    "    pred_df = detect_and_normalize_anomalies(pred_df, y_col, for_training=False)\n",
    "\n",
    "    # Fill missing features\n",
    "    for f in feats:\n",
    "        if f not in pred_df.columns: pred_df[f] = 0\n",
    "        else: pred_df[f] = pred_df[f].fillna(pred_df[f].median() if pred_df[f].notna().sum()>0 else 0)\n",
    "\n",
    "    if '_value_source' in pred_df.columns:\n",
    "        print(f\"\\nValue sources: {dict(pred_df['_value_source'].value_counts())}\")\n",
    "\n",
    "    if 'anomaly_flag' in pred_df.columns:\n",
    "        anomaly_cnt = (pred_df['anomaly_flag'] > 0).sum()\n",
    "        if anomaly_cnt > 0:\n",
    "            print(f\"⚠️  {anomaly_cnt} properties had features normalized due to anomalies\")\n",
    "\n",
    "    X = pred_df[feats].values\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    ids = pred_df[id_col].values\n",
    "    states = pred_df[state_col].values if state_col and state_col in pred_df.columns else ['Unknown']*len(pred_df)\n",
    "    actual = pred_df[y_col].values if y_col in pred_df.columns else [np.nan]*len(pred_df)\n",
    "    value_ind = pred_df['value_indicator'].values\n",
    "    value_src = pred_df['_value_source'].values\n",
    "    anomaly_flag = pred_df['anomaly_flag'].values if 'anomaly_flag' in pred_df.columns else [0]*len(pred_df)\n",
    "    anomaly_reason = pred_df['anomaly_reason'].values if 'anomaly_reason' in pred_df.columns else ['none']*len(pred_df)\n",
    "\n",
    "    result = pd.DataFrame({\n",
    "        'property_id':ids, 'state':states, 'value_indicator':value_ind, 'value_source':value_src,\n",
    "        'anomaly_flag':anomaly_flag, 'anomaly_reason':anomaly_reason,\n",
    "        'actual':actual, 'predicted':y_pred,\n",
    "        'error':[actual[i]-y_pred[i] if not np.isnan(actual[i]) else np.nan for i in range(len(actual))],\n",
    "        'pct_error':[100*(actual[i]-y_pred[i])/actual[i] if not np.isnan(actual[i]) and actual[i]!=0 else np.nan for i in range(len(actual))]\n",
    "    })\n",
    "\n",
    "    # ADD CONFIDENCE SCORING\n",
    "    result = add_prediction_confidence(result)\n",
    "\n",
    "    print(f\"\\n✓ Generated {len(result):,} predictions\")\n",
    "\n",
    "    # Show confidence summary\n",
    "    if 'confidence_score' in result.columns:\n",
    "        print(f\"\\nConfidence Summary:\")\n",
    "        print(f\"  High confidence (60-100): {(result['confidence_score'] >= 60).sum()}\")\n",
    "        print(f\"  Medium confidence (40-59): {((result['confidence_score'] >= 40) & (result['confidence_score'] < 60)).sum()}\")\n",
    "        print(f\"  Low confidence (0-39): {(result['confidence_score'] < 40).sum()}\")\n",
    "\n",
    "    valid = result['actual'].notna().sum()\n",
    "    if valid>0:\n",
    "        valid_df = result[result['actual'].notna()]\n",
    "        mae = mean_absolute_error(valid_df['actual'], valid_df['predicted'])\n",
    "        mape = np.mean(np.abs((valid_df['actual']-valid_df['predicted'])/valid_df['actual']))*100\n",
    "        r2 = r2_score(valid_df['actual'], valid_df['predicted'])\n",
    "        print(f\"\\nValidation ({valid}): MAE=${mae:,.0f} | MAPE={mape:.2f}% | R²={r2:.4f}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def save_results(results, out_dir, new_preds=None):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi = results['predictions'], results['metrics'], results['feature_importance']\n",
    "\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    ws = wb.create_sheet(\"Summary\", 0)\n",
    "    ws['A1'].font, ws['A1'].value = Font(bold=True,size=14), 'AGGRESSIVE ANOMALY DETECTION + SINGLE MODEL AVM'\n",
    "    ws['A2'].font, ws['A2'].value = Font(italic=True,size=10), 'Tighter thresholds → census penalty → confidence scoring → XGBoost'\n",
    "\n",
    "    data = [\n",
    "        ['Metric','Value'],\n",
    "        ['Train Properties',metrics['n_train']],\n",
    "        ['Test Properties',metrics['n_test']],\n",
    "        ['R²',f\"{metrics['r2']:.4f}\"],\n",
    "        ['MAE',f\"${metrics['mae']:,.0f}\"],\n",
    "        ['MAPE%',f\"{metrics['mape']:.2f}%\"]\n",
    "    ]\n",
    "    if new_preds is not None: data.append(['New Predictions',len(new_preds)])\n",
    "\n",
    "    for i,(k,v) in enumerate(data,5):\n",
    "        ws[f'A{i}'].font, ws[f'A{i}'].value, ws[f'B{i}'].value = Font(bold=True), k, v\n",
    "\n",
    "    ws = wb.create_sheet(\"Feature_Importance\")\n",
    "    for r_idx,row in enumerate(dataframe_to_rows(fi,index=False,header=True),1):\n",
    "        for c_idx,value in enumerate(row,1):\n",
    "            cell = ws.cell(row=r_idx,column=c_idx,value=value)\n",
    "            if r_idx==1:\n",
    "                cell.font = Font(bold=True,color='FFFFFF')\n",
    "                cell.fill = PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "\n",
    "    for sheet_name,data,color in [('Test_Predictions',preds,'366092'),('New_Predictions',new_preds,'4472C4')]:\n",
    "        if data is None: continue\n",
    "        ws = wb.create_sheet(sheet_name)\n",
    "        for i,h in enumerate(data.columns,1):\n",
    "            c = ws.cell(1,i,h)\n",
    "            c.font, c.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color=color,end_color=color,fill_type='solid')\n",
    "        for i,row in enumerate(data.itertuples(index=False),2):\n",
    "            for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/aggressive_avm_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "\n",
    "    preds.to_csv(f\"{out_dir}/aggressive_test_predictions_{ts}.csv\", index=False)\n",
    "    fi.to_csv(f\"{out_dir}/aggressive_importance_{ts}.csv\", index=False)\n",
    "    if new_preds is not None:\n",
    "        new_preds.to_csv(f\"{out_dir}/aggressive_new_predictions_{ts}.csv\", index=False)\n",
    "\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60+\"\\nAGGRESSIVE ANOMALY DETECTION + SINGLE MODEL AVM\\nTighter Thresholds → Census Penalty → Confidence Scoring\\n\"+\"=\"*60)\n",
    "\n",
    "    # Load and prepare training data\n",
    "    df, y_col, id_col, state_col = load_data(TRAINING_INPUT_PATH)\n",
    "    df = df[df[y_col]>=MIN_PRICE]\n",
    "    df = engineer(df, y_col)\n",
    "    df, kmeans = geo_cluster(df)\n",
    "    df = create_value_indicator(df)\n",
    "\n",
    "    # Define features\n",
    "    base = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\n",
    "            \"latitude\",\"longitude\",\"geo_cluster\",\"value_indicator\",\"log_value_indicator\"]\n",
    "    eng = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\"]\n",
    "    prior = [\"prior_sale_price\",\"prior_price_per_sqft\",\"prior_appreciated\",\"years_since_last_sale\",\"has_prior_sale\",\"recently_sold\"]\n",
    "    cluster = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "    census = [\"median_household_income\",\"median_home_value\",\"pct_bachelors_degree\",\"income_education_score\"]\n",
    "\n",
    "    all_feats = base + eng + prior + cluster + census\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "    df = df.dropna(subset=[y_col])\n",
    "\n",
    "    # Train model\n",
    "    results = train_single_model(df, feats, y_col, id_col, state_col)\n",
    "\n",
    "    # Predict new properties\n",
    "    new_preds = None\n",
    "    if PREDICTION_INPUT_PATH:\n",
    "        pred_df, _, _, _ = load_data(PREDICTION_INPUT_PATH)\n",
    "        new_preds = predict_new(pred_df, results['model'], feats, y_col, id_col, state_col,\n",
    "                               kmeans, results['cluster_stats'])\n",
    "\n",
    "    # Save results\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR, new_preds)\n",
    "\n",
    "    print(f\"\\n{'='*60}\\n✓ COMPLETE in {time.time()-t0:.1f}s\")\n",
    "    if new_preds is not None: print(f\"  New predictions: {len(new_preds):,}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__==\"__main__\": main()"
   ],
   "id": "19b1760e358f6110",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AGGRESSIVE ANOMALY DETECTION + SINGLE MODEL AVM\n",
      "Tighter Thresholds → Census Penalty → Confidence Scoring\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 90.4MB | Price:sale_price ID:property_id\n",
      "  ⚠️  Filtered 29,431 bad assessed values\n",
      "  ⚠️  Filtered 29,431 bad assessed values\n",
      "24/32 features available\n",
      "\n",
      "Training SINGLE GLOBAL MODEL on 127,258 properties\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING: Aggressive Anomaly Detection\n",
      "============================================================\n",
      "  ⚠️  Found 60831 properties with extreme price/value ratios\n",
      "      These will be EXCLUDED from training (likely data errors)\n",
      "  ✓ Detected 148,985 anomalies across 32 cohorts\n",
      "  ✓ Excluded 60,831 extreme anomalies from training\n",
      "  After filtering: 56,793 properties\n",
      "\n",
      "============================================================\n",
      "MODEL TRAINING\n",
      "============================================================\n",
      "  Training: 39,755 | Test: 17,038\n",
      "  Results: MAE=$230,548 | MAPE=16.38% | R²=0.7812\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\n",
      "1 records | 0.0MB | Price:sale_price ID:property_id\n",
      "\n",
      "============================================================\n",
      "PREDICTING 1 NEW PROPERTIES\n",
      "============================================================\n",
      "  ⚠️  Filtered 1 bad assessed values\n",
      "  ⚠️  Filtered 1 bad assessed values\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING: Aggressive Anomaly Detection\n",
      "============================================================\n",
      "⚠️  Skipping cohort-based anomaly detection - only 1 properties (need 10+)\n",
      "\n",
      "Value sources: {'census': np.int64(1)}\n",
      "\n",
      "✓ Generated 1 predictions\n",
      "\n",
      "Confidence Summary:\n",
      "  High confidence (60-100): 1\n",
      "  Medium confidence (40-59): 0\n",
      "  Low confidence (0-39): 0\n",
      "\n",
      "Validation (1): MAE=$130,194 | MAPE=24.75% | R²=nan\n",
      "\n",
      "Saving results...\n",
      "✓ Excel: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/aggressive_avm_20251224_014636.xlsx\n",
      "✓ CSVs saved\n",
      "\n",
      "============================================================\n",
      "✓ COMPLETE in 3.1s\n",
      "  New predictions: 1\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T07:24:20.306691Z",
     "start_time": "2025-12-24T07:24:17.016752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings, time, os\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Config\n",
    "MIN_PRICE, TEST_SIZE, RAND_STATE, N_JOBS, N_CLUSTERS = 20000, 0.3, 42, -1, 8\n",
    "N_EST = 200  # More trees for single model\n",
    "\n",
    "TRAINING_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\"\n",
    "PREDICTION_INPUT_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for c in df.select_dtypes(['float64']).columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes(['int64']).columns:\n",
    "        df[c] = df[c].astype('int8' if set(df[c].dropna().unique()).issubset({0,1}) else 'int32')\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = optimize_dtypes(pd.read_csv(path, low_memory=False))\n",
    "    df.columns = df.columns.str.lower()\n",
    "    y_col = next((c for c in ['sale_price','currentsalesprice','price','saleprice'] if c in df.columns), 'currentsalesprice')\n",
    "    id_col = next((c for c in ['cc_list_id','property_id','propertyid','id'] if c in df.columns), 'cc_list_id')\n",
    "    state_col = next((c for c in ['sample_state','state','state_code'] if c in df.columns), None)\n",
    "    print(f\"{len(df):,} records | {df.memory_usage(deep=True).sum()/1024**2:.1f}MB | Price:{y_col} ID:{id_col}\")\n",
    "    return df, y_col, id_col, state_col\n",
    "\n",
    "def filter_bad_assessed(df):\n",
    "    if 'assessed_total_value' not in df.columns:\n",
    "        return pd.Series([False]*len(df), index=df.index)\n",
    "\n",
    "    valid = (df['assessed_total_value']>10000) & (df['assessed_total_value']<100000000)\n",
    "\n",
    "    if 'living_sqft' in df.columns:\n",
    "        has_sqft = (df['living_sqft'].notna()) & (df['living_sqft']>100)\n",
    "        ppsf = df['assessed_total_value']/df['living_sqft']\n",
    "        valid &= ~(has_sqft & ((ppsf<20)|(ppsf>2000)))\n",
    "\n",
    "    if 'prior_sale_price' in df.columns and 'years_since_last_sale' in df.columns:\n",
    "        has_prior = (df['prior_sale_price']>10000) & (df['years_since_last_sale']<=15)\n",
    "        valid &= ~(has_prior & ((df['assessed_total_value']/df['prior_sale_price'])<0.10))\n",
    "\n",
    "    if 'median_home_value' in df.columns:\n",
    "        has_census = (df['median_home_value']>10000)\n",
    "        valid &= ~(has_census & ((df['assessed_total_value']/df['median_home_value'])<0.05))\n",
    "\n",
    "    invalid_cnt = ((df['assessed_total_value'].notna()) & ~valid).sum()\n",
    "    if invalid_cnt>0: print(f\"  ⚠️  Filtered {invalid_cnt:,} bad assessed values\")\n",
    "\n",
    "    return valid\n",
    "\n",
    "def create_value_indicator(df):\n",
    "    \"\"\"Create value_indicator from multiple sources - NO LEAKAGE\"\"\"\n",
    "    value_ind = pd.Series([np.nan]*len(df), index=df.index)\n",
    "    source = pd.Series(['none']*len(df), index=df.index)\n",
    "\n",
    "    # Priority 1: Prior sale PPSF × sqft\n",
    "    if all(c in df.columns for c in ['prior_sale_price','years_since_last_sale','living_sqft']):\n",
    "        has_prior = (df['prior_sale_price']>10000) & (df['years_since_last_sale']<=15) & (df['living_sqft']>400)\n",
    "        if has_prior.sum()>0:\n",
    "            prior_ppsf = df.loc[has_prior,'prior_sale_price'] / (df.loc[has_prior,'living_sqft']+1)\n",
    "            yrs = df.loc[has_prior,'years_since_last_sale'].fillna(5)\n",
    "            appreciated_ppsf = prior_ppsf * (1.04 ** yrs)\n",
    "            value_ind[has_prior] = appreciated_ppsf * df.loc[has_prior,'living_sqft']\n",
    "            source[has_prior] = 'prior_ppsf'\n",
    "\n",
    "    # Priority 2: Assessed value\n",
    "    if 'assessed_total_value' in df.columns:\n",
    "        valid = filter_bad_assessed(df) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            vals = df.loc[valid,'assessed_total_value']\n",
    "            mult = pd.Series([1.15]*len(vals), index=vals.index)\n",
    "            mult[vals<200000], mult[vals>=500000] = 1.1, 1.2\n",
    "            value_ind[valid], source[valid] = vals*mult, 'assessed'\n",
    "\n",
    "    # Priority 3: Census median\n",
    "    if 'median_home_value' in df.columns:\n",
    "        valid = (df['median_home_value']>10000) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            value_ind[valid], source[valid] = df.loc[valid,'median_home_value'], 'census'\n",
    "\n",
    "    # Priority 4: Geo cluster median\n",
    "    if 'cluster_med_price' in df.columns:\n",
    "        valid = (df['cluster_med_price']>10000) & value_ind.isna()\n",
    "        if valid.sum()>0:\n",
    "            value_ind[valid], source[valid] = df.loc[valid,'cluster_med_price'], 'cluster'\n",
    "\n",
    "    # Fallback\n",
    "    value_ind = value_ind.fillna(value_ind.median())\n",
    "    source[value_ind.isna()] = 'global_median'\n",
    "\n",
    "    df['value_indicator'] = value_ind\n",
    "    df['log_value_indicator'] = np.log1p(value_ind)\n",
    "    df['_value_source'] = source\n",
    "\n",
    "    return df\n",
    "\n",
    "def engineer(df, y_col, with_price=False):\n",
    "    if 'living_sqft' in df.columns:\n",
    "        if 'bedrooms' in df.columns: df['sqft_per_bedroom'] = df['living_sqft']/(df['bedrooms']+1)\n",
    "        df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "\n",
    "    if 'lot_sqft' in df.columns:\n",
    "        if 'living_sqft' in df.columns: df['lot_to_living_ratio'] = df['lot_sqft']/(df['living_sqft']+1)\n",
    "        if 'lot_acres' not in df.columns: df['lot_acres'] = df['lot_sqft']/43560\n",
    "\n",
    "    if 'year_built' in df.columns:\n",
    "        df['property_age'] = 2024-df['year_built']\n",
    "        df['is_new'] = ((2024-df['year_built'])<=5).astype('int8')\n",
    "        df['age_squared'] = (2024-df['year_built'])**2\n",
    "\n",
    "    if 'garage_spaces' in df.columns:\n",
    "        df['has_garage'] = (df['garage_spaces']>0).astype('int8')\n",
    "\n",
    "    lux = [df[c]/1000 if c=='living_sqft' else df[c] for c in ['living_sqft','full_baths','garage_spaces'] if c in df.columns]\n",
    "    df['luxury_score'] = sum(lux)/len(lux) if lux else 0\n",
    "\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        if 'years_since_last_sale' in df.columns:\n",
    "            df['prior_appreciated'] = df['prior_sale_price']*(1.04**df['years_since_last_sale'].fillna(5))\n",
    "        if 'living_sqft' in df.columns:\n",
    "            df['prior_price_per_sqft'] = df['prior_sale_price']/(df['living_sqft']+1)\n",
    "        df['has_prior_sale'] = df['prior_sale_price'].notna().astype('int8')\n",
    "\n",
    "    if 'assessed_total_value' in df.columns and 'living_sqft' in df.columns:\n",
    "        valid = filter_bad_assessed(df)\n",
    "        df['assessed_per_sqft'] = 0\n",
    "        df.loc[valid,'assessed_per_sqft'] = df.loc[valid,'assessed_total_value']/(df.loc[valid,'living_sqft']+1)\n",
    "\n",
    "    if 'median_household_income' in df.columns and 'pct_bachelors_degree' in df.columns:\n",
    "        df['income_education_score'] = df['median_household_income']*df['pct_bachelors_degree']\n",
    "\n",
    "    if 'prior_sale_date' in df.columns:\n",
    "        df['prior_sale_date'] = pd.to_datetime(df['prior_sale_date'], errors='coerce')\n",
    "        df['years_since_last_sale'] = (pd.Timestamp('2024-01-01')-df['prior_sale_date']).dt.days/365.25\n",
    "        df['recently_sold'] = (df['years_since_last_sale']<2).astype('int8')\n",
    "\n",
    "    if 'years_since_last_sale' in df.columns:\n",
    "        df['years_since_last_sale'] = df['years_since_last_sale'].fillna(999)\n",
    "\n",
    "    if 'prior_sale_price' in df.columns:\n",
    "        missing = df['prior_sale_price'].isna()\n",
    "        df.loc[missing,'prior_sale_price'] = df['prior_sale_price'].median()\n",
    "\n",
    "    if with_price and y_col in df.columns:\n",
    "        if 'living_sqft' in df.columns:\n",
    "            df['price_per_sqft'] = df[y_col]/(df['living_sqft']+1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def geo_cluster(df, kmeans=None):\n",
    "    if not all(c in df.columns for c in ['latitude','longitude']):\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans\n",
    "\n",
    "    valid = df[['latitude','longitude']].notna().all(axis=1)\n",
    "    if valid.sum()<N_CLUSTERS:\n",
    "        df['geo_cluster'] = 0\n",
    "        return df, kmeans\n",
    "\n",
    "    df['geo_cluster'] = 0\n",
    "    if kmeans is None:\n",
    "        kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RAND_STATE, batch_size=1000, n_init=3)\n",
    "        df.loc[valid,'geo_cluster'] = kmeans.fit_predict(df.loc[valid,['latitude','longitude']])\n",
    "    else:\n",
    "        df.loc[valid,'geo_cluster'] = kmeans.predict(df.loc[valid,['latitude','longitude']])\n",
    "\n",
    "    return df, kmeans\n",
    "\n",
    "def add_cluster_feats(df, cluster_stats):\n",
    "    if cluster_stats is None or 'geo_cluster' not in df.columns:\n",
    "        df['cluster_avg_price'] = df['cluster_med_price'] = 0\n",
    "        return df\n",
    "\n",
    "    df = df.merge(cluster_stats, on='geo_cluster', how='left')\n",
    "    med = cluster_stats['cluster_avg_price'].median()\n",
    "    df['cluster_avg_price'] = df['cluster_avg_price'].fillna(med)\n",
    "    df['cluster_med_price'] = df['cluster_med_price'].fillna(med)\n",
    "    return df\n",
    "\n",
    "def detect_and_normalize_anomalies(df, y_col, for_training=True):\n",
    "    \"\"\"\n",
    "    AGGRESSIVE neighborhood-level anomaly detection and normalization.\n",
    "    Detects properties with extreme feature combinations relative to their cohort.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PREPROCESSING: Aggressive Anomaly Detection\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    df['anomaly_flag'] = 0\n",
    "    df['anomaly_reason'] = 'none'\n",
    "\n",
    "    # Create cohorts: geo_cluster + value_indicator quartile\n",
    "    if 'geo_cluster' not in df.columns or 'value_indicator' not in df.columns:\n",
    "        print(\"⚠️  Skipping anomaly detection - missing clustering features\")\n",
    "        return df\n",
    "\n",
    "    # Skip for very small datasets\n",
    "    if len(df) < 10:\n",
    "        print(f\"⚠️  Skipping cohort-based anomaly detection - only {len(df)} properties (need 10+)\")\n",
    "        return df\n",
    "\n",
    "    # Handle quartile binning errors gracefully\n",
    "    try:\n",
    "        df['value_quartile'] = pd.qcut(\n",
    "            df['value_indicator'],\n",
    "            q=4,\n",
    "            labels=False,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        df['value_quartile'] = df['value_quartile'].map({\n",
    "            0: 'Q1', 1: 'Q2', 2: 'Q3', 3: 'Q4'\n",
    "        }).fillna('Q1')\n",
    "\n",
    "    except (ValueError, TypeError) as e:\n",
    "        print(f\"⚠️  Could not create quartiles ({str(e)}), using single cohort\")\n",
    "        df['value_quartile'] = 'Q1'\n",
    "\n",
    "    df['cohort'] = df['geo_cluster'].astype(str) + '_' + df['value_quartile'].astype(str)\n",
    "\n",
    "    # ULTRA AGGRESSIVE feature checks - VERY TIGHT thresholds\n",
    "    feature_checks = {\n",
    "        'living_sqft': ('sqft', 0.7, 1.5),      # Was 0.6-1.8, now 0.7-1.5\n",
    "        'full_baths': ('baths', 0, 7),           # Was 0-8, now 0-7\n",
    "        'luxury_score': ('luxury', 0.5, 2.0),    # Was 0.4-2.5, now 0.5-2.0\n",
    "        'property_age': ('age', -5, 100),        # Was -10-120, now -5-100\n",
    "        'bedrooms': ('beds', 0, 9),              # Was 0-10, now 0-9\n",
    "        'lot_sqft': ('lot', 0.4, 2.5),           # Was 0.3-3.0, now 0.4-2.5\n",
    "    }\n",
    "\n",
    "    anomalies_found = 0\n",
    "\n",
    "    for cohort in df['cohort'].unique():\n",
    "        cohort_df = df[df['cohort']==cohort]\n",
    "        if len(cohort_df) < 10:\n",
    "            continue\n",
    "\n",
    "        cohort_idx = cohort_df.index\n",
    "\n",
    "        # Check each feature for anomalies\n",
    "        for feat, (name, min_mult, max_mult) in feature_checks.items():\n",
    "            if feat not in df.columns:\n",
    "                continue\n",
    "\n",
    "            cohort_vals = cohort_df[feat].dropna()\n",
    "            if len(cohort_vals) < 5:\n",
    "                continue\n",
    "\n",
    "            cohort_median = cohort_vals.median()\n",
    "            cohort_std = cohort_vals.std()\n",
    "\n",
    "            if cohort_median == 0 or cohort_std == 0:\n",
    "                continue\n",
    "\n",
    "            # Method 1: Ratio-based detection (TIGHTER thresholds)\n",
    "            if feat in ['living_sqft', 'luxury_score', 'lot_sqft']:\n",
    "                ratios = df.loc[cohort_idx, feat] / cohort_median\n",
    "                anomaly_mask = (ratios < min_mult) | (ratios > max_mult)\n",
    "\n",
    "                if anomaly_mask.sum() > 0:\n",
    "                    df.loc[cohort_idx[anomaly_mask], feat] = np.clip(\n",
    "                        df.loc[cohort_idx[anomaly_mask], feat],\n",
    "                        cohort_median * min_mult,\n",
    "                        cohort_median * max_mult\n",
    "                    )\n",
    "                    df.loc[cohort_idx[anomaly_mask], 'anomaly_flag'] = 1\n",
    "                    df.loc[cohort_idx[anomaly_mask], 'anomaly_reason'] = f'{name}_extreme'\n",
    "                    anomalies_found += anomaly_mask.sum()\n",
    "\n",
    "            # Method 2: Z-score detection (ULTRA TIGHT - 2.0 std instead of 3)\n",
    "            elif feat in ['property_age', 'full_baths', 'bedrooms']:\n",
    "                z_scores = np.abs((df.loc[cohort_idx, feat] - cohort_median) / (cohort_std + 1))\n",
    "                anomaly_mask = z_scores > 2.0  # Was 2.5, now 2.0\n",
    "\n",
    "                if anomaly_mask.sum() > 0:\n",
    "                    df.loc[cohort_idx[anomaly_mask], feat] = np.clip(\n",
    "                        df.loc[cohort_idx[anomaly_mask], feat],\n",
    "                        cohort_median - 2.0*cohort_std,\n",
    "                        cohort_median + 2.0*cohort_std\n",
    "                    )\n",
    "                    df.loc[cohort_idx[anomaly_mask], 'anomaly_flag'] = 1\n",
    "                    df.loc[cohort_idx[anomaly_mask], 'anomaly_reason'] = f'{name}_outlier'\n",
    "                    anomalies_found += anomaly_mask.sum()\n",
    "\n",
    "    # NEW: Check living_sqft vs bedrooms ratio (TIGHTER)\n",
    "    if 'living_sqft' in df.columns and 'bedrooms' in df.columns:\n",
    "        sqft_per_bed = df['living_sqft'] / (df['bedrooms'] + 1)\n",
    "        # Typical range: 400-1200 sqft/bedroom (was 250-2000)\n",
    "        extreme_ratio = (sqft_per_bed < 300) | (sqft_per_bed > 1500)\n",
    "        if extreme_ratio.sum() > 0:\n",
    "            df.loc[extreme_ratio, 'anomaly_flag'] = 1\n",
    "            df.loc[extreme_ratio, 'anomaly_reason'] = 'sqft_bed_mismatch'\n",
    "            anomalies_found += extreme_ratio.sum()\n",
    "\n",
    "    # EXISTING: Cross-feature consistency (TIGHTER)\n",
    "    if 'living_sqft' in df.columns and 'luxury_score' in df.columns:\n",
    "        sqft_low = df['living_sqft'] < df['living_sqft'].quantile(0.30)  # Was 0.25\n",
    "        luxury_high = df['luxury_score'] > df['luxury_score'].quantile(0.70)  # Was 0.75\n",
    "        inconsistent = sqft_low & luxury_high\n",
    "\n",
    "        if inconsistent.sum() > 0:\n",
    "            df.loc[inconsistent, 'luxury_score'] = df.loc[inconsistent, 'luxury_score'] * 0.5  # Was 0.6\n",
    "            df.loc[inconsistent, 'anomaly_flag'] = 1\n",
    "            df.loc[inconsistent, 'anomaly_reason'] = 'sqft_luxury_mismatch'\n",
    "            anomalies_found += inconsistent.sum()\n",
    "\n",
    "    # NEW: HEAVILY penalize census-based value indicators (they're very unreliable)\n",
    "    if '_value_source' in df.columns:\n",
    "        census_mask = df['_value_source'] == 'census'\n",
    "        if census_mask.sum() > 0:\n",
    "            # AGGRESSIVE: Reduce impact of census values significantly\n",
    "            if 'luxury_score' in df.columns:\n",
    "                df.loc[census_mask, 'luxury_score'] = df.loc[census_mask, 'luxury_score'] * 0.75  # Was 0.85, now 0.75\n",
    "            if 'living_sqft' in df.columns:\n",
    "                median_sqft = df['living_sqft'].median()\n",
    "                # More aggressive sqft reduction for census properties\n",
    "                extreme_sqft = df.loc[census_mask, 'living_sqft'] > median_sqft * 1.3  # Was 1.5, now 1.3\n",
    "                if extreme_sqft.sum() > 0:\n",
    "                    df.loc[census_mask & extreme_sqft, 'living_sqft'] = df.loc[census_mask & extreme_sqft, 'living_sqft'] * 0.85  # Was 0.9, now 0.85\n",
    "\n",
    "            # Also dampen any engineered features that might inflate predictions\n",
    "            if 'sqft_per_bedroom' in df.columns:\n",
    "                df.loc[census_mask, 'sqft_per_bedroom'] = df.loc[census_mask, 'sqft_per_bedroom'] * 0.9\n",
    "\n",
    "            df.loc[census_mask, 'anomaly_flag'] = 1\n",
    "            df.loc[census_mask, 'anomaly_reason'] = 'census_unreliable'\n",
    "            anomalies_found += census_mask.sum()\n",
    "\n",
    "    # TRAINING ONLY: Check price vs value_indicator ratio (ULTRA TIGHT)\n",
    "    if for_training and y_col in df.columns:\n",
    "        df['price_to_value_ratio'] = df[y_col] / (df['value_indicator'] + 1)\n",
    "\n",
    "        # ULTRA TIGHT: ratio >2.0x or <0.6x (was 2.5x and 0.5x)\n",
    "        extreme_ratio = (df['price_to_value_ratio'] > 2.0) | (df['price_to_value_ratio'] < 0.6)\n",
    "\n",
    "        if extreme_ratio.sum() > 0:\n",
    "            print(f\"  ⚠️  Found {extreme_ratio.sum()} properties with extreme price/value ratios\")\n",
    "            print(f\"      These will be EXCLUDED from training (likely data errors)\")\n",
    "            df.loc[extreme_ratio, 'anomaly_flag'] = 2  # Flag 2 = exclude\n",
    "            df.loc[extreme_ratio, 'anomaly_reason'] = 'extreme_price_ratio'\n",
    "            anomalies_found += extreme_ratio.sum()\n",
    "\n",
    "    if anomalies_found > 0:\n",
    "        cohort_count = df['cohort'].nunique() if 'cohort' in df.columns else 0\n",
    "        print(f\"  ✓ Detected {anomalies_found:,} anomalies across {cohort_count} cohorts\")\n",
    "    else:\n",
    "        print(f\"  ✓ No anomalies detected\")\n",
    "\n",
    "    # For training: exclude extreme anomalies (flag=2)\n",
    "    if for_training:\n",
    "        before = len(df)\n",
    "        df = df[df['anomaly_flag'] != 2].copy()\n",
    "        if len(df) < before:\n",
    "            print(f\"  ✓ Excluded {before-len(df):,} extreme anomalies from training\")\n",
    "\n",
    "    # Clean up temporary columns\n",
    "    cols_to_drop = ['value_quartile', 'cohort']\n",
    "    if 'price_to_value_ratio' in df.columns:\n",
    "        cols_to_drop.append('price_to_value_ratio')\n",
    "    df = df.drop(columns=[c for c in cols_to_drop if c in df.columns])\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_prediction_confidence(pred_df):\n",
    "    \"\"\"\n",
    "    Add a confidence score and warning flags for predictions.\n",
    "    Helps identify predictions that are likely to be unreliable.\n",
    "    \"\"\"\n",
    "    pred_df['confidence_score'] = 100  # Start at 100%\n",
    "    pred_df['warning_flags'] = ''\n",
    "\n",
    "    warnings = []\n",
    "\n",
    "    # Check 1: Predicted vs value_indicator ratio (TIGHTER)\n",
    "    if 'predicted' in pred_df.columns and 'value_indicator' in pred_df.columns:\n",
    "        ratio = pred_df['predicted'] / (pred_df['value_indicator'] + 1)\n",
    "\n",
    "        # More aggressive penalties for deviation\n",
    "        extreme_high = ratio > 1.6  # Was 2.0, now 1.6\n",
    "        moderate_high = (ratio > 1.3) & (ratio <= 1.6)  # New moderate tier\n",
    "        extreme_low = ratio < 0.7  # Was 0.6, now 0.7\n",
    "\n",
    "        if extreme_high.sum() > 0:\n",
    "            pred_df.loc[extreme_high, 'confidence_score'] -= 50  # Was 40, now 50\n",
    "            pred_df.loc[extreme_high, 'warning_flags'] = pred_df.loc[extreme_high, 'warning_flags'] + 'PRED_MUCH_HIGHER_THAN_VALUE|'\n",
    "\n",
    "        if moderate_high.sum() > 0:\n",
    "            pred_df.loc[moderate_high, 'confidence_score'] -= 30\n",
    "            pred_df.loc[moderate_high, 'warning_flags'] = pred_df.loc[moderate_high, 'warning_flags'] + 'PRED_HIGHER_THAN_VALUE|'\n",
    "\n",
    "        if extreme_low.sum() > 0:\n",
    "            pred_df.loc[extreme_low, 'confidence_score'] -= 35  # Was 30, now 35\n",
    "            pred_df.loc[extreme_low, 'warning_flags'] = pred_df.loc[extreme_low, 'warning_flags'] + 'PRED_MUCH_LOWER_THAN_VALUE|'\n",
    "\n",
    "    # Check 2: Census-based value indicators are VERY unreliable (more aggressive)\n",
    "    if 'value_source' in pred_df.columns:\n",
    "        census_mask = pred_df['value_source'] == 'census'\n",
    "        if census_mask.sum() > 0:\n",
    "            pred_df.loc[census_mask, 'confidence_score'] -= 35  # Was 25, now 35\n",
    "            pred_df.loc[census_mask, 'warning_flags'] = pred_df.loc[census_mask, 'warning_flags'] + 'CENSUS_VALUE_UNRELIABLE|'\n",
    "\n",
    "    # Check 3: Properties with anomaly flags\n",
    "    if 'anomaly_flag' in pred_df.columns:\n",
    "        anomaly_mask = pred_df['anomaly_flag'] > 0\n",
    "        if anomaly_mask.sum() > 0:\n",
    "            pred_df.loc[anomaly_mask, 'confidence_score'] -= 20\n",
    "            pred_df.loc[anomaly_mask, 'warning_flags'] = pred_df.loc[anomaly_mask, 'warning_flags'] + 'FEATURES_NORMALIZED|'\n",
    "\n",
    "    # Check 4: Missing key features (if we had them in training)\n",
    "    if 'prior_sale_price' in pred_df.columns:\n",
    "        missing_prior = pred_df['prior_sale_price'].isna() | (pred_df['prior_sale_price'] == pred_df['prior_sale_price'].median())\n",
    "        if missing_prior.sum() > 0:\n",
    "            pred_df.loc[missing_prior, 'confidence_score'] -= 15\n",
    "            pred_df.loc[missing_prior, 'warning_flags'] = pred_df.loc[missing_prior, 'warning_flags'] + 'NO_PRIOR_SALE|'\n",
    "\n",
    "    # Clean up warning flags\n",
    "    pred_df['warning_flags'] = pred_df['warning_flags'].str.rstrip('|')\n",
    "    pred_df['warning_flags'] = pred_df['warning_flags'].replace('', 'NONE')\n",
    "\n",
    "    # Ensure confidence doesn't go below 0\n",
    "    pred_df['confidence_score'] = pred_df['confidence_score'].clip(lower=0)\n",
    "\n",
    "    # Add recommendation (stricter thresholds)\n",
    "    pred_df['recommendation'] = 'USE'\n",
    "    pred_df.loc[pred_df['confidence_score'] < 50, 'recommendation'] = 'CAUTION'  # Was 40, now 50\n",
    "    pred_df.loc[pred_df['confidence_score'] < 30, 'recommendation'] = 'DO_NOT_USE'  # Was 20, now 30\n",
    "\n",
    "    return pred_df\n",
    "\n",
    "def train_single_model(df, feats, y_col, id_col, state_col):\n",
    "    print(f\"\\nTraining SINGLE GLOBAL MODEL on {len(df):,} properties\")\n",
    "\n",
    "    # Engineer price features for anomaly detection\n",
    "    df = engineer(df, y_col, with_price=True)\n",
    "\n",
    "    # PREPROCESSING: Detect and normalize anomalies\n",
    "    df = detect_and_normalize_anomalies(df, y_col, for_training=True)\n",
    "\n",
    "    # Basic filtering\n",
    "    if 'price_per_sqft' in df.columns:\n",
    "        lb,ub = df['price_per_sqft'].quantile([.05,.95])\n",
    "        df = df[(df['price_per_sqft']>=lb)&(df['price_per_sqft']<=ub)].drop(columns=['price_per_sqft'])\n",
    "    if 'lot_sqft' in df.columns: df = df[df['lot_sqft']<=df['lot_sqft'].quantile(.98)]\n",
    "    if 'year_built' in df.columns: df = df[(df['year_built']>=1900)&(df['year_built']<=2025)]\n",
    "\n",
    "    print(f\"  After filtering: {len(df):,} properties\")\n",
    "\n",
    "    # Split train/test\n",
    "    train_idx = df.sample(frac=1-TEST_SIZE, random_state=RAND_STATE).index\n",
    "    train_df = df.loc[train_idx].copy()\n",
    "    test_df = df.loc[df.index.difference(train_idx)].copy()\n",
    "\n",
    "    # Add cluster stats from training data\n",
    "    cluster_stats = None\n",
    "    if 'geo_cluster' in train_df.columns:\n",
    "        cluster_stats = train_df.groupby('geo_cluster')[y_col].agg(['mean','median']).reset_index()\n",
    "        cluster_stats.columns = ['geo_cluster','cluster_avg_price','cluster_med_price']\n",
    "        train_df = add_cluster_feats(train_df, cluster_stats)\n",
    "        test_df = add_cluster_feats(test_df, cluster_stats)\n",
    "\n",
    "    X_tr, y_tr = train_df[feats].values, train_df[y_col].values\n",
    "    X_te, y_te = test_df[feats].values, test_df[y_col].values\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MODEL TRAINING\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Training: {len(X_tr):,} | Test: {len(X_te):,}\")\n",
    "\n",
    "    # Train single model\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=N_EST,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RAND_STATE,\n",
    "        n_jobs=N_JOBS,\n",
    "        tree_method='hist'\n",
    "    ).fit(X_tr, y_tr, verbose=False)\n",
    "\n",
    "    y_pred = model.predict(X_te)\n",
    "\n",
    "    mae = mean_absolute_error(y_te, y_pred)\n",
    "    mape = np.mean(np.abs((y_te-y_pred)/y_te))*100\n",
    "    r2 = r2_score(y_te, y_pred)\n",
    "\n",
    "    print(f\"  Results: MAE=${mae:,.0f} | MAPE={mape:.2f}% | R²={r2:.4f}\")\n",
    "\n",
    "    # Feature importance\n",
    "    scores = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    imp = [(feats[int(k[1:])],v) for k,v in scores.items() if int(k[1:])<len(feats)]\n",
    "    imp.sort(key=lambda x: x[1], reverse=True)\n",
    "    total = sum(v for _,v in imp)\n",
    "    importance_df = pd.DataFrame([\n",
    "        {'feature':f,'gain':g,'importance':g/total} for f,g in imp[:20]\n",
    "    ]) if total>0 else pd.DataFrame(columns=['feature','gain','importance'])\n",
    "\n",
    "    # Predictions\n",
    "    ids = test_df[id_col].values\n",
    "    states = test_df[state_col].values if state_col and state_col in test_df.columns else ['Unknown']*len(test_df)\n",
    "    value_ind = test_df['value_indicator'].values if 'value_indicator' in test_df.columns else [np.nan]*len(test_df)\n",
    "    value_src = test_df['_value_source'].values if '_value_source' in test_df.columns else ['unknown']*len(test_df)\n",
    "    anomaly_flag = test_df['anomaly_flag'].values if 'anomaly_flag' in test_df.columns else [0]*len(test_df)\n",
    "\n",
    "    preds_df = pd.DataFrame({\n",
    "        'property_id':ids, 'state':states, 'value_indicator':value_ind, 'value_source':value_src,\n",
    "        'anomaly_flag':anomaly_flag,\n",
    "        'actual':y_te, 'predicted':y_pred,\n",
    "        'error':y_te-y_pred,\n",
    "        'pct_error':100*(y_te-y_pred)/y_te\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'metrics': {'mae':mae, 'mape':mape, 'r2':r2, 'n_train':len(X_tr), 'n_test':len(X_te)},\n",
    "        'predictions': preds_df,\n",
    "        'feature_importance': importance_df,\n",
    "        'cluster_stats': cluster_stats\n",
    "    }\n",
    "\n",
    "def predict_new(pred_df, model, feats, y_col, id_col, state_col, kmeans, cluster_stats):\n",
    "    print(f\"\\n{'='*60}\\nPREDICTING {len(pred_df):,} NEW PROPERTIES\\n{'='*60}\")\n",
    "\n",
    "    pred_df = engineer(pred_df, y_col)\n",
    "    pred_df, _ = geo_cluster(pred_df, kmeans)\n",
    "    pred_df = create_value_indicator(pred_df)\n",
    "    pred_df = add_cluster_feats(pred_df, cluster_stats)\n",
    "\n",
    "    # AGGRESSIVE anomaly detection\n",
    "    pred_df = detect_and_normalize_anomalies(pred_df, y_col, for_training=False)\n",
    "\n",
    "    # Fill missing features\n",
    "    for f in feats:\n",
    "        if f not in pred_df.columns: pred_df[f] = 0\n",
    "        else: pred_df[f] = pred_df[f].fillna(pred_df[f].median() if pred_df[f].notna().sum()>0 else 0)\n",
    "\n",
    "    if '_value_source' in pred_df.columns:\n",
    "        print(f\"\\nValue sources: {dict(pred_df['_value_source'].value_counts())}\")\n",
    "\n",
    "    if 'anomaly_flag' in pred_df.columns:\n",
    "        anomaly_cnt = (pred_df['anomaly_flag'] > 0).sum()\n",
    "        if anomaly_cnt > 0:\n",
    "            print(f\"⚠️  {anomaly_cnt} properties had features normalized due to anomalies\")\n",
    "\n",
    "    X = pred_df[feats].values\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # CRITICAL: Hard cap predictions to prevent extreme overpredictions\n",
    "    # Don't let prediction exceed value_indicator by more than 1.8x (or 1.5x for census)\n",
    "    pred_to_value_ratio = y_pred / (pred_df['value_indicator'].values + 1)\n",
    "\n",
    "    # Even stricter cap for census-based properties\n",
    "    census_mask = pred_df['_value_source'] == 'census'\n",
    "    max_ratio = np.where(census_mask, 1.5, 1.8)  # 1.5x for census, 1.8x for others\n",
    "\n",
    "    extreme_high = pred_to_value_ratio > max_ratio\n",
    "\n",
    "    if extreme_high.sum() > 0:\n",
    "        print(f\"⚠️  CAPPING {extreme_high.sum()} extreme predictions\")\n",
    "        y_pred[extreme_high] = pred_df.loc[extreme_high, 'value_indicator'].values * max_ratio[extreme_high]\n",
    "\n",
    "    # Also cap very low predictions\n",
    "    extreme_low = pred_to_value_ratio < 0.6\n",
    "    if extreme_low.sum() > 0:\n",
    "        print(f\"⚠️  CAPPING {extreme_low.sum()} extreme low predictions (<0.6x value_indicator)\")\n",
    "        y_pred[extreme_low] = pred_df.loc[extreme_low, 'value_indicator'].values * 0.6\n",
    "\n",
    "    ids = pred_df[id_col].values\n",
    "    states = pred_df[state_col].values if state_col and state_col in pred_df.columns else ['Unknown']*len(pred_df)\n",
    "    actual = pred_df[y_col].values if y_col in pred_df.columns else [np.nan]*len(pred_df)\n",
    "    value_ind = pred_df['value_indicator'].values\n",
    "    value_src = pred_df['_value_source'].values\n",
    "    anomaly_flag = pred_df['anomaly_flag'].values if 'anomaly_flag' in pred_df.columns else [0]*len(pred_df)\n",
    "    anomaly_reason = pred_df['anomaly_reason'].values if 'anomaly_reason' in pred_df.columns else ['none']*len(pred_df)\n",
    "\n",
    "    result = pd.DataFrame({\n",
    "        'property_id':ids, 'state':states, 'value_indicator':value_ind, 'value_source':value_src,\n",
    "        'anomaly_flag':anomaly_flag, 'anomaly_reason':anomaly_reason,\n",
    "        'actual':actual, 'predicted':y_pred,\n",
    "        'error':[actual[i]-y_pred[i] if not np.isnan(actual[i]) else np.nan for i in range(len(actual))],\n",
    "        'pct_error':[100*(actual[i]-y_pred[i])/actual[i] if not np.isnan(actual[i]) and actual[i]!=0 else np.nan for i in range(len(actual))]\n",
    "    })\n",
    "\n",
    "    # ADD CONFIDENCE SCORING\n",
    "    result = add_prediction_confidence(result)\n",
    "\n",
    "    print(f\"\\n✓ Generated {len(result):,} predictions\")\n",
    "\n",
    "    # Show confidence summary\n",
    "    if 'confidence_score' in result.columns:\n",
    "        print(f\"\\nConfidence Summary:\")\n",
    "        print(f\"  High confidence (70-100): {(result['confidence_score'] >= 70).sum()}\")\n",
    "        print(f\"  Medium confidence (50-69): {((result['confidence_score'] >= 50) & (result['confidence_score'] < 70)).sum()}\")\n",
    "        print(f\"  Low confidence (30-49): {((result['confidence_score'] >= 30) & (result['confidence_score'] < 50)).sum()}\")\n",
    "        print(f\"  Very low confidence (0-29): {(result['confidence_score'] < 30).sum()}\")\n",
    "\n",
    "    valid = result['actual'].notna().sum()\n",
    "    if valid>0:\n",
    "        valid_df = result[result['actual'].notna()]\n",
    "        mae = mean_absolute_error(valid_df['actual'], valid_df['predicted'])\n",
    "        mape = np.mean(np.abs((valid_df['actual']-valid_df['predicted'])/valid_df['actual']))*100\n",
    "        r2 = r2_score(valid_df['actual'], valid_df['predicted'])\n",
    "        print(f\"\\nValidation ({valid}): MAE=${mae:,.0f} | MAPE={mape:.2f}% | R²={r2:.4f}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def save_results(results, out_dir, new_preds=None):\n",
    "    print(f\"\\nSaving results...\")\n",
    "    preds, metrics, fi = results['predictions'], results['metrics'], results['feature_importance']\n",
    "\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    ws = wb.create_sheet(\"Summary\", 0)\n",
    "    ws['A1'].font, ws['A1'].value = Font(bold=True,size=14), 'ULTRA AGGRESSIVE ANOMALY DETECTION + AVM'\n",
    "    ws['A2'].font, ws['A2'].value = Font(italic=True,size=10), 'Strictest thresholds → heavy census penalty → hard prediction caps → confidence scoring'\n",
    "\n",
    "    data = [\n",
    "        ['Metric','Value'],\n",
    "        ['Train Properties',metrics['n_train']],\n",
    "        ['Test Properties',metrics['n_test']],\n",
    "        ['R²',f\"{metrics['r2']:.4f}\"],\n",
    "        ['MAE',f\"${metrics['mae']:,.0f}\"],\n",
    "        ['MAPE%',f\"{metrics['mape']:.2f}%\"]\n",
    "    ]\n",
    "    if new_preds is not None: data.append(['New Predictions',len(new_preds)])\n",
    "\n",
    "    for i,(k,v) in enumerate(data,5):\n",
    "        ws[f'A{i}'].font, ws[f'A{i}'].value, ws[f'B{i}'].value = Font(bold=True), k, v\n",
    "\n",
    "    ws = wb.create_sheet(\"Feature_Importance\")\n",
    "    for r_idx,row in enumerate(dataframe_to_rows(fi,index=False,header=True),1):\n",
    "        for c_idx,value in enumerate(row,1):\n",
    "            cell = ws.cell(row=r_idx,column=c_idx,value=value)\n",
    "            if r_idx==1:\n",
    "                cell.font = Font(bold=True,color='FFFFFF')\n",
    "                cell.fill = PatternFill(start_color='366092',end_color='366092',fill_type='solid')\n",
    "\n",
    "    for sheet_name,data,color in [('Test_Predictions',preds,'366092'),('New_Predictions',new_preds,'4472C4')]:\n",
    "        if data is None: continue\n",
    "        ws = wb.create_sheet(sheet_name)\n",
    "        for i,h in enumerate(data.columns,1):\n",
    "            c = ws.cell(1,i,h)\n",
    "            c.font, c.fill = Font(bold=True,color='FFFFFF'), PatternFill(start_color=color,end_color=color,fill_type='solid')\n",
    "        for i,row in enumerate(data.itertuples(index=False),2):\n",
    "            for j,v in enumerate(row,1): ws.cell(i,j,v)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    xl_path = f\"{out_dir}/ultra_aggressive_avm_{ts}.xlsx\"\n",
    "    wb.save(xl_path)\n",
    "\n",
    "    preds.to_csv(f\"{out_dir}/ultra_aggressive_test_predictions_{ts}.csv\", index=False)\n",
    "    fi.to_csv(f\"{out_dir}/ultra_aggressive_importance_{ts}.csv\", index=False)\n",
    "    if new_preds is not None:\n",
    "        new_preds.to_csv(f\"{out_dir}/ultra_aggressive_new_predictions_{ts}.csv\", index=False)\n",
    "\n",
    "    print(f\"✓ Excel: {xl_path}\")\n",
    "    print(f\"✓ CSVs saved\")\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"=\"*60+\"\\nULTRA AGGRESSIVE ANOMALY DETECTION + AVM\\nStrictest Thresholds → Heavy Census Penalty → Hard Prediction Caps\\n\"+\"=\"*60)\n",
    "\n",
    "    # Load and prepare training data\n",
    "    df, y_col, id_col, state_col = load_data(TRAINING_INPUT_PATH)\n",
    "    df = df[df[y_col]>=MIN_PRICE]\n",
    "    df = engineer(df, y_col)\n",
    "    df, kmeans = geo_cluster(df)\n",
    "    df = create_value_indicator(df)\n",
    "\n",
    "    # Define features\n",
    "    base = [\"living_sqft\",\"lot_sqft\",\"year_built\",\"bedrooms\",\"full_baths\",\"half_baths\",\"garage_spaces\",\n",
    "            \"latitude\",\"longitude\",\"geo_cluster\",\"value_indicator\",\"log_value_indicator\"]\n",
    "    eng = [\"sqft_per_bedroom\",\"lot_to_living_ratio\",\"property_age\",\"is_new\",\"has_garage\",\"luxury_score\",\"log_sqft\",\"age_squared\"]\n",
    "    prior = [\"prior_sale_price\",\"prior_price_per_sqft\",\"prior_appreciated\",\"years_since_last_sale\",\"has_prior_sale\",\"recently_sold\"]\n",
    "    cluster = [\"cluster_avg_price\",\"cluster_med_price\"]\n",
    "    census = [\"median_household_income\",\"median_home_value\",\"pct_bachelors_degree\",\"income_education_score\"]\n",
    "\n",
    "    all_feats = base + eng + prior + cluster + census\n",
    "    feats = [f for f in all_feats if f in df.columns]\n",
    "\n",
    "    print(f\"{len(feats)}/{len(all_feats)} features available\")\n",
    "\n",
    "    cols = list(set(feats+[y_col,id_col]+([state_col] if state_col and state_col in df.columns else [])))\n",
    "    df = df[[c for c in cols if c in df.columns]].copy()\n",
    "    df[feats] = df[feats].fillna(df[feats].median())\n",
    "    df = df.dropna(subset=[y_col])\n",
    "\n",
    "    # Train model\n",
    "    results = train_single_model(df, feats, y_col, id_col, state_col)\n",
    "\n",
    "    # Predict new properties\n",
    "    new_preds = None\n",
    "    if PREDICTION_INPUT_PATH:\n",
    "        pred_df, _, _, _ = load_data(PREDICTION_INPUT_PATH)\n",
    "        new_preds = predict_new(pred_df, results['model'], feats, y_col, id_col, state_col,\n",
    "                               kmeans, results['cluster_stats'])\n",
    "\n",
    "    # Save results\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    save_results(results, OUTPUT_DIR, new_preds)\n",
    "\n",
    "    print(f\"\\n{'='*60}\\n✓ COMPLETE in {time.time()-t0:.1f}s\")\n",
    "    if new_preds is not None: print(f\"  New predictions: {len(new_preds):,}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__==\"__main__\": main()"
   ],
   "id": "d18b107fc4de9eb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ULTRA AGGRESSIVE ANOMALY DETECTION + AVM\n",
      "Strictest Thresholds → Heavy Census Penalty → Hard Prediction Caps\n",
      "============================================================\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/Untitled 5_2025-12-23-1212.csv\n",
      "127,326 records | 90.4MB | Price:sale_price ID:property_id\n",
      "  ⚠️  Filtered 29,431 bad assessed values\n",
      "  ⚠️  Filtered 29,431 bad assessed values\n",
      "24/32 features available\n",
      "\n",
      "Training SINGLE GLOBAL MODEL on 127,258 properties\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING: Aggressive Anomaly Detection\n",
      "============================================================\n",
      "  ⚠️  Found 75048 properties with extreme price/value ratios\n",
      "      These will be EXCLUDED from training (likely data errors)\n",
      "  ✓ Detected 210,788 anomalies across 32 cohorts\n",
      "  ✓ Excluded 75,048 extreme anomalies from training\n",
      "  After filtering: 44,703 properties\n",
      "\n",
      "============================================================\n",
      "MODEL TRAINING\n",
      "============================================================\n",
      "  Training: 31,292 | Test: 13,411\n",
      "  Results: MAE=$214,446 | MAPE=14.78% | R²=0.8059\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/MLS_w_luxury_AVM_outliers.csv\n",
      "1 records | 0.0MB | Price:sale_price ID:property_id\n",
      "\n",
      "============================================================\n",
      "PREDICTING 1 NEW PROPERTIES\n",
      "============================================================\n",
      "  ⚠️  Filtered 1 bad assessed values\n",
      "  ⚠️  Filtered 1 bad assessed values\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING: Aggressive Anomaly Detection\n",
      "============================================================\n",
      "⚠️  Skipping cohort-based anomaly detection - only 1 properties (need 10+)\n",
      "\n",
      "Value sources: {'census': np.int64(1)}\n",
      "\n",
      "✓ Generated 1 predictions\n",
      "\n",
      "Confidence Summary:\n",
      "  High confidence (70-100): 0\n",
      "  Medium confidence (50-69): 1\n",
      "  Low confidence (30-49): 0\n",
      "  Very low confidence (0-29): 0\n",
      "\n",
      "Validation (1): MAE=$173,067 | MAPE=32.90% | R²=nan\n",
      "\n",
      "Saving results...\n",
      "✓ Excel: /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/ultra_aggressive_avm_20251224_022419.xlsx\n",
      "✓ CSVs saved\n",
      "\n",
      "============================================================\n",
      "✓ COMPLETE in 3.1s\n",
      "  New predictions: 1\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "40515ca96c963322"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
