{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-19T21:57:35.610576Z",
     "start_time": "2025-12-19T21:57:07.873395Z"
    }
   },
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "CSV_FILE_PATH = '/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/input_census_neighborhood_image_data_12K.csv'\n",
    "PROPERTY_ID_COL = 'PROPERTY_ID'\n",
    "COMMENTS_COL = 'PUBLIC_LISTING_COMMENTS'\n",
    "PARSED_OUTPUT_COL = 'PARSED_OUTPUT'  # Column with image analysis data\n",
    "\n",
    "# LDA Parameters\n",
    "N_TOPICS = 5\n",
    "N_TOP_WORDS = 15\n",
    "MAX_FEATURES = 150\n",
    "MIN_N = 2  # minimum n-gram size\n",
    "MAX_N = 3  # maximum n-gram size\n",
    "\n",
    "# Common stopwords (including subjective adjectives)\n",
    "STOPWORDS = {\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
    "    'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was', 'will',\n",
    "    'with', 'w/', 'been', 'all', 'this', 'been', 'you', 'into', 'offers',\n",
    "    'offered', 'includes', 'some', 'tons', 'nice', 'ample', 'yrs',\n",
    "    # Subjective/descriptive adjectives\n",
    "    'adorable', 'charm', 'character', 'beautiful', 'stunning', 'unique',\n",
    "    'modern', 'decorative', 'tastefully', 'tasteful', 'convenient', 'spacious',\n",
    "    'plus', 'lovely', 'charming', 'gorgeous', 'amazing', 'wonderful', 'perfect',\n",
    "    'excellent', 'fantastic', 'incredible', 'magnificent', 'spectacular',\n",
    "    'delightful', 'elegant', 'cozy', 'inviting', 'warm', 'bright', 'airy',\n",
    "    'luxurious', 'premium', 'quality', 'great', 'good', 'better', 'best',\n",
    "    'original', 'covered', 'welcomes', 'showcasing', 'spanning', 'framed',\n",
    "    'abound', 'throughout'\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def extract_prominent_features(parsed_output):\n",
    "    \"\"\"\n",
    "    Extract all prominent_features from the parsed output dictionary.\n",
    "\n",
    "    Args:\n",
    "        parsed_output: String representation of dict or dict object\n",
    "\n",
    "    Returns:\n",
    "        List of all prominent features across all images\n",
    "    \"\"\"\n",
    "    if pd.isna(parsed_output):\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # Convert string to dict if needed\n",
    "        if isinstance(parsed_output, str):\n",
    "            data = ast.literal_eval(parsed_output)\n",
    "        else:\n",
    "            data = parsed_output\n",
    "\n",
    "        # Extract all prominent features from all images\n",
    "        all_features = []\n",
    "        for idx, image_data in data.items():\n",
    "            if 'prominent_features' in image_data:\n",
    "                all_features.extend(image_data['prominent_features'])\n",
    "\n",
    "        return all_features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing prominent features: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and tokenize text, removing stopwords and numbers.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s-]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    # Filter out stopwords, short tokens, and tokens containing digits\n",
    "    filtered = [t for t in tokens\n",
    "                if t not in STOPWORDS\n",
    "                and len(t) > 1\n",
    "                and not any(char.isdigit() for char in t)]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "\n",
    "def extract_ngrams_from_text(text, min_n=2, max_n=3):\n",
    "    \"\"\"Extract n-grams from text.\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "\n",
    "    cleaned = clean_text(text)\n",
    "    tokens = cleaned.split()\n",
    "\n",
    "    ngrams = []\n",
    "    for n in range(min_n, max_n + 1):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = '_'.join(tokens[i:i + n])\n",
    "            ngrams.append(ngram)\n",
    "\n",
    "    return ' '.join(ngrams)\n",
    "\n",
    "\n",
    "def convert_feature_to_tuple(feature):\n",
    "    \"\"\"\n",
    "    Convert a prominent feature phrase to an n-gram tuple.\n",
    "    Example: 'ceiling fan' -> 'ceiling_fan'\n",
    "    \"\"\"\n",
    "    # Clean and join words with underscore\n",
    "    words = feature.lower().strip().split()\n",
    "    # Remove stopwords from the feature\n",
    "    words = [w for w in words if w not in STOPWORDS]\n",
    "    return '_'.join(words) if words else ''\n",
    "\n",
    "\n",
    "def combine_features_and_text(row, comments_col, parsed_col, min_n=2, max_n=3):\n",
    "    \"\"\"\n",
    "    Combine prominent features from images with n-grams from text comments.\n",
    "\n",
    "    Args:\n",
    "        row: DataFrame row\n",
    "        comments_col: Column name for text comments\n",
    "        parsed_col: Column name for parsed image output\n",
    "\n",
    "    Returns:\n",
    "        Combined string of features and n-grams\n",
    "    \"\"\"\n",
    "    combined_tokens = []\n",
    "\n",
    "    # 1. Extract and convert prominent_features from images\n",
    "    prominent_features = extract_prominent_features(row[parsed_col])\n",
    "    for feature in prominent_features:\n",
    "        feature_tuple = convert_feature_to_tuple(feature)\n",
    "        if feature_tuple:  # Only add non-empty tuples\n",
    "            combined_tokens.append(feature_tuple)\n",
    "\n",
    "    # 2. Extract n-grams from text comments\n",
    "    comments_ngrams = extract_ngrams_from_text(row[comments_col], min_n, max_n)\n",
    "    if comments_ngrams:\n",
    "        combined_tokens.append(comments_ngrams)\n",
    "\n",
    "    return ' '.join(combined_tokens)\n",
    "\n",
    "\n",
    "def load_and_prepare_data(csv_path, id_col, comments_col, parsed_col):\n",
    "    \"\"\"Load data from CSV and prepare combined feature documents.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded {len(df)} records from CSV\")\n",
    "\n",
    "        # Filter out rows with missing data\n",
    "        df = df[df[comments_col].notna() | df[parsed_col].notna()]\n",
    "        print(f\"Found {len(df)} records with comments or image data\")\n",
    "\n",
    "        # Combine features and text for each property\n",
    "        print(\"\\nCombining image features and text comments...\")\n",
    "        df['combined_document'] = df.apply(\n",
    "            lambda row: combine_features_and_text(row, comments_col, parsed_col, MIN_N, MAX_N),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Filter out empty documents\n",
    "        df = df[df['combined_document'].str.strip() != '']\n",
    "        print(f\"Created {len(df)} combined documents for LDA\")\n",
    "\n",
    "        return df[id_col].tolist(), df['combined_document'].tolist()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CSV file not found at {csv_path}\")\n",
    "        return None, None\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Column {e} not found in CSV\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def train_lda(documents, property_ids=None, n_topics=3, n_top_words=10, max_features=100):\n",
    "    \"\"\"Train LDA model on combined documents (image features + text n-grams).\"\"\"\n",
    "\n",
    "    # Create document-term matrix\n",
    "    # Note: documents already contain tuples, so we don't need additional processing\n",
    "    vectorizer = CountVectorizer(max_features=max_features, min_df=1)\n",
    "    doc_term_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "    print(f\"\\nVocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
    "    print(f\"Document-term matrix shape: {doc_term_matrix.shape}\")\n",
    "\n",
    "    # Train LDA\n",
    "    print(f\"\\nTraining LDA with {n_topics} topics...\")\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        random_state=42,\n",
    "        max_iter=20,\n",
    "        learning_method='batch'\n",
    "    )\n",
    "    lda.fit(doc_term_matrix)\n",
    "\n",
    "    # Display topics\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DISCOVERED TOPICS (Image Features + Text)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_indices = topic.argsort()[-n_top_words:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        top_weights = [topic[i] for i in top_indices]\n",
    "\n",
    "        print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "        print(\"-\" * 40)\n",
    "        for word, weight in zip(top_words, top_weights):\n",
    "            print(f\"  {word:<40} {weight:.3f}\")\n",
    "\n",
    "    # Show document-topic distribution\n",
    "    doc_topics = lda.transform(doc_term_matrix)\n",
    "\n",
    "    # Calculate average topic distribution\n",
    "    avg_topic_dist = doc_topics.mean(axis=0)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"AVERAGE TOPIC DISTRIBUTION ACROSS ALL DOCUMENTS\")\n",
    "    print(\"=\" * 60)\n",
    "    for topic_idx, avg_prob in enumerate(avg_topic_dist):\n",
    "        print(f\"  Topic {topic_idx + 1}: {avg_prob:.1%}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DOCUMENT-TOPIC DISTRIBUTION (Sample)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Show first 10 documents\n",
    "    num_to_show = min(10, len(doc_topics))\n",
    "    for i in range(num_to_show):\n",
    "        prop_id = property_ids[i] if property_ids else f\"Doc {i + 1}\"\n",
    "        print(f\"\\nProperty {prop_id}:\")\n",
    "        for topic_idx, prob in enumerate(doc_topics[i]):\n",
    "            if prob > 0.1:  # Only show topics with >10% probability\n",
    "                print(f\"  Topic {topic_idx + 1}: {prob:.3f}\")\n",
    "\n",
    "    if len(doc_topics) > num_to_show:\n",
    "        print(f\"\\n... ({len(doc_topics) - num_to_show} more documents)\")\n",
    "\n",
    "    return lda, vectorizer, doc_topics\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"COMBINED IMAGE + TEXT LDA TOPIC MODELING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Load and prepare data\n",
    "    property_ids, documents = load_and_prepare_data(\n",
    "        CSV_FILE_PATH,\n",
    "        PROPERTY_ID_COL,\n",
    "        COMMENTS_COL,\n",
    "        PARSED_OUTPUT_COL\n",
    "    )\n",
    "\n",
    "    if documents is None or len(documents) == 0:\n",
    "        print(\"\\nNo documents found. Exiting.\")\n",
    "    else:\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"TRAINING LDA ON {len(documents)} COMBINED DOCUMENTS\")\n",
    "        print(f\"Text n-gram range: {MIN_N}-{MAX_N}\")\n",
    "        print(f\"Number of topics: {N_TOPICS}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        lda_model, vectorizer, doc_topics = train_lda(\n",
    "            documents,\n",
    "            property_ids=property_ids,\n",
    "            n_topics=N_TOPICS,\n",
    "            n_top_words=N_TOP_WORDS,\n",
    "            max_features=MAX_FEATURES\n",
    "        )\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ANALYSIS COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\\nData streams combined:\")\n",
    "        print(\"  • Image prominent_features (converted to tuples)\")\n",
    "        print(\"  • Text comments (parsed into n-grams)\")\n",
    "        print(\"\\nTo improve results:\")\n",
    "        print(\"  • Adjust N_TOPICS based on your corpus size\")\n",
    "        print(\"  • Increase MAX_FEATURES for larger vocabulary\")\n",
    "        print(\"  • Tune MIN_N and MAX_N for different n-gram sizes\")\n",
    "        print(\"  • Add more domain-specific stopwords if needed\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMBINED IMAGE + TEXT LDA TOPIC MODELING\n",
      "============================================================\n",
      "Loaded 15062 records from CSV\n",
      "Found 14662 records with comments or image data\n",
      "\n",
      "Combining image features and text comments...\n",
      "Created 14662 combined documents for LDA\n",
      "\n",
      "============================================================\n",
      "TRAINING LDA ON 14662 COMBINED DOCUMENTS\n",
      "Text n-gram range: 2-3\n",
      "Number of topics: 5\n",
      "============================================================\n",
      "\n",
      "Vocabulary size: 150\n",
      "Document-term matrix shape: (14662, 150)\n",
      "\n",
      "Training LDA with 5 topics...\n",
      "\n",
      "============================================================\n",
      "DISCOVERED TOPICS (Image Features + Text)\n",
      "============================================================\n",
      "\n",
      "Topic 1:\n",
      "----------------------------------------\n",
      "  neutral_paint                            8594.595\n",
      "  carpeted_floor                           7778.804\n",
      "  ceiling_fan                              4797.855\n",
      "  natural_light                            4073.137\n",
      "  wood                                     2786.189\n",
      "  neutral_finishes                         2723.284\n",
      "  tub                                      2341.950\n",
      "  look_flooring                            2326.186\n",
      "  built                                    2063.260\n",
      "  laminate_flooring                        2055.188\n",
      "  laminate                                 1957.192\n",
      "  single                                   1760.506\n",
      "  wood_cabinets                            1729.702\n",
      "  shower_combo                             1576.720\n",
      "  vinyl                                    1519.951\n",
      "\n",
      "Topic 2:\n",
      "----------------------------------------\n",
      "  well                                     4385.586\n",
      "  single                                   3155.837\n",
      "  car_garage                               2536.218\n",
      "  carpet_flooring                          2292.190\n",
      "  two                                      1894.579\n",
      "  attached_two                             1731.895\n",
      "  front_porch                              1354.880\n",
      "  sink_vanity                              1140.422\n",
      "  maintained_lawn                          1012.672\n",
      "  curb_appeal                              1003.460\n",
      "  mature_trees                             974.337\n",
      "  natural_light                            808.445\n",
      "  neutral                                  755.973\n",
      "  driveway                                 731.103\n",
      "  concrete_driveway                        727.426\n",
      "\n",
      "Topic 3:\n",
      "----------------------------------------\n",
      "  in                                       5881.191\n",
      "  walk                                     2444.576\n",
      "  de                                       1817.197\n",
      "  well                                     1665.003\n",
      "  in_closet                                1594.061\n",
      "  built                                    1336.416\n",
      "  living_room                              1241.274\n",
      "  dining_room                              976.995\n",
      "  family_room                              871.321\n",
      "  eat                                      700.438\n",
      "  granite_countertops                      631.201\n",
      "  stainless_steel_appliances               603.499\n",
      "  multi                                    485.417\n",
      "  master_bedroom                           422.202\n",
      "  hardwood_flooring                        375.186\n",
      "\n",
      "Topic 4:\n",
      "----------------------------------------\n",
      "  ceiling_fan                              5007.155\n",
      "  hardwood_flooring                        4782.948\n",
      "  well                                     4532.579\n",
      "  tile_flooring                            3822.851\n",
      "  neutral_finishes                         3720.323\n",
      "  carpeted_floors                          3674.943\n",
      "  natural_light                            3455.125\n",
      "  carpeted_flooring                        3182.896\n",
      "  carpeted_floor                           3071.974\n",
      "  _natural_light                           2650.312\n",
      "  neutral_paint                            2534.400\n",
      "  two                                      2500.391\n",
      "  built                                    1545.642\n",
      "  car_garage                               1536.672\n",
      "  natural_light_window                     1487.567\n",
      "\n",
      "Topic 5:\n",
      "----------------------------------------\n",
      "  hardwood_floors                          26315.682\n",
      "  natural_light                            13601.107\n",
      "  built                                    10959.468\n",
      "  ceiling_fan                              8014.069\n",
      "  well                                     7485.972\n",
      "  carpeted_floor                           5774.105\n",
      "  hardwood_flooring                        4869.311\n",
      "  tile_flooring                            4728.779\n",
      "  mature_trees                             4342.256\n",
      "  large_windows                            4260.663\n",
      "  granite_countertops                      3933.292\n",
      "  neutral_paint                            3578.979\n",
      "  chandelier                               3419.115\n",
      "  neutral_finishes                         3400.035\n",
      "  recessed_lighting                        3351.493\n",
      "\n",
      "============================================================\n",
      "AVERAGE TOPIC DISTRIBUTION ACROSS ALL DOCUMENTS\n",
      "============================================================\n",
      "  Topic 1: 20.9%\n",
      "  Topic 2: 16.1%\n",
      "  Topic 3: 8.4%\n",
      "  Topic 4: 18.3%\n",
      "  Topic 5: 36.3%\n",
      "\n",
      "============================================================\n",
      "DOCUMENT-TOPIC DISTRIBUTION (Sample)\n",
      "============================================================\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 2: 0.127\n",
      "  Topic 5: 0.864\n",
      "\n",
      "Property 143995964:\n",
      "  Topic 1: 0.980\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 2: 0.124\n",
      "  Topic 5: 0.867\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 2: 0.127\n",
      "  Topic 5: 0.864\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 2: 0.124\n",
      "  Topic 5: 0.867\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 2: 0.127\n",
      "  Topic 5: 0.864\n",
      "\n",
      "Property 143697419:\n",
      "  Topic 2: 0.381\n",
      "  Topic 4: 0.568\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 2: 0.127\n",
      "  Topic 5: 0.864\n",
      "\n",
      "Property 143995964:\n",
      "  Topic 1: 0.980\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 2: 0.127\n",
      "  Topic 5: 0.864\n",
      "\n",
      "... (14652 more documents)\n",
      "\n",
      "============================================================\n",
      "ANALYSIS COMPLETE\n",
      "============================================================\n",
      "\n",
      "Data streams combined:\n",
      "  • Image prominent_features (converted to tuples)\n",
      "  • Text comments (parsed into n-grams)\n",
      "\n",
      "To improve results:\n",
      "  • Adjust N_TOPICS based on your corpus size\n",
      "  • Increase MAX_FEATURES for larger vocabulary\n",
      "  • Tune MIN_N and MAX_N for different n-gram sizes\n",
      "  • Add more domain-specific stopwords if needed\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ea99384525732e61"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
