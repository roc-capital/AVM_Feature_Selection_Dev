{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-19T22:01:31.906498Z",
     "start_time": "2025-12-19T22:01:01.967795Z"
    }
   },
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "CSV_FILE_PATH = '/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/input_census_neighborhood_image_data_12K.csv'\n",
    "PROPERTY_ID_COL = 'PROPERTY_ID'\n",
    "COMMENTS_COL = 'PUBLIC_LISTING_COMMENTS'\n",
    "PARSED_OUTPUT_COL = 'PARSED_OUTPUT'  # Column with image analysis data\n",
    "\n",
    "# LDA Parameters\n",
    "N_TOPICS = 10\n",
    "N_TOP_WORDS = 15\n",
    "MAX_FEATURES = 150\n",
    "MIN_N = 2  # minimum n-gram size\n",
    "MAX_N = 3  # maximum n-gram size\n",
    "\n",
    "# Common stopwords (including subjective adjectives)\n",
    "STOPWORDS = {\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
    "    'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was', 'will',\n",
    "    'with', 'w/', 'been', 'all', 'this', 'been', 'you', 'into', 'offers',\n",
    "    'offered', 'includes', 'some', 'tons', 'nice', 'ample', 'yrs',\n",
    "    # Subjective/descriptive adjectives\n",
    "    'adorable', 'charm', 'character', 'beautiful', 'stunning', 'unique',\n",
    "    'modern', 'decorative', 'tastefully', 'tasteful', 'convenient', 'spacious',\n",
    "    'plus', 'lovely', 'charming', 'gorgeous', 'amazing', 'wonderful', 'perfect',\n",
    "    'excellent', 'fantastic', 'incredible', 'magnificent', 'spectacular',\n",
    "    'delightful', 'elegant', 'cozy', 'inviting', 'warm', 'bright', 'airy',\n",
    "    'luxurious', 'premium', 'quality', 'great', 'good', 'better', 'best',\n",
    "    'original', 'covered', 'welcomes', 'showcasing', 'spanning', 'framed',\n",
    "    'abound', 'throughout'\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def extract_prominent_features(parsed_output):\n",
    "    \"\"\"\n",
    "    Extract all prominent_features from the parsed output dictionary.\n",
    "\n",
    "    Args:\n",
    "        parsed_output: String representation of dict or dict object\n",
    "\n",
    "    Returns:\n",
    "        List of all prominent features across all images\n",
    "    \"\"\"\n",
    "    if pd.isna(parsed_output):\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # Convert string to dict if needed\n",
    "        if isinstance(parsed_output, str):\n",
    "            data = ast.literal_eval(parsed_output)\n",
    "        else:\n",
    "            data = parsed_output\n",
    "\n",
    "        # Extract all prominent features from all images\n",
    "        all_features = []\n",
    "        for idx, image_data in data.items():\n",
    "            if 'prominent_features' in image_data:\n",
    "                all_features.extend(image_data['prominent_features'])\n",
    "\n",
    "        return all_features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing prominent features: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and tokenize text, removing stopwords and numbers.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s-]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    # Filter out stopwords, short tokens, and tokens containing digits\n",
    "    filtered = [t for t in tokens\n",
    "                if t not in STOPWORDS\n",
    "                and len(t) > 1\n",
    "                and not any(char.isdigit() for char in t)]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "\n",
    "def extract_ngrams_from_text(text, min_n=2, max_n=3):\n",
    "    \"\"\"Extract n-grams from text.\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "\n",
    "    cleaned = clean_text(text)\n",
    "    tokens = cleaned.split()\n",
    "\n",
    "    ngrams = []\n",
    "    for n in range(min_n, max_n + 1):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = '_'.join(tokens[i:i + n])\n",
    "            ngrams.append(ngram)\n",
    "\n",
    "    return ' '.join(ngrams)\n",
    "\n",
    "\n",
    "def convert_feature_to_tuple(feature):\n",
    "    \"\"\"\n",
    "    Convert a prominent feature phrase to an n-gram tuple.\n",
    "    Example: 'ceiling fan' -> 'ceiling_fan'\n",
    "    \"\"\"\n",
    "    # Clean and join words with underscore\n",
    "    words = feature.lower().strip().split()\n",
    "    # Remove stopwords from the feature\n",
    "    words = [w for w in words if w not in STOPWORDS]\n",
    "    return '_'.join(words) if words else ''\n",
    "\n",
    "\n",
    "def combine_features_and_text(row, comments_col, parsed_col, min_n=2, max_n=3):\n",
    "    \"\"\"\n",
    "    Combine prominent features from images with n-grams from text comments.\n",
    "\n",
    "    Args:\n",
    "        row: DataFrame row\n",
    "        comments_col: Column name for text comments\n",
    "        parsed_col: Column name for parsed image output\n",
    "\n",
    "    Returns:\n",
    "        Combined string of features and n-grams\n",
    "    \"\"\"\n",
    "    combined_tokens = []\n",
    "\n",
    "    # 1. Extract and convert prominent_features from images\n",
    "    prominent_features = extract_prominent_features(row[parsed_col])\n",
    "    for feature in prominent_features:\n",
    "        feature_tuple = convert_feature_to_tuple(feature)\n",
    "        if feature_tuple:  # Only add non-empty tuples\n",
    "            combined_tokens.append(feature_tuple)\n",
    "\n",
    "    # 2. Extract n-grams from text comments\n",
    "    comments_ngrams = extract_ngrams_from_text(row[comments_col], min_n, max_n)\n",
    "    if comments_ngrams:\n",
    "        combined_tokens.append(comments_ngrams)\n",
    "\n",
    "    return ' '.join(combined_tokens)\n",
    "\n",
    "\n",
    "def load_and_prepare_data(csv_path, id_col, comments_col, parsed_col):\n",
    "    \"\"\"Load data from CSV and prepare combined feature documents.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded {len(df)} records from CSV\")\n",
    "\n",
    "        # Filter out rows with missing data\n",
    "        df = df[df[comments_col].notna() | df[parsed_col].notna()]\n",
    "        print(f\"Found {len(df)} records with comments or image data\")\n",
    "\n",
    "        # Combine features and text for each property\n",
    "        print(\"\\nCombining image features and text comments...\")\n",
    "        df['combined_document'] = df.apply(\n",
    "            lambda row: combine_features_and_text(row, comments_col, parsed_col, MIN_N, MAX_N),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Filter out empty documents\n",
    "        df = df[df['combined_document'].str.strip() != '']\n",
    "        print(f\"Created {len(df)} combined documents for LDA\")\n",
    "\n",
    "        return df[id_col].tolist(), df['combined_document'].tolist()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CSV file not found at {csv_path}\")\n",
    "        return None, None\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Column {e} not found in CSV\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def train_lda(documents, property_ids=None, n_topics=3, n_top_words=10, max_features=100):\n",
    "    \"\"\"Train LDA model on combined documents (image features + text n-grams).\"\"\"\n",
    "\n",
    "    # Create document-term matrix\n",
    "    # Note: documents already contain tuples, so we don't need additional processing\n",
    "    vectorizer = CountVectorizer(max_features=max_features, min_df=1)\n",
    "    doc_term_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "    print(f\"\\nVocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
    "    print(f\"Document-term matrix shape: {doc_term_matrix.shape}\")\n",
    "\n",
    "    # Train LDA\n",
    "    print(f\"\\nTraining LDA with {n_topics} topics...\")\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        random_state=42,\n",
    "        max_iter=20,\n",
    "        learning_method='batch'\n",
    "    )\n",
    "    lda.fit(doc_term_matrix)\n",
    "\n",
    "    # Display topics\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DISCOVERED TOPICS (Image Features + Text)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_indices = topic.argsort()[-n_top_words:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        top_weights = [topic[i] for i in top_indices]\n",
    "\n",
    "        print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "        print(\"-\" * 40)\n",
    "        for word, weight in zip(top_words, top_weights):\n",
    "            print(f\"  {word:<40} {weight:.3f}\")\n",
    "\n",
    "    # Show document-topic distribution\n",
    "    doc_topics = lda.transform(doc_term_matrix)\n",
    "\n",
    "    # Calculate average topic distribution\n",
    "    avg_topic_dist = doc_topics.mean(axis=0)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"AVERAGE TOPIC DISTRIBUTION ACROSS ALL DOCUMENTS\")\n",
    "    print(\"=\" * 60)\n",
    "    for topic_idx, avg_prob in enumerate(avg_topic_dist):\n",
    "        print(f\"  Topic {topic_idx + 1}: {avg_prob:.1%}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DOCUMENT-TOPIC DISTRIBUTION (Sample)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Show first 10 documents\n",
    "    num_to_show = min(10, len(doc_topics))\n",
    "    for i in range(num_to_show):\n",
    "        prop_id = property_ids[i] if property_ids else f\"Doc {i + 1}\"\n",
    "        print(f\"\\nProperty {prop_id}:\")\n",
    "        for topic_idx, prob in enumerate(doc_topics[i]):\n",
    "            if prob > 0.1:  # Only show topics with >10% probability\n",
    "                print(f\"  Topic {topic_idx + 1}: {prob:.3f}\")\n",
    "\n",
    "    if len(doc_topics) > num_to_show:\n",
    "        print(f\"\\n... ({len(doc_topics) - num_to_show} more documents)\")\n",
    "\n",
    "    return lda, vectorizer, doc_topics\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"COMBINED IMAGE + TEXT LDA TOPIC MODELING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Load and prepare data\n",
    "    property_ids, documents = load_and_prepare_data(\n",
    "        CSV_FILE_PATH,\n",
    "        PROPERTY_ID_COL,\n",
    "        COMMENTS_COL,\n",
    "        PARSED_OUTPUT_COL\n",
    "    )\n",
    "\n",
    "    if documents is None or len(documents) == 0:\n",
    "        print(\"\\nNo documents found. Exiting.\")\n",
    "    else:\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"TRAINING LDA ON {len(documents)} COMBINED DOCUMENTS\")\n",
    "        print(f\"Text n-gram range: {MIN_N}-{MAX_N}\")\n",
    "        print(f\"Number of topics: {N_TOPICS}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        lda_model, vectorizer, doc_topics = train_lda(\n",
    "            documents,\n",
    "            property_ids=property_ids,\n",
    "            n_topics=N_TOPICS,\n",
    "            n_top_words=N_TOP_WORDS,\n",
    "            max_features=MAX_FEATURES\n",
    "        )\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ANALYSIS COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\\nData streams combined:\")\n",
    "        print(\"  • Image prominent_features (converted to tuples)\")\n",
    "        print(\"  • Text comments (parsed into n-grams)\")\n",
    "        print(\"\\nTo improve results:\")\n",
    "        print(\"  • Adjust N_TOPICS based on your corpus size\")\n",
    "        print(\"  • Increase MAX_FEATURES for larger vocabulary\")\n",
    "        print(\"  • Tune MIN_N and MAX_N for different n-gram sizes\")\n",
    "        print(\"  • Add more domain-specific stopwords if needed\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMBINED IMAGE + TEXT LDA TOPIC MODELING\n",
      "============================================================\n",
      "Loaded 15062 records from CSV\n",
      "Found 14662 records with comments or image data\n",
      "\n",
      "Combining image features and text comments...\n",
      "Created 14662 combined documents for LDA\n",
      "\n",
      "============================================================\n",
      "TRAINING LDA ON 14662 COMBINED DOCUMENTS\n",
      "Text n-gram range: 2-3\n",
      "Number of topics: 10\n",
      "============================================================\n",
      "\n",
      "Vocabulary size: 150\n",
      "Document-term matrix shape: (14662, 150)\n",
      "\n",
      "Training LDA with 10 topics...\n",
      "\n",
      "============================================================\n",
      "DISCOVERED TOPICS (Image Features + Text)\n",
      "============================================================\n",
      "\n",
      "Topic 1:\n",
      "----------------------------------------\n",
      "  wood                                     2786.100\n",
      "  look_flooring                            2326.100\n",
      "  carpeted_floor                           1245.370\n",
      "  natural_light                            725.884\n",
      "  neutral_paint                            717.665\n",
      "  wood_cabinets                            528.593\n",
      "  laminate_flooring                        393.672\n",
      "  built                                    389.146\n",
      "  ceiling_fan                              378.493\n",
      "  neutral_finishes                         367.029\n",
      "  laminate                                 286.585\n",
      "  natural_light_window                     252.666\n",
      "  clean                                    238.295\n",
      "  two                                      218.999\n",
      "  stainless_refrigerator                   208.545\n",
      "\n",
      "Topic 2:\n",
      "----------------------------------------\n",
      "  single                                   3112.772\n",
      "  neutral_paint                            1513.982\n",
      "  carpet_flooring                          1497.509\n",
      "  natural_light                            1243.953\n",
      "  well                                     1122.871\n",
      "  ceiling_fan                              1115.528\n",
      "  sink_vanity                              1088.381\n",
      "  vinyl                                    995.550\n",
      "  tile_flooring                            991.952\n",
      "  mature_trees                             856.097\n",
      "  vinyl_flooring                           827.729\n",
      "  built                                    678.826\n",
      "  neutral_finishes                         555.832\n",
      "  driveway                                 544.023\n",
      "  patio                                    489.784\n",
      "\n",
      "Topic 3:\n",
      "----------------------------------------\n",
      "  in                                       5788.416\n",
      "  walk                                     2440.696\n",
      "  in_closet                                1469.727\n",
      "  built                                    1064.466\n",
      "  living_room                              996.059\n",
      "  dining_room                              835.289\n",
      "  eat                                      690.220\n",
      "  family_room                              678.956\n",
      "  formal_dining_room                       328.749\n",
      "  master_bedroom                           320.664\n",
      "  dining_area                              261.758\n",
      "  stainless_steel_appliances               246.779\n",
      "  ins                                      222.156\n",
      "  well                                     220.851\n",
      "  car_garage                               206.812\n",
      "\n",
      "Topic 4:\n",
      "----------------------------------------\n",
      "  well                                     5265.725\n",
      "  carpeted_floor                           2865.782\n",
      "  de                                       1817.034\n",
      "  natural_light                            1808.603\n",
      "  ceiling_fan                              1652.292\n",
      "  built                                    1469.064\n",
      "  hardwood_flooring                        1355.073\n",
      "  carpeted_flooring                        1240.688\n",
      "  two                                      985.475\n",
      "  mature_trees                             897.220\n",
      "  natural_light_window                     822.226\n",
      "  curb_appeal                              795.810\n",
      "  landscaping                              765.207\n",
      "  maintained_lawn                          740.913\n",
      "  car_garage                               739.162\n",
      "\n",
      "Topic 5:\n",
      "----------------------------------------\n",
      "  hardwood_floors                          7229.962\n",
      "  built                                    6435.675\n",
      "  natural_light                            5367.554\n",
      "  carpeted_floor                           4114.008\n",
      "  ceiling_fan                              3018.100\n",
      "  chandelier                               2511.927\n",
      "  large_windows                            2503.090\n",
      "  tile_flooring                            2097.061\n",
      "  mature_trees                             2074.963\n",
      "  in_shelving                              1704.079\n",
      "  wainscoting                              1637.512\n",
      "  well                                     1522.153\n",
      "  two                                      1367.310\n",
      "  mature_landscaping                       1354.568\n",
      "  recessed_lighting                        1300.380\n",
      "\n",
      "Topic 6:\n",
      "----------------------------------------\n",
      "  neutral_paint                            3227.518\n",
      "  ceiling_fan                              1978.987\n",
      "  carpeted_floor                           1919.680\n",
      "  laminate_flooring                        1584.564\n",
      "  natural_light                            1524.150\n",
      "  laminate                                 1416.064\n",
      "  tub                                      1231.194\n",
      "  single                                   1089.977\n",
      "  laminate_countertops                     1001.167\n",
      "  hardwood                                 902.602\n",
      "  neutral_finishes                         895.704\n",
      "  shower_combo                             838.911\n",
      "  vinyl_siding                             806.987\n",
      "  vinyl                                    668.818\n",
      "  vaulted_ceiling                          610.745\n",
      "\n",
      "Topic 7:\n",
      "----------------------------------------\n",
      "  hardwood_floors                          17155.061\n",
      "  natural_light                            5785.804\n",
      "  well                                     4329.868\n",
      "  ceiling_fan                              4109.894\n",
      "  built                                    2503.509\n",
      "  neutral_paint                            2323.362\n",
      "  neutral_finishes                         2238.862\n",
      "  mature_trees                             1885.408\n",
      "  white_cabinetry                          1656.840\n",
      "  curb_appeal                              1467.162\n",
      "  lighting                                 1384.678\n",
      "  stainless_steel_appliances               1370.804\n",
      "  natural_light_window                     1317.713\n",
      "  fireplace                                1175.922\n",
      "  tile_flooring                            1037.844\n",
      "\n",
      "Topic 8:\n",
      "----------------------------------------\n",
      "  hardwood_flooring                        4245.653\n",
      "  well                                     3248.716\n",
      "  built                                    2343.822\n",
      "  hardwood_floors                          1979.412\n",
      "  granite_countertops                      1923.781\n",
      "  tile_flooring                            1779.891\n",
      "  stainless_steel_appliances               1381.582\n",
      "  natural_light                            1312.560\n",
      "  carpeted_flooring                        1184.092\n",
      "  high                                     1003.137\n",
      "  multi                                    998.428\n",
      "  recessed_lighting                        941.474\n",
      "  open                                     807.232\n",
      "  cabinetry                                803.003\n",
      "  pendant_lighting                         759.565\n",
      "\n",
      "Topic 9:\n",
      "----------------------------------------\n",
      "  carpeted_floors                          3025.543\n",
      "  _natural_light                           2784.378\n",
      "  car_garage                               2539.391\n",
      "  two                                      2457.268\n",
      "  well                                     1834.430\n",
      "  attached_two                             1532.061\n",
      "  ceiling_fan                              1325.013\n",
      "  front_porch                              1142.601\n",
      "  neutral_finishes                         1024.516\n",
      "  large_windows_                           976.422\n",
      "  carpeted_flooring                        974.084\n",
      "  natural_light                            795.197\n",
      "  concrete_driveway                        615.971\n",
      "  fireplace                                569.420\n",
      "  natural_light_window                     563.963\n",
      "\n",
      "Topic 10:\n",
      "----------------------------------------\n",
      "  carpeted_floor                           6248.564\n",
      "  neutral_paint                            5481.103\n",
      "  ceiling_fan                              4337.740\n",
      "  tile_flooring                            3665.741\n",
      "  natural_light                            3478.283\n",
      "  neutral_finishes                         3109.244\n",
      "  hardwood_flooring                        2621.398\n",
      "  two                                      2276.467\n",
      "  vaulted_ceiling                          1680.452\n",
      "  tile_floor                               1594.874\n",
      "  wood_cabinets                            1566.846\n",
      "  large_windows                            1376.318\n",
      "  large_window                             1296.497\n",
      "  tub                                      1182.489\n",
      "  car_garage                               1175.101\n",
      "\n",
      "============================================================\n",
      "AVERAGE TOPIC DISTRIBUTION ACROSS ALL DOCUMENTS\n",
      "============================================================\n",
      "  Topic 1: 3.6%\n",
      "  Topic 2: 9.2%\n",
      "  Topic 3: 5.3%\n",
      "  Topic 4: 10.4%\n",
      "  Topic 5: 13.6%\n",
      "  Topic 6: 8.9%\n",
      "  Topic 7: 15.6%\n",
      "  Topic 8: 9.3%\n",
      "  Topic 9: 10.4%\n",
      "  Topic 10: 13.8%\n",
      "\n",
      "============================================================\n",
      "DOCUMENT-TOPIC DISTRIBUTION (Sample)\n",
      "============================================================\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 5: 0.846\n",
      "\n",
      "Property 143995964:\n",
      "  Topic 1: 0.435\n",
      "  Topic 6: 0.545\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 5: 0.765\n",
      "  Topic 10: 0.121\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 5: 0.846\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 5: 0.765\n",
      "  Topic 10: 0.121\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 5: 0.846\n",
      "\n",
      "Property 143697419:\n",
      "  Topic 4: 0.689\n",
      "  Topic 10: 0.244\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 5: 0.846\n",
      "\n",
      "Property 143995964:\n",
      "  Topic 1: 0.435\n",
      "  Topic 6: 0.545\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 5: 0.846\n",
      "\n",
      "... (14652 more documents)\n",
      "\n",
      "============================================================\n",
      "ANALYSIS COMPLETE\n",
      "============================================================\n",
      "\n",
      "Data streams combined:\n",
      "  • Image prominent_features (converted to tuples)\n",
      "  • Text comments (parsed into n-grams)\n",
      "\n",
      "To improve results:\n",
      "  • Adjust N_TOPICS based on your corpus size\n",
      "  • Increase MAX_FEATURES for larger vocabulary\n",
      "  • Tune MIN_N and MAX_N for different n-gram sizes\n",
      "  • Add more domain-specific stopwords if needed\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T22:36:00.373837Z",
     "start_time": "2025-12-19T22:35:31.994251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "CSV_FILE_PATH = '/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/input_census_neighborhood_image_data_12K.csv'\n",
    "PROPERTY_ID_COL = 'PROPERTY_ID'\n",
    "COMMENTS_COL = 'PUBLIC_LISTING_COMMENTS'\n",
    "PARSED_OUTPUT_COL = 'PARSED_OUTPUT'\n",
    "\n",
    "# LDA Parameters\n",
    "N_TOPICS = 8\n",
    "N_TOP_WORDS = 10\n",
    "MAX_FEATURES = 200\n",
    "MIN_N = 2\n",
    "MAX_N = 3\n",
    "\n",
    "# Topic Sparsity Controls\n",
    "MAX_FEATURES_PER_TOPIC = 20\n",
    "TOPIC_WORD_PRIOR = 0.01\n",
    "\n",
    "# Stopwords\n",
    "STOPWORDS = {\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
    "    'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was', 'will',\n",
    "    'with', 'w/', 'been', 'all', 'this', 'you', 'into', 'offers',\n",
    "    'offered', 'includes', 'some', 'tons', 'nice', 'ample', 'yrs',\n",
    "    'adorable', 'charm', 'character', 'beautiful', 'stunning', 'unique',\n",
    "    'modern', 'decorative', 'tastefully', 'tasteful', 'convenient', 'spacious',\n",
    "    'plus', 'lovely', 'charming', 'gorgeous', 'amazing', 'wonderful', 'perfect',\n",
    "    'excellent', 'fantastic', 'incredible', 'magnificent', 'spectacular',\n",
    "    'delightful', 'elegant', 'cozy', 'inviting', 'warm', 'bright', 'airy',\n",
    "    'luxurious', 'premium', 'quality', 'great', 'good', 'better', 'best',\n",
    "    'original', 'covered', 'welcomes', 'showcasing', 'spanning', 'framed',\n",
    "    'abound', 'throughout'\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def extract_prominent_features(parsed_output):\n",
    "    \"\"\"Extract all prominent_features from the parsed output dictionary.\"\"\"\n",
    "    if pd.isna(parsed_output):\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        if isinstance(parsed_output, str):\n",
    "            data = ast.literal_eval(parsed_output)\n",
    "        else:\n",
    "            data = parsed_output\n",
    "\n",
    "        all_features = []\n",
    "        for idx, image_data in data.items():\n",
    "            if 'prominent_features' in image_data:\n",
    "                all_features.extend(image_data['prominent_features'])\n",
    "\n",
    "        return all_features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing prominent features: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and tokenize text, removing stopwords and numbers.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s-]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    filtered = [t for t in tokens\n",
    "                if t not in STOPWORDS\n",
    "                and len(t) > 1\n",
    "                and not any(char.isdigit() for char in t)]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "\n",
    "def extract_ngrams_from_text(text, min_n=2, max_n=3):\n",
    "    \"\"\"Extract n-grams from text.\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "\n",
    "    cleaned = clean_text(text)\n",
    "    tokens = cleaned.split()\n",
    "\n",
    "    ngrams = []\n",
    "    for n in range(min_n, max_n + 1):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = '_'.join(tokens[i:i + n])\n",
    "            ngrams.append(ngram)\n",
    "\n",
    "    return ' '.join(ngrams)\n",
    "\n",
    "\n",
    "def convert_feature_to_tuple(feature):\n",
    "    \"\"\"Convert a prominent feature phrase to an n-gram tuple.\"\"\"\n",
    "    words = feature.lower().strip().split()\n",
    "    words = [w for w in words if w not in STOPWORDS]\n",
    "    return '_'.join(words) if words else ''\n",
    "\n",
    "\n",
    "def combine_features_and_text(row, comments_col, parsed_col, min_n=2, max_n=3):\n",
    "    \"\"\"Combine prominent features from images with n-grams from text comments (deduplicated).\"\"\"\n",
    "    all_tokens = []\n",
    "\n",
    "    # 1. Extract and convert prominent_features from images\n",
    "    prominent_features = extract_prominent_features(row[parsed_col])\n",
    "    for feature in prominent_features:\n",
    "        feature_tuple = convert_feature_to_tuple(feature)\n",
    "        if feature_tuple:\n",
    "            all_tokens.append(feature_tuple)\n",
    "\n",
    "    # 2. Extract n-grams from text comments\n",
    "    comments_ngrams = extract_ngrams_from_text(row[comments_col], min_n, max_n)\n",
    "    if comments_ngrams:\n",
    "        all_tokens.extend(comments_ngrams.split())\n",
    "\n",
    "    # 3. DEDUPLICATE\n",
    "    seen = set()\n",
    "    unique_tokens = []\n",
    "    for token in all_tokens:\n",
    "        if token not in seen:\n",
    "            seen.add(token)\n",
    "            unique_tokens.append(token)\n",
    "\n",
    "    return ' '.join(unique_tokens)\n",
    "\n",
    "\n",
    "def load_and_prepare_data(csv_path, id_col, comments_col, parsed_col):\n",
    "    \"\"\"Load data from CSV and prepare combined feature documents.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded {len(df)} records from CSV\")\n",
    "\n",
    "        df = df[df[comments_col].notna() | df[parsed_col].notna()]\n",
    "        print(f\"Found {len(df)} records with comments or image data\")\n",
    "\n",
    "        print(\"\\nCombining image features and text comments...\")\n",
    "        df['combined_document'] = df.apply(\n",
    "            lambda row: combine_features_and_text(row, comments_col, parsed_col, MIN_N, MAX_N),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        df = df[df['combined_document'].str.strip() != '']\n",
    "        print(f\"Created {len(df)} combined documents for LDA\")\n",
    "\n",
    "        return df[id_col].tolist(), df['combined_document'].tolist()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CSV file not found at {csv_path}\")\n",
    "        return None, None\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Column {e} not found in CSV\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def sparsify_topics(lda_model, max_features_per_topic):\n",
    "    \"\"\"Limit each topic to only its top N features.\"\"\"\n",
    "    for topic_idx in range(lda_model.components_.shape[0]):\n",
    "        topic = lda_model.components_[topic_idx]\n",
    "        top_indices = topic.argsort()[-max_features_per_topic:]\n",
    "        mask = np.zeros_like(topic, dtype=bool)\n",
    "        mask[top_indices] = True\n",
    "        topic[~mask] = 0\n",
    "        if topic.sum() > 0:\n",
    "            lda_model.components_[topic_idx] = topic / topic.sum()\n",
    "\n",
    "    return lda_model\n",
    "\n",
    "\n",
    "def train_lda(documents, property_ids=None, n_topics=3, n_top_words=10, max_features=100):\n",
    "    \"\"\"Train LDA model on combined documents.\"\"\"\n",
    "\n",
    "    vectorizer = CountVectorizer(max_features=max_features, min_df=1)\n",
    "    doc_term_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "    print(f\"\\nVocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
    "    print(f\"Document-term matrix shape: {doc_term_matrix.shape}\")\n",
    "\n",
    "    print(f\"\\nTraining LDA with {n_topics} topics...\")\n",
    "    print(f\"Topic sparsity: max {MAX_FEATURES_PER_TOPIC} features per topic\")\n",
    "\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        topic_word_prior=TOPIC_WORD_PRIOR,\n",
    "        random_state=42,\n",
    "        max_iter=20,\n",
    "        learning_method='batch'\n",
    "    )\n",
    "    lda.fit(doc_term_matrix)\n",
    "\n",
    "    lda = sparsify_topics(lda, MAX_FEATURES_PER_TOPIC)\n",
    "\n",
    "    # Display topics\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DISCOVERED TOPICS (Image Features + Text)\")\n",
    "    print(f\"[Limited to {MAX_FEATURES_PER_TOPIC} features per topic]\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for topic_idx in range(lda.components_.shape[0]):\n",
    "        topic = lda.components_[topic_idx]\n",
    "\n",
    "        non_zero_indices = np.where(topic > 0)[0]\n",
    "        weights = topic[non_zero_indices]\n",
    "        sorted_idx = np.argsort(weights)[::-1]\n",
    "        top_n = min(n_top_words, len(sorted_idx))\n",
    "\n",
    "        print(f\"\\nTopic {topic_idx + 1} ({len(non_zero_indices)} total features):\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        for i in range(top_n):\n",
    "            feature_idx = non_zero_indices[sorted_idx[i]]\n",
    "            feature_name = feature_names[feature_idx]\n",
    "            feature_weight = topic[feature_idx]\n",
    "            print(f\"  {feature_name:<40} {feature_weight:.4f}\")\n",
    "\n",
    "    # Document-topic distribution\n",
    "    doc_topics = lda.transform(doc_term_matrix)\n",
    "    avg_topic_dist = doc_topics.mean(axis=0)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"AVERAGE TOPIC DISTRIBUTION ACROSS ALL DOCUMENTS\")\n",
    "    print(\"=\" * 60)\n",
    "    for topic_idx, avg_prob in enumerate(avg_topic_dist):\n",
    "        print(f\"  Topic {topic_idx + 1}: {avg_prob:.1%}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DOCUMENT-TOPIC DISTRIBUTION (Sample)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    num_to_show = min(10, len(doc_topics))\n",
    "    for i in range(num_to_show):\n",
    "        prop_id = property_ids[i] if property_ids else f\"Doc {i + 1}\"\n",
    "        print(f\"\\nProperty {prop_id}:\")\n",
    "        for topic_idx, prob in enumerate(doc_topics[i]):\n",
    "            if prob > 0.1:\n",
    "                print(f\"  Topic {topic_idx + 1}: {prob:.3f}\")\n",
    "\n",
    "    if len(doc_topics) > num_to_show:\n",
    "        print(f\"\\n... ({len(doc_topics) - num_to_show} more documents)\")\n",
    "\n",
    "    return lda, vectorizer, doc_topics\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"COMBINED IMAGE + TEXT LDA TOPIC MODELING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    property_ids, documents = load_and_prepare_data(\n",
    "        CSV_FILE_PATH,\n",
    "        PROPERTY_ID_COL,\n",
    "        COMMENTS_COL,\n",
    "        PARSED_OUTPUT_COL\n",
    "    )\n",
    "\n",
    "    if documents is None or len(documents) == 0:\n",
    "        print(\"\\nNo documents found. Exiting.\")\n",
    "    else:\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"TRAINING LDA ON {len(documents)} COMBINED DOCUMENTS\")\n",
    "        print(f\"Text n-gram range: {MIN_N}-{MAX_N}\")\n",
    "        print(f\"Number of topics: {N_TOPICS}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        lda_model, vectorizer, doc_topics = train_lda(\n",
    "            documents,\n",
    "            property_ids=property_ids,\n",
    "            n_topics=N_TOPICS,\n",
    "            n_top_words=N_TOP_WORDS,\n",
    "            max_features=MAX_FEATURES\n",
    "        )\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ANALYSIS COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\\nData streams combined:\")\n",
    "        print(\"  • Image prominent_features (converted to tuples)\")\n",
    "        print(\"  • Text comments (parsed into n-grams)\")\n",
    "        print(\"\\nTo improve results:\")\n",
    "        print(\"  • Adjust N_TOPICS based on your corpus size\")\n",
    "        print(\"  • Increase MAX_FEATURES for larger vocabulary\")\n",
    "        print(\"  • Tune MIN_N and MAX_N for different n-gram sizes\")\n",
    "        print(\"  • Add more domain-specific stopwords if needed\")"
   ],
   "id": "ea99384525732e61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMBINED IMAGE + TEXT LDA TOPIC MODELING\n",
      "============================================================\n",
      "Loaded 15062 records from CSV\n",
      "Found 14662 records with comments or image data\n",
      "\n",
      "Combining image features and text comments...\n",
      "Created 14662 combined documents for LDA\n",
      "\n",
      "============================================================\n",
      "TRAINING LDA ON 14662 COMBINED DOCUMENTS\n",
      "Text n-gram range: 2-3\n",
      "Number of topics: 8\n",
      "============================================================\n",
      "\n",
      "Vocabulary size: 200\n",
      "Document-term matrix shape: (14662, 200)\n",
      "\n",
      "Training LDA with 8 topics...\n",
      "Topic sparsity: max 20 features per topic\n",
      "\n",
      "============================================================\n",
      "DISCOVERED TOPICS (Image Features + Text)\n",
      "[Limited to 20 features per topic]\n",
      "============================================================\n",
      "\n",
      "Topic 1 (20 total features):\n",
      "----------------------------------------\n",
      "  well                                     0.1841\n",
      "  built                                    0.1566\n",
      "  natural_light                            0.0631\n",
      "  hardwood_floors                          0.0597\n",
      "  ceiling_fan                              0.0445\n",
      "  hardwood_flooring                        0.0432\n",
      "  in_shelving                              0.0409\n",
      "  stainless_steel_appliances               0.0404\n",
      "  granite_countertops                      0.0391\n",
      "  tile_flooring                            0.0379\n",
      "\n",
      "Topic 2 (20 total features):\n",
      "----------------------------------------\n",
      "  two                                      0.0975\n",
      "  _natural_light                           0.0878\n",
      "  neutral_paint                            0.0677\n",
      "  natural_light                            0.0638\n",
      "  ceiling_fan                              0.0631\n",
      "  car_garage                               0.0615\n",
      "  neutral_finishes                         0.0608\n",
      "  carpeted_floor                           0.0528\n",
      "  built                                    0.0500\n",
      "  tile_flooring                            0.0489\n",
      "\n",
      "Topic 3 (20 total features):\n",
      "----------------------------------------\n",
      "  in                                       0.3402\n",
      "  walk                                     0.1538\n",
      "  in_closet                                0.0876\n",
      "  built                                    0.0570\n",
      "  move                                     0.0497\n",
      "  natural_light                            0.0362\n",
      "  well                                     0.0306\n",
      "  master_suite                             0.0273\n",
      "  dining_room                              0.0253\n",
      "  living_room                              0.0238\n",
      "\n",
      "Topic 4 (20 total features):\n",
      "----------------------------------------\n",
      "  well                                     0.1548\n",
      "  car_garage                               0.1408\n",
      "  attached_two                             0.0942\n",
      "  two                                      0.0938\n",
      "  eat                                      0.0921\n",
      "  front_porch                              0.0565\n",
      "  in_kitchen                               0.0411\n",
      "  concrete_driveway                        0.0343\n",
      "  single                                   0.0338\n",
      "  living_room                              0.0324\n",
      "\n",
      "Topic 5 (20 total features):\n",
      "----------------------------------------\n",
      "  wood                                     0.1241\n",
      "  look_flooring                            0.0993\n",
      "  laminate_flooring                        0.0714\n",
      "  natural_light                            0.0667\n",
      "  to                                       0.0630\n",
      "  hardwood                                 0.0568\n",
      "  laminate                                 0.0529\n",
      "  neutral_paint                            0.0497\n",
      "  built                                    0.0486\n",
      "  ceiling_fan                              0.0470\n",
      "\n",
      "Topic 6 (20 total features):\n",
      "----------------------------------------\n",
      "  natural_light                            0.1205\n",
      "  built                                    0.1008\n",
      "  mature_trees                             0.0809\n",
      "  hardwood_floors                          0.0660\n",
      "  ceiling_fan                              0.0569\n",
      "  neutral_paint                            0.0518\n",
      "  large_windows                            0.0453\n",
      "  carpeted_floor                           0.0449\n",
      "  well                                     0.0420\n",
      "  wainscoting                              0.0412\n",
      "\n",
      "Topic 7 (20 total features):\n",
      "----------------------------------------\n",
      "  de                                       0.3069\n",
      "  cul                                      0.1240\n",
      "  sac                                      0.1157\n",
      "  well                                     0.0923\n",
      "  single                                   0.0481\n",
      "  ceiling_fan                              0.0361\n",
      "  white_cabinetry                          0.0270\n",
      "  eat                                      0.0244\n",
      "  porch                                    0.0219\n",
      "  vaulted_ceiling                          0.0215\n",
      "\n",
      "Topic 8 (20 total features):\n",
      "----------------------------------------\n",
      "  single                                   0.0942\n",
      "  well                                     0.0815\n",
      "  neutral_paint                            0.0801\n",
      "  ceiling_fan                              0.0770\n",
      "  carpeted_floor                           0.0690\n",
      "  natural_light                            0.0614\n",
      "  vinyl                                    0.0574\n",
      "  tub                                      0.0511\n",
      "  neutral_finishes                         0.0488\n",
      "  tile_flooring                            0.0452\n",
      "\n",
      "============================================================\n",
      "AVERAGE TOPIC DISTRIBUTION ACROSS ALL DOCUMENTS\n",
      "============================================================\n",
      "  Topic 1: 22.1%\n",
      "  Topic 2: 15.0%\n",
      "  Topic 3: 6.0%\n",
      "  Topic 4: 11.1%\n",
      "  Topic 5: 6.7%\n",
      "  Topic 6: 17.2%\n",
      "  Topic 7: 3.3%\n",
      "  Topic 8: 18.7%\n",
      "\n",
      "============================================================\n",
      "DOCUMENT-TOPIC DISTRIBUTION (Sample)\n",
      "============================================================\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 1: 0.449\n",
      "  Topic 6: 0.462\n",
      "\n",
      "Property 143995964:\n",
      "  Topic 5: 0.647\n",
      "  Topic 8: 0.333\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 1: 0.438\n",
      "  Topic 6: 0.474\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 1: 0.449\n",
      "  Topic 6: 0.462\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 1: 0.438\n",
      "  Topic 6: 0.474\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 1: 0.449\n",
      "  Topic 6: 0.462\n",
      "\n",
      "Property 143697419:\n",
      "  Topic 4: 0.236\n",
      "  Topic 6: 0.701\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 1: 0.449\n",
      "  Topic 6: 0.462\n",
      "\n",
      "Property 143995964:\n",
      "  Topic 5: 0.647\n",
      "  Topic 8: 0.333\n",
      "\n",
      "Property 143697491:\n",
      "  Topic 1: 0.449\n",
      "  Topic 6: 0.462\n",
      "\n",
      "... (14652 more documents)\n",
      "\n",
      "============================================================\n",
      "ANALYSIS COMPLETE\n",
      "============================================================\n",
      "\n",
      "Data streams combined:\n",
      "  • Image prominent_features (converted to tuples)\n",
      "  • Text comments (parsed into n-grams)\n",
      "\n",
      "To improve results:\n",
      "  • Adjust N_TOPICS based on your corpus size\n",
      "  • Increase MAX_FEATURES for larger vocabulary\n",
      "  • Tune MIN_N and MAX_N for different n-gram sizes\n",
      "  • Add more domain-specific stopwords if needed\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T22:39:08.718417Z",
     "start_time": "2025-12-19T22:38:41.507446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import ast\n",
    "import re\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CONFIG\n",
    "UNIFIED_DATA_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/input_census_neighborhood_image_data_12K.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "MIN_PRICE = 100000\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# LDA CONFIG (from v12 script)\n",
    "COMMENTS_COL = 'public_listing_comments'  # lowercase to match df.columns.str.lower()\n",
    "PARSED_OUTPUT_COL = 'parsed_output'  # lowercase to match df.columns.str.lower()\n",
    "N_TOPICS = 8\n",
    "MIN_N = 2\n",
    "MAX_N = 3\n",
    "MAX_FEATURES = 200\n",
    "MAX_FEATURES_PER_TOPIC = 20\n",
    "TOPIC_WORD_PRIOR = 0.01\n",
    "\n",
    "STOPWORDS = {\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
    "    'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was', 'will',\n",
    "    'with', 'w/', 'been', 'all', 'this', 'you', 'into', 'offers',\n",
    "    'offered', 'includes', 'some', 'tons', 'nice', 'ample', 'yrs',\n",
    "    'adorable', 'charm', 'character', 'beautiful', 'stunning', 'unique',\n",
    "    'modern', 'decorative', 'tastefully', 'tasteful', 'convenient', 'spacious',\n",
    "    'plus', 'lovely', 'charming', 'gorgeous', 'amazing', 'wonderful', 'perfect',\n",
    "    'excellent', 'fantastic', 'incredible', 'magnificent', 'spectacular',\n",
    "    'delightful', 'elegant', 'cozy', 'inviting', 'warm', 'bright', 'airy',\n",
    "    'luxurious', 'premium', 'quality', 'great', 'good', 'better', 'best',\n",
    "    'original', 'covered', 'welcomes', 'showcasing', 'spanning', 'framed',\n",
    "    'abound', 'throughout'\n",
    "}\n",
    "\n",
    "# Price tiers\n",
    "PRICE_TIERS = {\n",
    "    'very_low': (0, 200000),\n",
    "    'low': (200000, 300000),\n",
    "    'lower_mid': (300000, 400000),\n",
    "    'mid': (400000, 500000),\n",
    "    'upper_mid': (500000, 650000),\n",
    "    'high': (650000, 850000),\n",
    "    'very_high': (850000, 1500000),\n",
    "    'ultra_high': (1500000, np.inf)\n",
    "}\n",
    "\n",
    "COLUMN_MAP = {\n",
    "    'sumlivingareasqft': 'living_sqft',\n",
    "    'lotsizesqft': 'lot_sqft',\n",
    "    'sumbuildingsqft': 'building_sqft',\n",
    "    'sumbasementsqft': 'basement_sqft',\n",
    "    'basementfinishedsqft': 'basement_finished_sqft',\n",
    "    'sumgaragesqft': 'garage_sqft',\n",
    "    'sumgrossareasqft': 'gross_sqft'\n",
    "}\n",
    "\n",
    "BASE_FEATURES = [\"living_sqft\", \"lot_sqft\", \"year_built\", \"bedrooms\",\n",
    "                 \"full_baths\", \"half_baths\", \"garage_spaces\", \"latitude\", \"longitude\"]\n",
    "\n",
    "CENSUS_FEATURES = [\"pct_bachelors_degree\", \"median_household_income\",\n",
    "                   \"median_home_value\", \"pct_owner_occupied\", \"unemployment_rate\"]\n",
    "\n",
    "IMAGE_CONDITION = [\"gran_c_in\", \"gran_c_ex\", \"gran_c\", \"high_c_in\", \"high_c_ex\", \"high_c\"]\n",
    "\n",
    "IMAGE_BOOLEAN = []  # Add if you have boolean image features\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LDA FUNCTIONS (from v12 script)\n",
    "# ============================================================\n",
    "\n",
    "def extract_prominent_features(parsed_output):\n",
    "    \"\"\"Extract all prominent_features from the parsed output dictionary.\"\"\"\n",
    "    if pd.isna(parsed_output):\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        if isinstance(parsed_output, str):\n",
    "            data = ast.literal_eval(parsed_output)\n",
    "        else:\n",
    "            data = parsed_output\n",
    "\n",
    "        all_features = []\n",
    "        for idx, image_data in data.items():\n",
    "            if 'prominent_features' in image_data:\n",
    "                all_features.extend(image_data['prominent_features'])\n",
    "\n",
    "        return all_features\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and tokenize text.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s-]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    filtered = [t for t in tokens\n",
    "                if t not in STOPWORDS\n",
    "                and len(t) > 1\n",
    "                and not any(char.isdigit() for char in t)]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "\n",
    "def extract_ngrams_from_text(text, min_n=2, max_n=3):\n",
    "    \"\"\"Extract n-grams from text.\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "\n",
    "    cleaned = clean_text(text)\n",
    "    tokens = cleaned.split()\n",
    "\n",
    "    ngrams = []\n",
    "    for n in range(min_n, max_n + 1):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = '_'.join(tokens[i:i + n])\n",
    "            ngrams.append(ngram)\n",
    "\n",
    "    return ' '.join(ngrams)\n",
    "\n",
    "\n",
    "def convert_feature_to_tuple(feature):\n",
    "    \"\"\"Convert a prominent feature phrase to an n-gram tuple.\"\"\"\n",
    "    words = feature.lower().strip().split()\n",
    "    words = [w for w in words if w not in STOPWORDS]\n",
    "    return '_'.join(words) if words else ''\n",
    "\n",
    "\n",
    "def combine_features_and_text(row, comments_col, parsed_col, min_n=2, max_n=3):\n",
    "    \"\"\"Combine prominent features from images with n-grams from text comments.\"\"\"\n",
    "    all_tokens = []\n",
    "\n",
    "    prominent_features = extract_prominent_features(row[parsed_col])\n",
    "    for feature in prominent_features:\n",
    "        feature_tuple = convert_feature_to_tuple(feature)\n",
    "        if feature_tuple:\n",
    "            all_tokens.append(feature_tuple)\n",
    "\n",
    "    comments_ngrams = extract_ngrams_from_text(row[comments_col], min_n, max_n)\n",
    "    if comments_ngrams:\n",
    "        all_tokens.extend(comments_ngrams.split())\n",
    "\n",
    "    # Deduplicate\n",
    "    seen = set()\n",
    "    unique_tokens = []\n",
    "    for token in all_tokens:\n",
    "        if token not in seen:\n",
    "            seen.add(token)\n",
    "            unique_tokens.append(token)\n",
    "\n",
    "    return ' '.join(unique_tokens)\n",
    "\n",
    "\n",
    "def sparsify_topics(lda_model, max_features_per_topic):\n",
    "    \"\"\"Limit each topic to only its top N features.\"\"\"\n",
    "    for topic_idx in range(lda_model.components_.shape[0]):\n",
    "        topic = lda_model.components_[topic_idx]\n",
    "        top_indices = topic.argsort()[-max_features_per_topic:]\n",
    "        mask = np.zeros_like(topic, dtype=bool)\n",
    "        mask[top_indices] = True\n",
    "        topic[~mask] = 0\n",
    "        if topic.sum() > 0:\n",
    "            lda_model.components_[topic_idx] = topic / topic.sum()\n",
    "\n",
    "    return lda_model\n",
    "\n",
    "\n",
    "def train_lda_model(df):\n",
    "    \"\"\"Train LDA model on combined image+text features.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING LDA MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create combined documents\n",
    "    print(\"Combining image features and text comments...\")\n",
    "    df['combined_document'] = df.apply(\n",
    "        lambda row: combine_features_and_text(row, COMMENTS_COL, PARSED_OUTPUT_COL, MIN_N, MAX_N),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    documents = df['combined_document'].tolist()\n",
    "    print(f\"Created {len(documents)} documents\")\n",
    "\n",
    "    # Train LDA\n",
    "    vectorizer = CountVectorizer(max_features=MAX_FEATURES, min_df=1)\n",
    "    doc_term_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "    print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
    "    print(f\"Training LDA with {N_TOPICS} topics...\")\n",
    "\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=N_TOPICS,\n",
    "        topic_word_prior=TOPIC_WORD_PRIOR,\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=20,\n",
    "        learning_method='batch'\n",
    "    )\n",
    "    lda.fit(doc_term_matrix)\n",
    "    lda = sparsify_topics(lda, MAX_FEATURES_PER_TOPIC)\n",
    "\n",
    "    # Get topic distributions for all documents\n",
    "    doc_topics = lda.transform(doc_term_matrix)\n",
    "\n",
    "    # Add topic features to dataframe\n",
    "    for i in range(N_TOPICS):\n",
    "        df[f'topic_{i+1}'] = doc_topics[:, i]\n",
    "\n",
    "    print(f\"✓ Added {N_TOPICS} topic features to dataframe\")\n",
    "\n",
    "    return df, lda, vectorizer\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# XGBOOST FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def load_and_prep(filepath):\n",
    "    \"\"\"Load and prep data.\"\"\"\n",
    "    print(f\"\\nLoading {filepath}...\")\n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df = df.rename(columns=COLUMN_MAP)\n",
    "\n",
    "    price_col = next((c for c in ['sale_price', 'currentsalesprice', 'price'] if c in df.columns), None)\n",
    "    if not price_col:\n",
    "        raise ValueError(\"No price column found\")\n",
    "\n",
    "    if 'living_sqft' not in df.columns:\n",
    "        raise ValueError(\"living_sqft not found\")\n",
    "\n",
    "    df = df[df[price_col] >= MIN_PRICE].copy()\n",
    "    print(f\"Records after price filter: {len(df):,}\")\n",
    "\n",
    "    # Engineer features\n",
    "    df['sqft_per_bedroom'] = df['living_sqft'] / (df['bedrooms'] + 1)\n",
    "    df['lot_to_living_ratio'] = df['lot_sqft'] / (df['living_sqft'] + 1)\n",
    "    df['property_age'] = 2024 - df['year_built']\n",
    "    df['has_garage'] = (df['garage_spaces'] > 0).astype('int8')\n",
    "    df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "\n",
    "    if 'basement_finished_sqft' in df.columns and 'basement_sqft' in df.columns:\n",
    "        df['basement_finished_ratio'] = df['basement_finished_sqft'] / (df['basement_sqft'] + 1)\n",
    "\n",
    "    if 'garage_sqft' in df.columns:\n",
    "        df['has_large_garage'] = (df['garage_sqft'] > 500).astype('int8')\n",
    "\n",
    "    if 'gross_sqft' in df.columns:\n",
    "        df['living_to_gross_ratio'] = df['living_sqft'] / (df['gross_sqft'] + 1)\n",
    "\n",
    "    # Geo clusters\n",
    "    valid = df[['latitude', 'longitude']].notna().all(axis=1)\n",
    "    if valid.sum() >= 8:\n",
    "        kmeans = MiniBatchKMeans(n_clusters=8, random_state=RANDOM_STATE, batch_size=1000)\n",
    "        df.loc[valid, 'geo_cluster'] = kmeans.fit_predict(df.loc[valid, ['latitude', 'longitude']])\n",
    "        df['geo_cluster'] = df['geo_cluster'].fillna(0)\n",
    "    else:\n",
    "        df['geo_cluster'] = 0\n",
    "\n",
    "    # Assign price tiers\n",
    "    df['price_tier'] = df[price_col].apply(\n",
    "        lambda p: next((t for t, (l, h) in PRICE_TIERS.items() if l <= p < h), 'ultra_high'))\n",
    "\n",
    "    return df, price_col\n",
    "\n",
    "\n",
    "def train_tier(tier_df, features, price_col, tier_name, include_topics=True):\n",
    "    \"\"\"Train one tier with or without topic features.\"\"\"\n",
    "\n",
    "    # Split data\n",
    "    train_df, test_df = train_test_split(tier_df, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Calculate cluster features from training data only\n",
    "    if 'geo_cluster' in train_df.columns:\n",
    "        train_stats = train_df.groupby('geo_cluster')[price_col].agg(['mean', 'median']).reset_index()\n",
    "        train_stats.columns = ['geo_cluster', 'cluster_avg_price', 'cluster_med_price']\n",
    "\n",
    "        train_df = train_df.drop(['cluster_avg_price', 'cluster_med_price'], axis=1, errors='ignore')\n",
    "        train_df = train_df.merge(train_stats, on='geo_cluster', how='left')\n",
    "\n",
    "        test_df = test_df.drop(['cluster_avg_price', 'cluster_med_price'], axis=1, errors='ignore')\n",
    "        test_df = test_df.merge(train_stats, on='geo_cluster', how='left')\n",
    "\n",
    "        train_median = train_df[price_col].median()\n",
    "        train_df[['cluster_avg_price', 'cluster_med_price']] = train_df[\n",
    "            ['cluster_avg_price', 'cluster_med_price']].fillna(train_median)\n",
    "        test_df[['cluster_avg_price', 'cluster_med_price']] = test_df[\n",
    "            ['cluster_avg_price', 'cluster_med_price']].fillna(train_median)\n",
    "\n",
    "        features_with_cluster = features + ['cluster_avg_price', 'cluster_med_price']\n",
    "    else:\n",
    "        features_with_cluster = features\n",
    "\n",
    "    # Impute using training data only\n",
    "    train_medians = train_df[features_with_cluster].median()\n",
    "    train_df[features_with_cluster] = train_df[features_with_cluster].fillna(train_medians)\n",
    "    test_df[features_with_cluster] = test_df[features_with_cluster].fillna(train_medians)\n",
    "\n",
    "    X_train, y_train = train_df[features_with_cluster].values, train_df[price_col].values\n",
    "    X_test, y_test = test_df[features_with_cluster].values, test_df[price_col].values\n",
    "\n",
    "    # Log transform\n",
    "    y_train_model = np.log1p(y_train)\n",
    "\n",
    "    # Train quantile model (median)\n",
    "    model = XGBRegressor(\n",
    "        objective='reg:quantileerror',\n",
    "        quantile_alpha=0.5,\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        tree_method='hist',\n",
    "        verbosity=0\n",
    "    )\n",
    "    model.fit(X_train, y_train_model, verbose=False)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = np.expm1(model.predict(X_test))\n",
    "\n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'metrics': {\n",
    "            'n_train': len(X_train),\n",
    "            'n_test': len(X_test),\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'r2': r2\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution with and without LDA topics.\"\"\"\n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STRATIFIED AVM WITH LDA TOPICS COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Load data\n",
    "    df, price_col = load_and_prep(UNIFIED_DATA_PATH)\n",
    "\n",
    "    # Debug: Check what columns we have\n",
    "    print(f\"\\n🔍 DEBUG: Checking for LDA columns...\")\n",
    "    print(f\"Looking for: '{COMMENTS_COL}' and '{PARSED_OUTPUT_COL}'\")\n",
    "\n",
    "    # Show columns that contain 'comment' or 'parsed'\n",
    "    comment_cols = [c for c in df.columns if 'comment' in c.lower()]\n",
    "    parsed_cols = [c for c in df.columns if 'parsed' in c.lower() or 'output' in c.lower()]\n",
    "\n",
    "    if comment_cols:\n",
    "        print(f\"Found comment-related columns: {comment_cols}\")\n",
    "    if parsed_cols:\n",
    "        print(f\"Found parsed/output-related columns: {parsed_cols}\")\n",
    "\n",
    "    if not comment_cols and not parsed_cols:\n",
    "        print(f\"No comment or parsed columns found in dataframe\")\n",
    "        print(f\"First 10 columns: {df.columns.tolist()[:10]}\")\n",
    "\n",
    "    # Check if we have necessary columns for LDA\n",
    "    if COMMENTS_COL not in df.columns or PARSED_OUTPUT_COL not in df.columns:\n",
    "        print(f\"\\n⚠ Missing {COMMENTS_COL} or {PARSED_OUTPUT_COL} - skipping LDA\")\n",
    "        print(\"Running baseline model only...\")\n",
    "        has_lda = False\n",
    "    else:\n",
    "        # Train LDA and add topic features\n",
    "        df, lda_model, vectorizer = train_lda_model(df)\n",
    "        has_lda = True\n",
    "\n",
    "    # Get base features\n",
    "    all_base_features = BASE_FEATURES + CENSUS_FEATURES + IMAGE_CONDITION + IMAGE_BOOLEAN + [\n",
    "        'sqft_per_bedroom', 'lot_to_living_ratio', 'property_age', 'has_garage',\n",
    "        'log_sqft', 'geo_cluster', 'cluster_avg_price', 'cluster_med_price',\n",
    "        'basement_finished_ratio', 'has_large_garage', 'living_to_gross_ratio'\n",
    "    ]\n",
    "    base_features = [f for f in all_base_features if f in df.columns]\n",
    "\n",
    "    # Add topic features if available\n",
    "    topic_features = [f'topic_{i+1}' for i in range(N_TOPICS)] if has_lda else []\n",
    "    features_with_topics = base_features + topic_features\n",
    "\n",
    "    # Fill missing values\n",
    "    df[base_features] = df[base_features].fillna(df[base_features].median())\n",
    "    if has_lda:\n",
    "        df[topic_features] = df[topic_features].fillna(0)\n",
    "    df = df.dropna(subset=[price_col])\n",
    "\n",
    "    print(f\"\\nBase features: {len(base_features)}\")\n",
    "    if has_lda:\n",
    "        print(f\"Topic features: {len(topic_features)}\")\n",
    "        print(f\"Total with topics: {len(features_with_topics)}\")\n",
    "\n",
    "    # Train both models (with and without topics)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING MODELS BY PRICE TIER\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results_baseline = {}\n",
    "    results_with_topics = {} if has_lda else None\n",
    "\n",
    "    comparison_data = []\n",
    "\n",
    "    for tier_name, (low, high) in PRICE_TIERS.items():\n",
    "        tier_df = df[df['price_tier'] == tier_name].copy()\n",
    "\n",
    "        if len(tier_df) < 50:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{tier_name} (${low/1000:.0f}K-${high/1000:.0f}K): {len(tier_df):,} samples\")\n",
    "\n",
    "        # Train baseline (without topics)\n",
    "        result_baseline = train_tier(tier_df, base_features, price_col, tier_name, include_topics=False)\n",
    "        results_baseline[tier_name] = result_baseline\n",
    "\n",
    "        mae_base = result_baseline['metrics']['mae']\n",
    "        mape_base = result_baseline['metrics']['mape']\n",
    "        r2_base = result_baseline['metrics']['r2']\n",
    "\n",
    "        print(f\"  Baseline:     MAE=${mae_base:,.0f} | MAPE={mape_base:.2f}% | R²={r2_base:.3f}\")\n",
    "\n",
    "        # Train with topics if available\n",
    "        if has_lda:\n",
    "            result_topics = train_tier(tier_df, features_with_topics, price_col, tier_name, include_topics=True)\n",
    "            results_with_topics[tier_name] = result_topics\n",
    "\n",
    "            mae_topics = result_topics['metrics']['mae']\n",
    "            mape_topics = result_topics['metrics']['mape']\n",
    "            r2_topics = result_topics['metrics']['r2']\n",
    "\n",
    "            # Calculate improvement\n",
    "            mape_improvement = ((mape_base - mape_topics) / mape_base) * 100\n",
    "\n",
    "            print(f\"  With Topics:  MAE=${mae_topics:,.0f} | MAPE={mape_topics:.2f}% | R²={r2_topics:.3f}\")\n",
    "\n",
    "            improvement_symbol = \"↓\" if mape_topics < mape_base else \"↑\"\n",
    "            print(f\"  Δ MAPE:       {improvement_symbol} {abs(mape_improvement):.2f}%\")\n",
    "\n",
    "            comparison_data.append({\n",
    "                'tier': tier_name,\n",
    "                'n_samples': len(tier_df),\n",
    "                'baseline_mape': mape_base,\n",
    "                'topics_mape': mape_topics,\n",
    "                'mape_improvement_pct': mape_improvement,\n",
    "                'baseline_mae': mae_base,\n",
    "                'topics_mae': mae_topics,\n",
    "                'baseline_r2': r2_base,\n",
    "                'topics_r2': r2_topics\n",
    "            })\n",
    "\n",
    "    # Overall summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"OVERALL COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if has_lda and comparison_data:\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "        avg_baseline_mape = comparison_df['baseline_mape'].mean()\n",
    "        avg_topics_mape = comparison_df['topics_mape'].mean()\n",
    "        overall_improvement = ((avg_baseline_mape - avg_topics_mape) / avg_baseline_mape) * 100\n",
    "\n",
    "        print(f\"\\nAverage MAPE (Baseline):    {avg_baseline_mape:.2f}%\")\n",
    "        print(f\"Average MAPE (With Topics): {avg_topics_mape:.2f}%\")\n",
    "        print(f\"Overall Improvement:        {overall_improvement:+.2f}%\")\n",
    "\n",
    "        # Save comparison\n",
    "        import os\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        comparison_df.to_csv(f\"{OUTPUT_DIR}/lda_comparison.csv\", index=False)\n",
    "\n",
    "        print(f\"\\n✓ Saved comparison to {OUTPUT_DIR}/lda_comparison.csv\")\n",
    "\n",
    "    print(f\"\\nTotal time: {time.time() - start:.1f}s\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "a76559eed55c4afc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STRATIFIED AVM WITH LDA TOPICS COMPARISON\n",
      "============================================================\n",
      "\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/input_census_neighborhood_image_data_12K.csv...\n",
      "Records after price filter: 9,366\n",
      "\n",
      "🔍 DEBUG: Checking for LDA columns...\n",
      "Looking for: 'public_listing_comments' and 'parsed_output'\n",
      "Found comment-related columns: ['public_listing_comments']\n",
      "Found parsed/output-related columns: ['parsed_output']\n",
      "\n",
      "============================================================\n",
      "TRAINING LDA MODEL\n",
      "============================================================\n",
      "Combining image features and text comments...\n",
      "Created 9366 documents\n",
      "Vocabulary size: 200\n",
      "Training LDA with 8 topics...\n",
      "✓ Added 8 topic features to dataframe\n",
      "\n",
      "Base features: 26\n",
      "Topic features: 8\n",
      "Total with topics: 34\n",
      "\n",
      "============================================================\n",
      "TRAINING MODELS BY PRICE TIER\n",
      "============================================================\n",
      "\n",
      "very_low ($0K-$200K): 3,183 samples\n",
      "  Baseline:     MAE=$10,056 | MAPE=6.75% | R²=0.572\n",
      "  With Topics:  MAE=$10,340 | MAPE=6.98% | R²=0.579\n",
      "  Δ MAPE:       ↑ 3.47%\n",
      "\n",
      "low ($200K-$300K): 2,526 samples\n",
      "  Baseline:     MAE=$8,385 | MAPE=3.34% | R²=0.604\n",
      "  With Topics:  MAE=$8,684 | MAPE=3.46% | R²=0.601\n",
      "  Δ MAPE:       ↑ 3.66%\n",
      "\n",
      "lower_mid ($300K-$400K): 1,255 samples\n",
      "  Baseline:     MAE=$9,036 | MAPE=2.57% | R²=0.566\n",
      "  With Topics:  MAE=$9,327 | MAPE=2.67% | R²=0.577\n",
      "  Δ MAPE:       ↑ 3.66%\n",
      "\n",
      "mid ($400K-$500K): 809 samples\n",
      "  Baseline:     MAE=$9,142 | MAPE=2.07% | R²=0.608\n",
      "  With Topics:  MAE=$9,649 | MAPE=2.19% | R²=0.593\n",
      "  Δ MAPE:       ↑ 5.63%\n",
      "\n",
      "upper_mid ($500K-$650K): 679 samples\n",
      "  Baseline:     MAE=$10,620 | MAPE=1.78% | R²=0.677\n",
      "  With Topics:  MAE=$10,617 | MAPE=1.78% | R²=0.674\n",
      "  Δ MAPE:       ↑ 0.09%\n",
      "\n",
      "high ($650K-$850K): 412 samples\n",
      "  Baseline:     MAE=$10,938 | MAPE=1.52% | R²=0.779\n",
      "  With Topics:  MAE=$11,940 | MAPE=1.66% | R²=0.744\n",
      "  Δ MAPE:       ↑ 9.21%\n",
      "\n",
      "very_high ($850K-$1500K): 259 samples\n",
      "  Baseline:     MAE=$63,655 | MAPE=5.31% | R²=0.633\n",
      "  With Topics:  MAE=$60,313 | MAPE=4.94% | R²=0.643\n",
      "  Δ MAPE:       ↓ 6.98%\n",
      "\n",
      "ultra_high ($1500K-$infK): 243 samples\n",
      "  Baseline:     MAE=$5,803,556 | MAPE=27.39% | R²=0.026\n",
      "  With Topics:  MAE=$2,829,670 | MAPE=20.86% | R²=0.533\n",
      "  Δ MAPE:       ↓ 23.85%\n",
      "\n",
      "============================================================\n",
      "OVERALL COMPARISON\n",
      "============================================================\n",
      "\n",
      "Average MAPE (Baseline):    6.34%\n",
      "Average MAPE (With Topics): 5.57%\n",
      "Overall Improvement:        +12.21%\n",
      "\n",
      "✓ Saved comparison to /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/lda_comparison.csv\n",
      "\n",
      "Total time: 27.2s\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T22:51:29.159036Z",
     "start_time": "2025-12-19T22:51:07.159117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import ast\n",
    "import re\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CONFIG\n",
    "UNIFIED_DATA_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/input_census_neighborhood_image_data_12K.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "MIN_PRICE = 100000\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# LDA CONFIG (from v12 script)\n",
    "COMMENTS_COL = 'public_listing_comments'  # lowercase to match df.columns.str.lower()\n",
    "PARSED_OUTPUT_COL = 'parsed_output'  # lowercase to match df.columns.str.lower()\n",
    "N_TOPICS = 8\n",
    "MIN_N = 2\n",
    "MAX_N = 3\n",
    "MAX_FEATURES = 200\n",
    "MAX_FEATURES_PER_TOPIC = 20\n",
    "TOPIC_WORD_PRIOR = 0.01\n",
    "\n",
    "STOPWORDS = {\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
    "    'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was', 'will',\n",
    "    'with', 'w/', 'been', 'all', 'this', 'you', 'into', 'offers',\n",
    "    'offered', 'includes', 'some', 'tons', 'nice', 'ample', 'yrs',\n",
    "    'adorable', 'charm', 'character', 'beautiful', 'stunning', 'unique',\n",
    "    'modern', 'decorative', 'tastefully', 'tasteful', 'convenient', 'spacious',\n",
    "    'plus', 'lovely', 'charming', 'gorgeous', 'amazing', 'wonderful', 'perfect',\n",
    "    'excellent', 'fantastic', 'incredible', 'magnificent', 'spectacular',\n",
    "    'delightful', 'elegant', 'cozy', 'inviting', 'warm', 'bright', 'airy',\n",
    "    'luxurious', 'premium', 'quality', 'great', 'good', 'better', 'best',\n",
    "    'original', 'covered', 'welcomes', 'showcasing', 'spanning', 'framed',\n",
    "    'abound', 'throughout'\n",
    "}\n",
    "\n",
    "# Price tiers\n",
    "PRICE_TIERS = {\n",
    "    'very_low': (0, 200000),\n",
    "    'low': (200000, 300000),\n",
    "    'lower_mid': (300000, 400000),\n",
    "    'mid': (400000, 500000),\n",
    "    'upper_mid': (500000, 650000),\n",
    "    'high': (650000, 850000),\n",
    "    'very_high': (850000, 1500000),\n",
    "    'ultra_high': (1500000, np.inf)\n",
    "}\n",
    "\n",
    "COLUMN_MAP = {\n",
    "    'sumlivingareasqft': 'living_sqft',\n",
    "    'lotsizesqft': 'lot_sqft',\n",
    "    'sumbuildingsqft': 'building_sqft',\n",
    "    'sumbasementsqft': 'basement_sqft',\n",
    "    'basementfinishedsqft': 'basement_finished_sqft',\n",
    "    'sumgaragesqft': 'garage_sqft',\n",
    "    'sumgrossareasqft': 'gross_sqft'\n",
    "}\n",
    "\n",
    "BASE_FEATURES = [\"living_sqft\", \"lot_sqft\", \"year_built\", \"bedrooms\",\n",
    "                 \"full_baths\", \"half_baths\", \"garage_spaces\", \"latitude\", \"longitude\"]\n",
    "\n",
    "CENSUS_FEATURES = [\"pct_bachelors_degree\", \"median_household_income\",\n",
    "                   \"median_home_value\", \"pct_owner_occupied\", \"unemployment_rate\"]\n",
    "\n",
    "IMAGE_CONDITION = [\"gran_c_in\", \"gran_c_ex\", \"gran_c\", \"high_c_in\", \"high_c_ex\", \"high_c\"]\n",
    "\n",
    "IMAGE_BOOLEAN = []  # Add if you have boolean image features\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LDA FUNCTIONS (from v12 script)\n",
    "# ============================================================\n",
    "\n",
    "def extract_prominent_features(parsed_output):\n",
    "    \"\"\"Extract all prominent_features from the parsed output dictionary.\"\"\"\n",
    "    if pd.isna(parsed_output):\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        if isinstance(parsed_output, str):\n",
    "            data = ast.literal_eval(parsed_output)\n",
    "        else:\n",
    "            data = parsed_output\n",
    "\n",
    "        all_features = []\n",
    "        for idx, image_data in data.items():\n",
    "            if 'prominent_features' in image_data:\n",
    "                all_features.extend(image_data['prominent_features'])\n",
    "\n",
    "        return all_features\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and tokenize text.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s-]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    filtered = [t for t in tokens\n",
    "                if t not in STOPWORDS\n",
    "                and len(t) > 1\n",
    "                and not any(char.isdigit() for char in t)]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "\n",
    "def extract_ngrams_from_text(text, min_n=2, max_n=3):\n",
    "    \"\"\"Extract n-grams from text.\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "\n",
    "    cleaned = clean_text(text)\n",
    "    tokens = cleaned.split()\n",
    "\n",
    "    ngrams = []\n",
    "    for n in range(min_n, max_n + 1):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = '_'.join(tokens[i:i + n])\n",
    "            ngrams.append(ngram)\n",
    "\n",
    "    return ' '.join(ngrams)\n",
    "\n",
    "\n",
    "def convert_feature_to_tuple(feature):\n",
    "    \"\"\"Convert a prominent feature phrase to an n-gram tuple.\"\"\"\n",
    "    words = feature.lower().strip().split()\n",
    "    words = [w for w in words if w not in STOPWORDS]\n",
    "    return '_'.join(words) if words else ''\n",
    "\n",
    "\n",
    "def combine_features_and_text(row, comments_col, parsed_col, min_n=2, max_n=3):\n",
    "    \"\"\"Combine prominent features from images with n-grams from text comments.\"\"\"\n",
    "    all_tokens = []\n",
    "\n",
    "    prominent_features = extract_prominent_features(row[parsed_col])\n",
    "    for feature in prominent_features:\n",
    "        feature_tuple = convert_feature_to_tuple(feature)\n",
    "        if feature_tuple:\n",
    "            all_tokens.append(feature_tuple)\n",
    "\n",
    "    comments_ngrams = extract_ngrams_from_text(row[comments_col], min_n, max_n)\n",
    "    if comments_ngrams:\n",
    "        all_tokens.extend(comments_ngrams.split())\n",
    "\n",
    "    # Deduplicate\n",
    "    seen = set()\n",
    "    unique_tokens = []\n",
    "    for token in all_tokens:\n",
    "        if token not in seen:\n",
    "            seen.add(token)\n",
    "            unique_tokens.append(token)\n",
    "\n",
    "    return ' '.join(unique_tokens)\n",
    "\n",
    "\n",
    "def sparsify_topics(lda_model, max_features_per_topic):\n",
    "    \"\"\"Limit each topic to only its top N features.\"\"\"\n",
    "    for topic_idx in range(lda_model.components_.shape[0]):\n",
    "        topic = lda_model.components_[topic_idx]\n",
    "        top_indices = topic.argsort()[-max_features_per_topic:]\n",
    "        mask = np.zeros_like(topic, dtype=bool)\n",
    "        mask[top_indices] = True\n",
    "        topic[~mask] = 0\n",
    "        if topic.sum() > 0:\n",
    "            lda_model.components_[topic_idx] = topic / topic.sum()\n",
    "\n",
    "    return lda_model\n",
    "\n",
    "\n",
    "def train_lda_model(df):\n",
    "    \"\"\"Train LDA model on combined image+text features.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING LDA MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create combined documents\n",
    "    print(\"Combining image features and text comments...\")\n",
    "    df['combined_document'] = df.apply(\n",
    "        lambda row: combine_features_and_text(row, COMMENTS_COL, PARSED_OUTPUT_COL, MIN_N, MAX_N),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    documents = df['combined_document'].tolist()\n",
    "    print(f\"Created {len(documents)} documents\")\n",
    "\n",
    "    # Train LDA\n",
    "    vectorizer = CountVectorizer(max_features=MAX_FEATURES, min_df=1)\n",
    "    doc_term_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "    print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
    "    print(f\"Training LDA with {N_TOPICS} topics...\")\n",
    "\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=N_TOPICS,\n",
    "        topic_word_prior=TOPIC_WORD_PRIOR,\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=20,\n",
    "        learning_method='batch'\n",
    "    )\n",
    "    lda.fit(doc_term_matrix)\n",
    "    lda = sparsify_topics(lda, MAX_FEATURES_PER_TOPIC)\n",
    "\n",
    "    # Get topic distributions for all documents\n",
    "    doc_topics = lda.transform(doc_term_matrix)\n",
    "\n",
    "    # Add topic features to dataframe\n",
    "    for i in range(N_TOPICS):\n",
    "        df[f'topic_{i+1}'] = doc_topics[:, i]\n",
    "\n",
    "    print(f\"✓ Added {N_TOPICS} topic features to dataframe\")\n",
    "\n",
    "    return df, lda, vectorizer\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# XGBOOST FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def load_and_prep(filepath):\n",
    "    \"\"\"Load and prep data.\"\"\"\n",
    "    print(f\"\\nLoading {filepath}...\")\n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df = df.rename(columns=COLUMN_MAP)\n",
    "\n",
    "    price_col = next((c for c in ['sale_price', 'currentsalesprice', 'price'] if c in df.columns), None)\n",
    "    if not price_col:\n",
    "        raise ValueError(\"No price column found\")\n",
    "\n",
    "    if 'living_sqft' not in df.columns:\n",
    "        raise ValueError(\"living_sqft not found\")\n",
    "\n",
    "    df = df[df[price_col] >= MIN_PRICE].copy()\n",
    "    print(f\"Records after price filter: {len(df):,}\")\n",
    "\n",
    "    # Engineer features\n",
    "    df['sqft_per_bedroom'] = df['living_sqft'] / (df['bedrooms'] + 1)\n",
    "    df['lot_to_living_ratio'] = df['lot_sqft'] / (df['living_sqft'] + 1)\n",
    "    df['property_age'] = 2024 - df['year_built']\n",
    "    df['has_garage'] = (df['garage_spaces'] > 0).astype('int8')\n",
    "    df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "\n",
    "    if 'basement_finished_sqft' in df.columns and 'basement_sqft' in df.columns:\n",
    "        df['basement_finished_ratio'] = df['basement_finished_sqft'] / (df['basement_sqft'] + 1)\n",
    "\n",
    "    if 'garage_sqft' in df.columns:\n",
    "        df['has_large_garage'] = (df['garage_sqft'] > 500).astype('int8')\n",
    "\n",
    "    if 'gross_sqft' in df.columns:\n",
    "        df['living_to_gross_ratio'] = df['living_sqft'] / (df['gross_sqft'] + 1)\n",
    "\n",
    "    # Geo clusters\n",
    "    valid = df[['latitude', 'longitude']].notna().all(axis=1)\n",
    "    if valid.sum() >= 8:\n",
    "        kmeans = MiniBatchKMeans(n_clusters=8, random_state=RANDOM_STATE, batch_size=1000)\n",
    "        df.loc[valid, 'geo_cluster'] = kmeans.fit_predict(df.loc[valid, ['latitude', 'longitude']])\n",
    "        df['geo_cluster'] = df['geo_cluster'].fillna(0)\n",
    "    else:\n",
    "        df['geo_cluster'] = 0\n",
    "\n",
    "    # Assign price tiers\n",
    "    df['price_tier'] = df[price_col].apply(\n",
    "        lambda p: next((t for t, (l, h) in PRICE_TIERS.items() if l <= p < h), 'ultra_high'))\n",
    "\n",
    "    return df, price_col\n",
    "\n",
    "\n",
    "def train_lda_on_split(train_df, test_df):\n",
    "    \"\"\"Train LDA ONLY on training data, then transform both train and test.\"\"\"\n",
    "\n",
    "    # Create combined documents for training data only\n",
    "    train_docs = train_df.apply(\n",
    "        lambda row: combine_features_and_text(row, COMMENTS_COL, PARSED_OUTPUT_COL, MIN_N, MAX_N),\n",
    "        axis=1\n",
    "    ).tolist()\n",
    "\n",
    "    # Train LDA on training documents only\n",
    "    vectorizer = CountVectorizer(max_features=MAX_FEATURES, min_df=1)\n",
    "    train_matrix = vectorizer.fit_transform(train_docs)\n",
    "\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=N_TOPICS,\n",
    "        topic_word_prior=TOPIC_WORD_PRIOR,\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=20,\n",
    "        learning_method='batch'\n",
    "    )\n",
    "    lda.fit(train_matrix)\n",
    "    lda = sparsify_topics(lda, MAX_FEATURES_PER_TOPIC)\n",
    "\n",
    "    # Transform training data\n",
    "    train_topics = lda.transform(train_matrix)\n",
    "    for i in range(N_TOPICS):\n",
    "        train_df[f'topic_{i+1}'] = train_topics[:, i]\n",
    "\n",
    "    # Create and transform test documents using trained vectorizer and LDA\n",
    "    test_docs = test_df.apply(\n",
    "        lambda row: combine_features_and_text(row, COMMENTS_COL, PARSED_OUTPUT_COL, MIN_N, MAX_N),\n",
    "        axis=1\n",
    "    ).tolist()\n",
    "\n",
    "    test_matrix = vectorizer.transform(test_docs)  # Use transform, not fit_transform\n",
    "    test_topics = lda.transform(test_matrix)\n",
    "    for i in range(N_TOPICS):\n",
    "        test_df[f'topic_{i+1}'] = test_topics[:, i]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def train_tier(tier_df, features, price_col, tier_name, include_topics=True, has_lda_columns=True):\n",
    "    \"\"\"Train one tier with or without topic features - NO DATA LEAKAGE.\"\"\"\n",
    "\n",
    "    # Split data FIRST\n",
    "    train_df, test_df = train_test_split(tier_df, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Train LDA on training data only if topics are requested\n",
    "    if include_topics and has_lda_columns:\n",
    "        train_df, test_df = train_lda_on_split(train_df.copy(), test_df.copy())\n",
    "\n",
    "    # Calculate cluster features from training data only\n",
    "    if 'geo_cluster' in train_df.columns:\n",
    "        train_stats = train_df.groupby('geo_cluster')[price_col].agg(['mean', 'median']).reset_index()\n",
    "        train_stats.columns = ['geo_cluster', 'cluster_avg_price', 'cluster_med_price']\n",
    "\n",
    "        train_df = train_df.drop(['cluster_avg_price', 'cluster_med_price'], axis=1, errors='ignore')\n",
    "        train_df = train_df.merge(train_stats, on='geo_cluster', how='left')\n",
    "\n",
    "        test_df = test_df.drop(['cluster_avg_price', 'cluster_med_price'], axis=1, errors='ignore')\n",
    "        test_df = test_df.merge(train_stats, on='geo_cluster', how='left')\n",
    "\n",
    "        train_median = train_df[price_col].median()\n",
    "        train_df[['cluster_avg_price', 'cluster_med_price']] = train_df[\n",
    "            ['cluster_avg_price', 'cluster_med_price']].fillna(train_median)\n",
    "        test_df[['cluster_avg_price', 'cluster_med_price']] = test_df[\n",
    "            ['cluster_avg_price', 'cluster_med_price']].fillna(train_median)\n",
    "\n",
    "        features_with_cluster = features + ['cluster_avg_price', 'cluster_med_price']\n",
    "    else:\n",
    "        features_with_cluster = features\n",
    "\n",
    "    # Impute using training data only\n",
    "    train_medians = train_df[features_with_cluster].median()\n",
    "    train_df[features_with_cluster] = train_df[features_with_cluster].fillna(train_medians)\n",
    "    test_df[features_with_cluster] = test_df[features_with_cluster].fillna(train_medians)\n",
    "\n",
    "    X_train, y_train = train_df[features_with_cluster].values, train_df[price_col].values\n",
    "    X_test, y_test = test_df[features_with_cluster].values, test_df[price_col].values\n",
    "\n",
    "    # Log transform\n",
    "    y_train_model = np.log1p(y_train)\n",
    "\n",
    "    # Train quantile model (median)\n",
    "    model = XGBRegressor(\n",
    "        objective='reg:quantileerror',\n",
    "        quantile_alpha=0.5,\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        tree_method='hist',\n",
    "        verbosity=0\n",
    "    )\n",
    "    model.fit(X_train, y_train_model, verbose=False)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = np.expm1(model.predict(X_test))\n",
    "\n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'metrics': {\n",
    "            'n_train': len(X_train),\n",
    "            'n_test': len(X_test),\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'r2': r2\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution with and without LDA topics - NO DATA LEAKAGE.\"\"\"\n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STRATIFIED AVM WITH LDA TOPICS COMPARISON\")\n",
    "    print(\"✓ LDA trained on TRAINING data only per tier\")\n",
    "    print(\"✓ No data leakage: test data never seen during LDA training\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Load data\n",
    "    df, price_col = load_and_prep(UNIFIED_DATA_PATH)\n",
    "\n",
    "    # Debug: Check what columns we have\n",
    "    print(f\"\\n🔍 DEBUG: Checking for LDA columns...\")\n",
    "    print(f\"Looking for: '{COMMENTS_COL}' and '{PARSED_OUTPUT_COL}'\")\n",
    "\n",
    "    # Show columns that contain 'comment' or 'parsed'\n",
    "    comment_cols = [c for c in df.columns if 'comment' in c.lower()]\n",
    "    parsed_cols = [c for c in df.columns if 'parsed' in c.lower() or 'output' in c.lower()]\n",
    "\n",
    "    if comment_cols:\n",
    "        print(f\"Found comment-related columns: {comment_cols}\")\n",
    "    if parsed_cols:\n",
    "        print(f\"Found parsed/output-related columns: {parsed_cols}\")\n",
    "\n",
    "    if not comment_cols and not parsed_cols:\n",
    "        print(f\"No comment or parsed columns found in dataframe\")\n",
    "        print(f\"First 10 columns: {df.columns.tolist()[:10]}\")\n",
    "\n",
    "    # Check if we have necessary columns for LDA\n",
    "    has_lda_columns = (COMMENTS_COL in df.columns and PARSED_OUTPUT_COL in df.columns)\n",
    "\n",
    "    if not has_lda_columns:\n",
    "        print(f\"\\n⚠ Missing {COMMENTS_COL} or {PARSED_OUTPUT_COL} - skipping LDA\")\n",
    "        print(\"Running baseline model only...\")\n",
    "\n",
    "    # Get base features\n",
    "    all_base_features = BASE_FEATURES + CENSUS_FEATURES + IMAGE_CONDITION + IMAGE_BOOLEAN + [\n",
    "        'sqft_per_bedroom', 'lot_to_living_ratio', 'property_age', 'has_garage',\n",
    "        'log_sqft', 'geo_cluster', 'cluster_avg_price', 'cluster_med_price',\n",
    "        'basement_finished_ratio', 'has_large_garage', 'living_to_gross_ratio'\n",
    "    ]\n",
    "    base_features = [f for f in all_base_features if f in df.columns]\n",
    "\n",
    "    # Topic features will be added per-tier during training (no leakage)\n",
    "    topic_features = [f'topic_{i+1}' for i in range(N_TOPICS)]\n",
    "    features_with_topics = base_features + topic_features\n",
    "\n",
    "    # Fill missing values for base features only\n",
    "    df[base_features] = df[base_features].fillna(df[base_features].median())\n",
    "    df = df.dropna(subset=[price_col])\n",
    "\n",
    "    print(f\"\\nBase features: {len(base_features)}\")\n",
    "    if has_lda_columns:\n",
    "        print(f\"Topic features (added per-tier): {len(topic_features)}\")\n",
    "        print(f\"Total with topics: {len(features_with_topics)}\")\n",
    "\n",
    "    # Train both models (with and without topics)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING MODELS BY PRICE TIER\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results_baseline = {}\n",
    "    results_with_topics = {} if has_lda_columns else None\n",
    "\n",
    "    comparison_data = []\n",
    "\n",
    "    for tier_name, (low, high) in PRICE_TIERS.items():\n",
    "        tier_df = df[df['price_tier'] == tier_name].copy()\n",
    "\n",
    "        if len(tier_df) < 50:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{tier_name} (${low/1000:.0f}K-${high/1000:.0f}K): {len(tier_df):,} samples\")\n",
    "\n",
    "        # Train baseline (without topics)\n",
    "        result_baseline = train_tier(tier_df, base_features, price_col, tier_name,\n",
    "                                     include_topics=False, has_lda_columns=has_lda_columns)\n",
    "        results_baseline[tier_name] = result_baseline\n",
    "\n",
    "        mae_base = result_baseline['metrics']['mae']\n",
    "        mape_base = result_baseline['metrics']['mape']\n",
    "        r2_base = result_baseline['metrics']['r2']\n",
    "\n",
    "        print(f\"  Baseline:     MAE=${mae_base:,.0f} | MAPE={mape_base:.2f}% | R²={r2_base:.3f}\")\n",
    "\n",
    "        # Train with topics if available\n",
    "        if has_lda_columns:\n",
    "            result_topics = train_tier(tier_df, features_with_topics, price_col, tier_name,\n",
    "                                      include_topics=True, has_lda_columns=has_lda_columns)\n",
    "            results_with_topics[tier_name] = result_topics\n",
    "\n",
    "            mae_topics = result_topics['metrics']['mae']\n",
    "            mape_topics = result_topics['metrics']['mape']\n",
    "            r2_topics = result_topics['metrics']['r2']\n",
    "\n",
    "            # Calculate improvement\n",
    "            mape_improvement = ((mape_base - mape_topics) / mape_base) * 100\n",
    "\n",
    "            print(f\"  With Topics:  MAE=${mae_topics:,.0f} | MAPE={mape_topics:.2f}% | R²={r2_topics:.3f}\")\n",
    "\n",
    "            improvement_symbol = \"↓\" if mape_topics < mape_base else \"↑\"\n",
    "            print(f\"  Δ MAPE:       {improvement_symbol} {abs(mape_improvement):.2f}%\")\n",
    "\n",
    "            comparison_data.append({\n",
    "                'tier': tier_name,\n",
    "                'n_samples': len(tier_df),\n",
    "                'baseline_mape': mape_base,\n",
    "                'topics_mape': mape_topics,\n",
    "                'mape_improvement_pct': mape_improvement,\n",
    "                'baseline_mae': mae_base,\n",
    "                'topics_mae': mae_topics,\n",
    "                'baseline_r2': r2_base,\n",
    "                'topics_r2': r2_topics\n",
    "            })\n",
    "\n",
    "    # Overall summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"OVERALL COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if has_lda_columns and comparison_data:\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "        avg_baseline_mape = comparison_df['baseline_mape'].mean()\n",
    "        avg_topics_mape = comparison_df['topics_mape'].mean()\n",
    "        overall_improvement = ((avg_baseline_mape - avg_topics_mape) / avg_baseline_mape) * 100\n",
    "\n",
    "        print(f\"\\nAverage MAPE (Baseline):    {avg_baseline_mape:.2f}%\")\n",
    "        print(f\"Average MAPE (With Topics): {avg_topics_mape:.2f}%\")\n",
    "        print(f\"Overall Improvement:        {overall_improvement:+.2f}%\")\n",
    "\n",
    "        # Save comparison\n",
    "        import os\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        comparison_df.to_csv(f\"{OUTPUT_DIR}/lda_comparison.csv\", index=False)\n",
    "\n",
    "        print(f\"\\n✓ Saved comparison to {OUTPUT_DIR}/lda_comparison.csv\")\n",
    "\n",
    "    print(f\"\\nTotal time: {time.time() - start:.1f}s\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DATA LEAKAGE PREVENTION:\")\n",
    "    print(\"✓ Train/test split happens FIRST\")\n",
    "    print(\"✓ LDA trained only on training data per tier\")\n",
    "    print(\"✓ Test data transformed using trained LDA (never seen during fit)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "c930074a1877884e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STRATIFIED AVM WITH LDA TOPICS COMPARISON\n",
      "✓ LDA trained on TRAINING data only per tier\n",
      "✓ No data leakage: test data never seen during LDA training\n",
      "============================================================\n",
      "\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/input_census_neighborhood_image_data_12K.csv...\n",
      "Records after price filter: 9,366\n",
      "\n",
      "🔍 DEBUG: Checking for LDA columns...\n",
      "Looking for: 'public_listing_comments' and 'parsed_output'\n",
      "Found comment-related columns: ['public_listing_comments']\n",
      "Found parsed/output-related columns: ['parsed_output']\n",
      "\n",
      "Base features: 26\n",
      "Topic features (added per-tier): 8\n",
      "Total with topics: 34\n",
      "\n",
      "============================================================\n",
      "TRAINING MODELS BY PRICE TIER\n",
      "============================================================\n",
      "\n",
      "very_low ($0K-$200K): 3,183 samples\n",
      "  Baseline:     MAE=$10,056 | MAPE=6.75% | R²=0.572\n",
      "  With Topics:  MAE=$11,491 | MAPE=7.95% | R²=0.554\n",
      "  Δ MAPE:       ↑ 17.82%\n",
      "\n",
      "low ($200K-$300K): 2,526 samples\n",
      "  Baseline:     MAE=$8,385 | MAPE=3.34% | R²=0.604\n",
      "  With Topics:  MAE=$8,694 | MAPE=3.46% | R²=0.591\n",
      "  Δ MAPE:       ↑ 3.54%\n",
      "\n",
      "lower_mid ($300K-$400K): 1,255 samples\n",
      "  Baseline:     MAE=$9,036 | MAPE=2.57% | R²=0.566\n",
      "  With Topics:  MAE=$9,236 | MAPE=2.63% | R²=0.561\n",
      "  Δ MAPE:       ↑ 2.27%\n",
      "\n",
      "mid ($400K-$500K): 809 samples\n",
      "  Baseline:     MAE=$9,142 | MAPE=2.07% | R²=0.608\n",
      "  With Topics:  MAE=$9,849 | MAPE=2.22% | R²=0.593\n",
      "  Δ MAPE:       ↑ 7.28%\n",
      "\n",
      "upper_mid ($500K-$650K): 679 samples\n",
      "  Baseline:     MAE=$10,620 | MAPE=1.78% | R²=0.677\n",
      "  With Topics:  MAE=$11,356 | MAPE=1.90% | R²=0.630\n",
      "  Δ MAPE:       ↑ 6.91%\n",
      "\n",
      "high ($650K-$850K): 412 samples\n",
      "  Baseline:     MAE=$10,938 | MAPE=1.52% | R²=0.779\n",
      "  With Topics:  MAE=$12,178 | MAPE=1.68% | R²=0.690\n",
      "  Δ MAPE:       ↑ 10.74%\n",
      "\n",
      "very_high ($850K-$1500K): 259 samples\n",
      "  Baseline:     MAE=$63,655 | MAPE=5.31% | R²=0.633\n",
      "  With Topics:  MAE=$61,490 | MAPE=5.09% | R²=0.666\n",
      "  Δ MAPE:       ↓ 4.31%\n",
      "\n",
      "ultra_high ($1500K-$infK): 243 samples\n",
      "  Baseline:     MAE=$5,803,556 | MAPE=27.39% | R²=0.026\n",
      "  With Topics:  MAE=$3,770,984 | MAPE=23.17% | R²=0.435\n",
      "  Δ MAPE:       ↓ 15.41%\n",
      "\n",
      "============================================================\n",
      "OVERALL COMPARISON\n",
      "============================================================\n",
      "\n",
      "Average MAPE (Baseline):    6.34%\n",
      "Average MAPE (With Topics): 6.01%\n",
      "Overall Improvement:        +5.19%\n",
      "\n",
      "✓ Saved comparison to /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/lda_comparison.csv\n",
      "\n",
      "Total time: 22.0s\n",
      "\n",
      "============================================================\n",
      "DATA LEAKAGE PREVENTION:\n",
      "✓ Train/test split happens FIRST\n",
      "✓ LDA trained only on training data per tier\n",
      "✓ Test data transformed using trained LDA (never seen during fit)\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T23:00:54.300843Z",
     "start_time": "2025-12-19T23:00:38.690420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import ast\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CONFIG\n",
    "UNIFIED_DATA_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/input_census_neighborhood_image_data_12K.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "MIN_PRICE = 100000\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# NMF CONFIG\n",
    "COMMENTS_COL = 'public_listing_comments'\n",
    "PARSED_OUTPUT_COL = 'parsed_output'\n",
    "N_TOPICS = 8\n",
    "MIN_N = 2\n",
    "MAX_N = 3\n",
    "MAX_FEATURES = 200\n",
    "\n",
    "# NMF-specific parameters\n",
    "NMF_INIT = 'nndsvda'  # Non-negative double SVD initialization\n",
    "NMF_ALPHA = 0.1  # Regularization strength\n",
    "NMF_L1_RATIO = 0.5  # Balance between L1 and L2 regularization\n",
    "\n",
    "STOPWORDS = {\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
    "    'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was', 'will',\n",
    "    'with', 'w/', 'been', 'all', 'this', 'you', 'into', 'offers',\n",
    "    'offered', 'includes', 'some', 'tons', 'nice', 'ample', 'yrs',\n",
    "    'adorable', 'charm', 'character', 'beautiful', 'stunning', 'unique',\n",
    "    'modern', 'decorative', 'tastefully', 'tasteful', 'convenient', 'spacious',\n",
    "    'plus', 'lovely', 'charming', 'gorgeous', 'amazing', 'wonderful', 'perfect',\n",
    "    'excellent', 'fantastic', 'incredible', 'magnificent', 'spectacular',\n",
    "    'delightful', 'elegant', 'cozy', 'inviting', 'warm', 'bright', 'airy',\n",
    "    'luxurious', 'premium', 'quality', 'great', 'good', 'better', 'best',\n",
    "    'original', 'covered', 'welcomes', 'showcasing', 'spanning', 'framed',\n",
    "    'abound', 'throughout'\n",
    "}\n",
    "\n",
    "# Price tiers\n",
    "PRICE_TIERS = {\n",
    "    'very_low': (0, 200000),\n",
    "    'low': (200000, 300000),\n",
    "    'lower_mid': (300000, 400000),\n",
    "    'mid': (400000, 500000),\n",
    "    'upper_mid': (500000, 650000),\n",
    "    'high': (650000, 850000),\n",
    "    'very_high': (850000, 1500000),\n",
    "    'ultra_high': (1500000, np.inf)\n",
    "}\n",
    "\n",
    "COLUMN_MAP = {\n",
    "    'sumlivingareasqft': 'living_sqft',\n",
    "    'lotsizesqft': 'lot_sqft',\n",
    "    'sumbuildingsqft': 'building_sqft',\n",
    "    'sumbasementsqft': 'basement_sqft',\n",
    "    'basementfinishedsqft': 'basement_finished_sqft',\n",
    "    'sumgaragesqft': 'garage_sqft',\n",
    "    'sumgrossareasqft': 'gross_sqft'\n",
    "}\n",
    "\n",
    "BASE_FEATURES = [\"living_sqft\", \"lot_sqft\", \"year_built\", \"bedrooms\",\n",
    "                 \"full_baths\", \"half_baths\", \"garage_spaces\", \"latitude\", \"longitude\"]\n",
    "\n",
    "CENSUS_FEATURES = [\"pct_bachelors_degree\", \"median_household_income\",\n",
    "                   \"median_home_value\", \"pct_owner_occupied\", \"unemployment_rate\"]\n",
    "\n",
    "IMAGE_CONDITION = [\"gran_c_in\", \"gran_c_ex\", \"gran_c\", \"high_c_in\", \"high_c_ex\", \"high_c\"]\n",
    "\n",
    "IMAGE_BOOLEAN = []\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# NMF FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def extract_prominent_features(parsed_output):\n",
    "    \"\"\"Extract all prominent_features from the parsed output dictionary.\"\"\"\n",
    "    if pd.isna(parsed_output):\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        if isinstance(parsed_output, str):\n",
    "            data = ast.literal_eval(parsed_output)\n",
    "        else:\n",
    "            data = parsed_output\n",
    "\n",
    "        all_features = []\n",
    "        for idx, image_data in data.items():\n",
    "            if 'prominent_features' in image_data:\n",
    "                all_features.extend(image_data['prominent_features'])\n",
    "\n",
    "        return all_features\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and tokenize text.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s-]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    filtered = [t for t in tokens\n",
    "                if t not in STOPWORDS\n",
    "                and len(t) > 1\n",
    "                and not any(char.isdigit() for char in t)]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "\n",
    "def extract_ngrams_from_text(text, min_n=2, max_n=3):\n",
    "    \"\"\"Extract n-grams from text.\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "\n",
    "    cleaned = clean_text(text)\n",
    "    tokens = cleaned.split()\n",
    "\n",
    "    ngrams = []\n",
    "    for n in range(min_n, max_n + 1):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = '_'.join(tokens[i:i + n])\n",
    "            ngrams.append(ngram)\n",
    "\n",
    "    return ' '.join(ngrams)\n",
    "\n",
    "\n",
    "def convert_feature_to_tuple(feature):\n",
    "    \"\"\"Convert a prominent feature phrase to an n-gram tuple.\"\"\"\n",
    "    words = feature.lower().strip().split()\n",
    "    words = [w for w in words if w not in STOPWORDS]\n",
    "    return '_'.join(words) if words else ''\n",
    "\n",
    "\n",
    "def combine_features_and_text(row, comments_col, parsed_col, min_n=2, max_n=3):\n",
    "    \"\"\"Combine prominent features from images with n-grams from text comments.\"\"\"\n",
    "    all_tokens = []\n",
    "\n",
    "    prominent_features = extract_prominent_features(row[parsed_col])\n",
    "    for feature in prominent_features:\n",
    "        feature_tuple = convert_feature_to_tuple(feature)\n",
    "        if feature_tuple:\n",
    "            all_tokens.append(feature_tuple)\n",
    "\n",
    "    comments_ngrams = extract_ngrams_from_text(row[comments_col], min_n, max_n)\n",
    "    if comments_ngrams:\n",
    "        all_tokens.extend(comments_ngrams.split())\n",
    "\n",
    "    # Deduplicate\n",
    "    seen = set()\n",
    "    unique_tokens = []\n",
    "    for token in all_tokens:\n",
    "        if token not in seen:\n",
    "            seen.add(token)\n",
    "            unique_tokens.append(token)\n",
    "\n",
    "    return ' '.join(unique_tokens)\n",
    "\n",
    "\n",
    "def train_nmf_on_split(train_df, test_df):\n",
    "    \"\"\"Train NMF ONLY on training data, then transform both train and test.\"\"\"\n",
    "\n",
    "    # Create combined documents for training data only\n",
    "    train_docs = train_df.apply(\n",
    "        lambda row: combine_features_and_text(row, COMMENTS_COL, PARSED_OUTPUT_COL, MIN_N, MAX_N),\n",
    "        axis=1\n",
    "    ).tolist()\n",
    "\n",
    "    # Train NMF on training documents only using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=MAX_FEATURES,\n",
    "        min_df=1,\n",
    "        ngram_range=(1, 1)  # Use unigrams since we already created n-grams\n",
    "    )\n",
    "    train_matrix = vectorizer.fit_transform(train_docs)\n",
    "\n",
    "    # Train NMF model\n",
    "    nmf = NMF(\n",
    "        n_components=N_TOPICS,\n",
    "        init=NMF_INIT,\n",
    "        random_state=RANDOM_STATE,\n",
    "        alpha_W=NMF_ALPHA,\n",
    "        alpha_H=NMF_ALPHA,\n",
    "        l1_ratio=NMF_L1_RATIO,\n",
    "        max_iter=200\n",
    "    )\n",
    "    train_topics = nmf.fit_transform(train_matrix)\n",
    "\n",
    "    # Add topic features to training data\n",
    "    for i in range(N_TOPICS):\n",
    "        train_df[f'topic_{i+1}'] = train_topics[:, i]\n",
    "\n",
    "    # Transform test documents using trained vectorizer and NMF\n",
    "    test_docs = test_df.apply(\n",
    "        lambda row: combine_features_and_text(row, COMMENTS_COL, PARSED_OUTPUT_COL, MIN_N, MAX_N),\n",
    "        axis=1\n",
    "    ).tolist()\n",
    "\n",
    "    test_matrix = vectorizer.transform(test_docs)  # Use transform, not fit_transform\n",
    "\n",
    "    # Check if test_matrix has all zeros (no matching vocabulary)\n",
    "    # Add small epsilon to avoid all-zero matrices\n",
    "    if test_matrix.max() == 0:\n",
    "        # If completely empty, use zeros for topics\n",
    "        test_topics = np.zeros((len(test_docs), N_TOPICS))\n",
    "    else:\n",
    "        # Add small epsilon to avoid numerical issues\n",
    "        test_matrix_dense = test_matrix.toarray()\n",
    "        test_matrix_dense = test_matrix_dense + 1e-10\n",
    "\n",
    "        # Convert back to sparse if beneficial\n",
    "        from scipy.sparse import csr_matrix\n",
    "        test_matrix_adjusted = csr_matrix(test_matrix_dense)\n",
    "\n",
    "        try:\n",
    "            test_topics = nmf.transform(test_matrix_adjusted)\n",
    "        except ValueError:\n",
    "            # If transform still fails, use zeros\n",
    "            print(\"  ⚠ Warning: NMF transform failed, using zero topics for test set\")\n",
    "            test_topics = np.zeros((len(test_docs), N_TOPICS))\n",
    "\n",
    "    # Add topic features to test data\n",
    "    for i in range(N_TOPICS):\n",
    "        test_df[f'topic_{i+1}'] = test_topics[:, i]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# XGBOOST FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def load_and_prep(filepath):\n",
    "    \"\"\"Load and prep data.\"\"\"\n",
    "    print(f\"\\nLoading {filepath}...\")\n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df = df.rename(columns=COLUMN_MAP)\n",
    "\n",
    "    price_col = next((c for c in ['sale_price', 'currentsalesprice', 'price'] if c in df.columns), None)\n",
    "    if not price_col:\n",
    "        raise ValueError(\"No price column found\")\n",
    "\n",
    "    if 'living_sqft' not in df.columns:\n",
    "        raise ValueError(\"living_sqft not found\")\n",
    "\n",
    "    df = df[df[price_col] >= MIN_PRICE].copy()\n",
    "    print(f\"Records after price filter: {len(df):,}\")\n",
    "\n",
    "    # Engineer features\n",
    "    df['sqft_per_bedroom'] = df['living_sqft'] / (df['bedrooms'] + 1)\n",
    "    df['lot_to_living_ratio'] = df['lot_sqft'] / (df['living_sqft'] + 1)\n",
    "    df['property_age'] = 2024 - df['year_built']\n",
    "    df['has_garage'] = (df['garage_spaces'] > 0).astype('int8')\n",
    "    df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "\n",
    "    if 'basement_finished_sqft' in df.columns and 'basement_sqft' in df.columns:\n",
    "        df['basement_finished_ratio'] = df['basement_finished_sqft'] / (df['basement_sqft'] + 1)\n",
    "\n",
    "    if 'garage_sqft' in df.columns:\n",
    "        df['has_large_garage'] = (df['garage_sqft'] > 500).astype('int8')\n",
    "\n",
    "    if 'gross_sqft' in df.columns:\n",
    "        df['living_to_gross_ratio'] = df['living_sqft'] / (df['gross_sqft'] + 1)\n",
    "\n",
    "    # Geo clusters\n",
    "    valid = df[['latitude', 'longitude']].notna().all(axis=1)\n",
    "    if valid.sum() >= 8:\n",
    "        kmeans = MiniBatchKMeans(n_clusters=8, random_state=RANDOM_STATE, batch_size=1000)\n",
    "        df.loc[valid, 'geo_cluster'] = kmeans.fit_predict(df.loc[valid, ['latitude', 'longitude']])\n",
    "        df['geo_cluster'] = df['geo_cluster'].fillna(0)\n",
    "    else:\n",
    "        df['geo_cluster'] = 0\n",
    "\n",
    "    # Assign price tiers\n",
    "    df['price_tier'] = df[price_col].apply(\n",
    "        lambda p: next((t for t, (l, h) in PRICE_TIERS.items() if l <= p < h), 'ultra_high'))\n",
    "\n",
    "    return df, price_col\n",
    "\n",
    "\n",
    "def train_tier(tier_df, features, price_col, tier_name, include_topics=True, has_nmf_columns=True):\n",
    "    \"\"\"Train one tier with or without topic features - NO DATA LEAKAGE.\"\"\"\n",
    "\n",
    "    # Split data FIRST\n",
    "    train_df, test_df = train_test_split(tier_df, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Train NMF on training data only if topics are requested\n",
    "    if include_topics and has_nmf_columns:\n",
    "        train_df, test_df = train_nmf_on_split(train_df.copy(), test_df.copy())\n",
    "\n",
    "    # Calculate cluster features from training data only\n",
    "    if 'geo_cluster' in train_df.columns:\n",
    "        train_stats = train_df.groupby('geo_cluster')[price_col].agg(['mean', 'median']).reset_index()\n",
    "        train_stats.columns = ['geo_cluster', 'cluster_avg_price', 'cluster_med_price']\n",
    "\n",
    "        train_df = train_df.drop(['cluster_avg_price', 'cluster_med_price'], axis=1, errors='ignore')\n",
    "        train_df = train_df.merge(train_stats, on='geo_cluster', how='left')\n",
    "\n",
    "        test_df = test_df.drop(['cluster_avg_price', 'cluster_med_price'], axis=1, errors='ignore')\n",
    "        test_df = test_df.merge(train_stats, on='geo_cluster', how='left')\n",
    "\n",
    "        train_median = train_df[price_col].median()\n",
    "        train_df[['cluster_avg_price', 'cluster_med_price']] = train_df[\n",
    "            ['cluster_avg_price', 'cluster_med_price']].fillna(train_median)\n",
    "        test_df[['cluster_avg_price', 'cluster_med_price']] = test_df[\n",
    "            ['cluster_avg_price', 'cluster_med_price']].fillna(train_median)\n",
    "\n",
    "        features_with_cluster = features + ['cluster_avg_price', 'cluster_med_price']\n",
    "    else:\n",
    "        features_with_cluster = features\n",
    "\n",
    "    # Impute using training data only\n",
    "    train_medians = train_df[features_with_cluster].median()\n",
    "    train_df[features_with_cluster] = train_df[features_with_cluster].fillna(train_medians)\n",
    "    test_df[features_with_cluster] = test_df[features_with_cluster].fillna(train_medians)\n",
    "\n",
    "    X_train, y_train = train_df[features_with_cluster].values, train_df[price_col].values\n",
    "    X_test, y_test = test_df[features_with_cluster].values, test_df[price_col].values\n",
    "\n",
    "    # Log transform\n",
    "    y_train_model = np.log1p(y_train)\n",
    "\n",
    "    # Train quantile model (median)\n",
    "    model = XGBRegressor(\n",
    "        objective='reg:quantileerror',\n",
    "        quantile_alpha=0.5,\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        tree_method='hist',\n",
    "        verbosity=0\n",
    "    )\n",
    "    model.fit(X_train, y_train_model, verbose=False)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = np.expm1(model.predict(X_test))\n",
    "\n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'metrics': {\n",
    "            'n_train': len(X_train),\n",
    "            'n_test': len(X_test),\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'r2': r2\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution with and without NMF topics - NO DATA LEAKAGE.\"\"\"\n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STRATIFIED AVM WITH NMF TOPICS COMPARISON\")\n",
    "    print(\"✓ NMF trained on TRAINING data only per tier\")\n",
    "    print(\"✓ Uses TF-IDF vectorization (better for NMF)\")\n",
    "    print(\"✓ No data leakage: test data never seen during NMF training\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Load data\n",
    "    df, price_col = load_and_prep(UNIFIED_DATA_PATH)\n",
    "\n",
    "    # Debug: Check what columns we have\n",
    "    print(f\"\\n🔍 DEBUG: Checking for NMF columns...\")\n",
    "    print(f\"Looking for: '{COMMENTS_COL}' and '{PARSED_OUTPUT_COL}'\")\n",
    "\n",
    "    comment_cols = [c for c in df.columns if 'comment' in c.lower()]\n",
    "    parsed_cols = [c for c in df.columns if 'parsed' in c.lower() or 'output' in c.lower()]\n",
    "\n",
    "    if comment_cols:\n",
    "        print(f\"Found comment-related columns: {comment_cols}\")\n",
    "    if parsed_cols:\n",
    "        print(f\"Found parsed/output-related columns: {parsed_cols}\")\n",
    "\n",
    "    if not comment_cols and not parsed_cols:\n",
    "        print(f\"No comment or parsed columns found in dataframe\")\n",
    "        print(f\"First 10 columns: {df.columns.tolist()[:10]}\")\n",
    "\n",
    "    # Check if we have necessary columns for NMF\n",
    "    has_nmf_columns = (COMMENTS_COL in df.columns and PARSED_OUTPUT_COL in df.columns)\n",
    "\n",
    "    if not has_nmf_columns:\n",
    "        print(f\"\\n⚠ Missing {COMMENTS_COL} or {PARSED_OUTPUT_COL} - skipping NMF\")\n",
    "        print(\"Running baseline model only...\")\n",
    "\n",
    "    # Get base features\n",
    "    all_base_features = BASE_FEATURES + CENSUS_FEATURES + IMAGE_CONDITION + IMAGE_BOOLEAN + [\n",
    "        'sqft_per_bedroom', 'lot_to_living_ratio', 'property_age', 'has_garage',\n",
    "        'log_sqft', 'geo_cluster', 'cluster_avg_price', 'cluster_med_price',\n",
    "        'basement_finished_ratio', 'has_large_garage', 'living_to_gross_ratio'\n",
    "    ]\n",
    "    base_features = [f for f in all_base_features if f in df.columns]\n",
    "\n",
    "    # Topic features will be added per-tier during training (no leakage)\n",
    "    topic_features = [f'topic_{i+1}' for i in range(N_TOPICS)]\n",
    "    features_with_topics = base_features + topic_features\n",
    "\n",
    "    # Fill missing values for base features only\n",
    "    df[base_features] = df[base_features].fillna(df[base_features].median())\n",
    "    df = df.dropna(subset=[price_col])\n",
    "\n",
    "    print(f\"\\nBase features: {len(base_features)}\")\n",
    "    if has_nmf_columns:\n",
    "        print(f\"NMF topic features (added per-tier): {len(topic_features)}\")\n",
    "        print(f\"Total with topics: {len(features_with_topics)}\")\n",
    "\n",
    "    # Train both models (with and without topics)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING MODELS BY PRICE TIER\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results_baseline = {}\n",
    "    results_with_topics = {} if has_nmf_columns else None\n",
    "\n",
    "    comparison_data = []\n",
    "\n",
    "    for tier_name, (low, high) in PRICE_TIERS.items():\n",
    "        tier_df = df[df['price_tier'] == tier_name].copy()\n",
    "\n",
    "        if len(tier_df) < 50:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{tier_name} (${low/1000:.0f}K-${high/1000:.0f}K): {len(tier_df):,} samples\")\n",
    "\n",
    "        # Train baseline (without topics)\n",
    "        result_baseline = train_tier(tier_df, base_features, price_col, tier_name,\n",
    "                                     include_topics=False, has_nmf_columns=has_nmf_columns)\n",
    "        results_baseline[tier_name] = result_baseline\n",
    "\n",
    "        mae_base = result_baseline['metrics']['mae']\n",
    "        mape_base = result_baseline['metrics']['mape']\n",
    "        r2_base = result_baseline['metrics']['r2']\n",
    "\n",
    "        print(f\"  Baseline:     MAE=${mae_base:,.0f} | MAPE={mape_base:.2f}% | R²={r2_base:.3f}\")\n",
    "\n",
    "        # Train with topics if available\n",
    "        if has_nmf_columns:\n",
    "            result_topics = train_tier(tier_df, features_with_topics, price_col, tier_name,\n",
    "                                      include_topics=True, has_nmf_columns=has_nmf_columns)\n",
    "            results_with_topics[tier_name] = result_topics\n",
    "\n",
    "            mae_topics = result_topics['metrics']['mae']\n",
    "            mape_topics = result_topics['metrics']['mape']\n",
    "            r2_topics = result_topics['metrics']['r2']\n",
    "\n",
    "            # Calculate improvement\n",
    "            mape_improvement = ((mape_base - mape_topics) / mape_base) * 100\n",
    "\n",
    "            print(f\"  With NMF:     MAE=${mae_topics:,.0f} | MAPE={mape_topics:.2f}% | R²={r2_topics:.3f}\")\n",
    "\n",
    "            improvement_symbol = \"↓\" if mape_topics < mape_base else \"↑\"\n",
    "            print(f\"  Δ MAPE:       {improvement_symbol} {abs(mape_improvement):.2f}%\")\n",
    "\n",
    "            comparison_data.append({\n",
    "                'tier': tier_name,\n",
    "                'n_samples': len(tier_df),\n",
    "                'baseline_mape': mape_base,\n",
    "                'nmf_mape': mape_topics,\n",
    "                'mape_improvement_pct': mape_improvement,\n",
    "                'baseline_mae': mae_base,\n",
    "                'nmf_mae': mae_topics,\n",
    "                'baseline_r2': r2_base,\n",
    "                'nmf_r2': r2_topics\n",
    "            })\n",
    "\n",
    "    # Overall summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"OVERALL COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if has_nmf_columns and comparison_data:\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "        avg_baseline_mape = comparison_df['baseline_mape'].mean()\n",
    "        avg_nmf_mape = comparison_df['nmf_mape'].mean()\n",
    "        overall_improvement = ((avg_baseline_mape - avg_nmf_mape) / avg_baseline_mape) * 100\n",
    "\n",
    "        print(f\"\\nAverage MAPE (Baseline):  {avg_baseline_mape:.2f}%\")\n",
    "        print(f\"Average MAPE (With NMF):  {avg_nmf_mape:.2f}%\")\n",
    "        print(f\"Overall Improvement:      {overall_improvement:+.2f}%\")\n",
    "\n",
    "        # Save comparison\n",
    "        import os\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        comparison_df.to_csv(f\"{OUTPUT_DIR}/nmf_comparison.csv\", index=False)\n",
    "\n",
    "        print(f\"\\n✓ Saved comparison to {OUTPUT_DIR}/nmf_comparison.csv\")\n",
    "\n",
    "    print(f\"\\nTotal time: {time.time() - start:.1f}s\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"NMF vs LDA DIFFERENCES:\")\n",
    "    print(\"• NMF uses TF-IDF (vs CountVectorizer)\")\n",
    "    print(\"• NMF finds additive parts-based topics\")\n",
    "    print(\"• NMF often more interpretable for text\")\n",
    "    print(\"• NMF enforces non-negativity\")\n",
    "    print(\"\\n\" + \"DATA LEAKAGE PREVENTION:\")\n",
    "    print(\"✓ Train/test split happens FIRST\")\n",
    "    print(\"✓ NMF trained only on training data per tier\")\n",
    "    print(\"✓ Test data transformed using trained NMF (never seen during fit)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "ca4634f008c0bdd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STRATIFIED AVM WITH NMF TOPICS COMPARISON\n",
      "✓ NMF trained on TRAINING data only per tier\n",
      "✓ Uses TF-IDF vectorization (better for NMF)\n",
      "✓ No data leakage: test data never seen during NMF training\n",
      "============================================================\n",
      "\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/input_census_neighborhood_image_data_12K.csv...\n",
      "Records after price filter: 9,366\n",
      "\n",
      "🔍 DEBUG: Checking for NMF columns...\n",
      "Looking for: 'public_listing_comments' and 'parsed_output'\n",
      "Found comment-related columns: ['public_listing_comments']\n",
      "Found parsed/output-related columns: ['parsed_output']\n",
      "\n",
      "Base features: 26\n",
      "NMF topic features (added per-tier): 8\n",
      "Total with topics: 34\n",
      "\n",
      "============================================================\n",
      "TRAINING MODELS BY PRICE TIER\n",
      "============================================================\n",
      "\n",
      "very_low ($0K-$200K): 3,183 samples\n",
      "  Baseline:     MAE=$10,056 | MAPE=6.75% | R²=0.572\n",
      "  ⚠ Warning: NMF transform failed, using zero topics for test set\n",
      "  With NMF:     MAE=$10,195 | MAPE=6.82% | R²=0.573\n",
      "  Δ MAPE:       ↑ 1.00%\n",
      "\n",
      "low ($200K-$300K): 2,526 samples\n",
      "  Baseline:     MAE=$8,385 | MAPE=3.34% | R²=0.604\n",
      "  ⚠ Warning: NMF transform failed, using zero topics for test set\n",
      "  With NMF:     MAE=$8,073 | MAPE=3.22% | R²=0.621\n",
      "  Δ MAPE:       ↓ 3.64%\n",
      "\n",
      "lower_mid ($300K-$400K): 1,255 samples\n",
      "  Baseline:     MAE=$9,036 | MAPE=2.57% | R²=0.566\n",
      "  ⚠ Warning: NMF transform failed, using zero topics for test set\n",
      "  With NMF:     MAE=$8,558 | MAPE=2.44% | R²=0.589\n",
      "  Δ MAPE:       ↓ 5.24%\n",
      "\n",
      "mid ($400K-$500K): 809 samples\n",
      "  Baseline:     MAE=$9,142 | MAPE=2.07% | R²=0.608\n",
      "  ⚠ Warning: NMF transform failed, using zero topics for test set\n",
      "  With NMF:     MAE=$9,733 | MAPE=2.20% | R²=0.575\n",
      "  Δ MAPE:       ↑ 6.43%\n",
      "\n",
      "upper_mid ($500K-$650K): 679 samples\n",
      "  Baseline:     MAE=$10,620 | MAPE=1.78% | R²=0.677\n",
      "  ⚠ Warning: NMF transform failed, using zero topics for test set\n",
      "  With NMF:     MAE=$10,997 | MAPE=1.85% | R²=0.658\n",
      "  Δ MAPE:       ↑ 3.95%\n",
      "\n",
      "high ($650K-$850K): 412 samples\n",
      "  Baseline:     MAE=$10,938 | MAPE=1.52% | R²=0.779\n",
      "  ⚠ Warning: NMF transform failed, using zero topics for test set\n",
      "  With NMF:     MAE=$11,305 | MAPE=1.58% | R²=0.769\n",
      "  Δ MAPE:       ↑ 3.54%\n",
      "\n",
      "very_high ($850K-$1500K): 259 samples\n",
      "  Baseline:     MAE=$63,655 | MAPE=5.31% | R²=0.633\n",
      "  ⚠ Warning: NMF transform failed, using zero topics for test set\n",
      "  With NMF:     MAE=$63,406 | MAPE=5.30% | R²=0.641\n",
      "  Δ MAPE:       ↓ 0.19%\n",
      "\n",
      "ultra_high ($1500K-$infK): 243 samples\n",
      "  Baseline:     MAE=$5,803,556 | MAPE=27.39% | R²=0.026\n",
      "  ⚠ Warning: NMF transform failed, using zero topics for test set\n",
      "  With NMF:     MAE=$5,016,720 | MAPE=23.42% | R²=0.216\n",
      "  Δ MAPE:       ↓ 14.51%\n",
      "\n",
      "============================================================\n",
      "OVERALL COMPARISON\n",
      "============================================================\n",
      "\n",
      "Average MAPE (Baseline):  6.34%\n",
      "Average MAPE (With NMF):  5.85%\n",
      "Overall Improvement:      +7.72%\n",
      "\n",
      "✓ Saved comparison to /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/nmf_comparison.csv\n",
      "\n",
      "Total time: 15.6s\n",
      "\n",
      "============================================================\n",
      "NMF vs LDA DIFFERENCES:\n",
      "• NMF uses TF-IDF (vs CountVectorizer)\n",
      "• NMF finds additive parts-based topics\n",
      "• NMF often more interpretable for text\n",
      "• NMF enforces non-negativity\n",
      "\n",
      "DATA LEAKAGE PREVENTION:\n",
      "✓ Train/test split happens FIRST\n",
      "✓ NMF trained only on training data per tier\n",
      "✓ Test data transformed using trained NMF (never seen during fit)\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T23:12:47.562936Z",
     "start_time": "2025-12-19T23:12:31.455414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import ast\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CONFIG\n",
    "UNIFIED_DATA_PATH = \"/Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/input_census_neighborhood_image_data_12K.csv\"\n",
    "OUTPUT_DIR = \"/Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs\"\n",
    "MIN_PRICE = 100000\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# NMF CONFIG\n",
    "COMMENTS_COL = 'public_listing_comments'\n",
    "PARSED_OUTPUT_COL = 'parsed_output'\n",
    "N_TOPICS = 8\n",
    "MIN_N = 2\n",
    "MAX_N = 3\n",
    "MAX_FEATURES = 200\n",
    "\n",
    "# NMF-specific parameters\n",
    "NMF_INIT = 'nndsvda'  # Non-negative double SVD initialization\n",
    "NMF_ALPHA = 0.1  # Regularization strength\n",
    "NMF_L1_RATIO = 0.5  # Balance between L1 and L2 regularization\n",
    "\n",
    "STOPWORDS = {\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
    "    'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was', 'will',\n",
    "    'with', 'w/', 'been', 'all', 'this', 'you', 'into', 'offers',\n",
    "    'offered', 'includes', 'some', 'tons', 'nice', 'ample', 'yrs',\n",
    "    'adorable', 'charm', 'character', 'beautiful', 'stunning', 'unique',\n",
    "    'modern', 'decorative', 'tastefully', 'tasteful', 'convenient', 'spacious',\n",
    "    'plus', 'lovely', 'charming', 'gorgeous', 'amazing', 'wonderful', 'perfect',\n",
    "    'excellent', 'fantastic', 'incredible', 'magnificent', 'spectacular',\n",
    "    'delightful', 'elegant', 'cozy', 'inviting', 'warm', 'bright', 'airy',\n",
    "    'luxurious', 'premium', 'quality', 'great', 'good', 'better', 'best',\n",
    "    'original', 'covered', 'welcomes', 'showcasing', 'spanning', 'framed',\n",
    "    'abound', 'throughout'\n",
    "}\n",
    "\n",
    "# Price tiers\n",
    "PRICE_TIERS = {\n",
    "    'very_low': (0, 200000),\n",
    "    'low': (200000, 300000),\n",
    "    'lower_mid': (300000, 400000),\n",
    "    'mid': (400000, 500000),\n",
    "    'upper_mid': (500000, 650000),\n",
    "    'high': (650000, 850000),\n",
    "    'very_high': (850000, 1500000),\n",
    "    'ultra_high': (1500000, np.inf)\n",
    "}\n",
    "\n",
    "COLUMN_MAP = {\n",
    "    'sumlivingareasqft': 'living_sqft',\n",
    "    'lotsizesqft': 'lot_sqft',\n",
    "    'sumbuildingsqft': 'building_sqft',\n",
    "    'sumbasementsqft': 'basement_sqft',\n",
    "    'basementfinishedsqft': 'basement_finished_sqft',\n",
    "    'sumgaragesqft': 'garage_sqft',\n",
    "    'sumgrossareasqft': 'gross_sqft'\n",
    "}\n",
    "\n",
    "BASE_FEATURES = [\"living_sqft\", \"lot_sqft\", \"year_built\", \"bedrooms\",\n",
    "                 \"full_baths\", \"half_baths\", \"garage_spaces\", \"latitude\", \"longitude\"]\n",
    "\n",
    "CENSUS_FEATURES = [\"pct_bachelors_degree\", \"median_household_income\",\n",
    "                   \"median_home_value\", \"pct_owner_occupied\", \"unemployment_rate\"]\n",
    "\n",
    "IMAGE_CONDITION = [\"gran_c_in\", \"gran_c_ex\", \"gran_c\", \"high_c_in\", \"high_c_ex\", \"high_c\"]\n",
    "\n",
    "IMAGE_BOOLEAN = []\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# NMF FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def extract_prominent_features(parsed_output):\n",
    "    \"\"\"Extract all prominent_features from the parsed output dictionary.\"\"\"\n",
    "    if pd.isna(parsed_output):\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        if isinstance(parsed_output, str):\n",
    "            data = ast.literal_eval(parsed_output)\n",
    "        else:\n",
    "            data = parsed_output\n",
    "\n",
    "        all_features = []\n",
    "        for idx, image_data in data.items():\n",
    "            if 'prominent_features' in image_data:\n",
    "                all_features.extend(image_data['prominent_features'])\n",
    "\n",
    "        return all_features\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and tokenize text.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s-]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    filtered = [t for t in tokens\n",
    "                if t not in STOPWORDS\n",
    "                and len(t) > 1\n",
    "                and not any(char.isdigit() for char in t)]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "\n",
    "def extract_ngrams_from_text(text, min_n=2, max_n=3):\n",
    "    \"\"\"Extract n-grams from text.\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "\n",
    "    cleaned = clean_text(text)\n",
    "    tokens = cleaned.split()\n",
    "\n",
    "    ngrams = []\n",
    "    for n in range(min_n, max_n + 1):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = '_'.join(tokens[i:i + n])\n",
    "            ngrams.append(ngram)\n",
    "\n",
    "    return ' '.join(ngrams)\n",
    "\n",
    "\n",
    "def convert_feature_to_tuple(feature):\n",
    "    \"\"\"Convert a prominent feature phrase to an n-gram tuple.\"\"\"\n",
    "    words = feature.lower().strip().split()\n",
    "    words = [w for w in words if w not in STOPWORDS]\n",
    "    return '_'.join(words) if words else ''\n",
    "\n",
    "\n",
    "def combine_features_and_text(row, comments_col, parsed_col, min_n=2, max_n=3):\n",
    "    \"\"\"Combine prominent features from images with n-grams from text comments.\"\"\"\n",
    "    all_tokens = []\n",
    "\n",
    "    prominent_features = extract_prominent_features(row[parsed_col])\n",
    "    for feature in prominent_features:\n",
    "        feature_tuple = convert_feature_to_tuple(feature)\n",
    "        if feature_tuple:\n",
    "            all_tokens.append(feature_tuple)\n",
    "\n",
    "    comments_ngrams = extract_ngrams_from_text(row[comments_col], min_n, max_n)\n",
    "    if comments_ngrams:\n",
    "        all_tokens.extend(comments_ngrams.split())\n",
    "\n",
    "    # Deduplicate\n",
    "    seen = set()\n",
    "    unique_tokens = []\n",
    "    for token in all_tokens:\n",
    "        if token not in seen:\n",
    "            seen.add(token)\n",
    "            unique_tokens.append(token)\n",
    "\n",
    "    return ' '.join(unique_tokens)\n",
    "\n",
    "\n",
    "def train_nmf_on_split(train_df, test_df):\n",
    "    \"\"\"Train NMF ONLY on training data, then transform both train and test.\"\"\"\n",
    "\n",
    "    # Create combined documents for training data only\n",
    "    train_docs = train_df.apply(\n",
    "        lambda row: combine_features_and_text(row, COMMENTS_COL, PARSED_OUTPUT_COL, MIN_N, MAX_N),\n",
    "        axis=1\n",
    "    ).tolist()\n",
    "\n",
    "    # Train NMF on training documents only using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=MAX_FEATURES,\n",
    "        min_df=1,\n",
    "        ngram_range=(1, 1)  # Use unigrams since we already created n-grams\n",
    "    )\n",
    "    train_matrix = vectorizer.fit_transform(train_docs)\n",
    "\n",
    "    # Train NMF model\n",
    "    nmf = NMF(\n",
    "        n_components=N_TOPICS,\n",
    "        init=NMF_INIT,\n",
    "        random_state=RANDOM_STATE,\n",
    "        alpha_W=NMF_ALPHA,\n",
    "        alpha_H=NMF_ALPHA,\n",
    "        l1_ratio=NMF_L1_RATIO,\n",
    "        max_iter=200\n",
    "    )\n",
    "    train_topics = nmf.fit_transform(train_matrix)\n",
    "\n",
    "    # Add topic features to training data\n",
    "    for i in range(N_TOPICS):\n",
    "        train_df[f'topic_{i+1}'] = train_topics[:, i]\n",
    "\n",
    "    # Transform test documents using trained vectorizer and NMF\n",
    "    test_docs = test_df.apply(\n",
    "        lambda row: combine_features_and_text(row, COMMENTS_COL, PARSED_OUTPUT_COL, MIN_N, MAX_N),\n",
    "        axis=1\n",
    "    ).tolist()\n",
    "\n",
    "    test_matrix = vectorizer.transform(test_docs)  # Use transform, not fit_transform\n",
    "\n",
    "    # Check if test_matrix has all zeros (no matching vocabulary)\n",
    "    # Add small epsilon to avoid all-zero matrices\n",
    "    if test_matrix.max() == 0:\n",
    "        # If completely empty, use zeros for topics\n",
    "        test_topics = np.zeros((len(test_docs), N_TOPICS))\n",
    "    else:\n",
    "        # Add small epsilon to avoid numerical issues\n",
    "        test_matrix_dense = test_matrix.toarray()\n",
    "        test_matrix_dense = test_matrix_dense + 1e-10\n",
    "\n",
    "        # Convert back to sparse if beneficial\n",
    "        from scipy.sparse import csr_matrix\n",
    "        test_matrix_adjusted = csr_matrix(test_matrix_dense)\n",
    "\n",
    "        try:\n",
    "            test_topics = nmf.transform(test_matrix_adjusted)\n",
    "        except ValueError:\n",
    "            # If transform still fails, use zeros\n",
    "            print(\"  ⚠ Warning: NMF transform failed, using zero topics for test set\")\n",
    "            test_topics = np.zeros((len(test_docs), N_TOPICS))\n",
    "\n",
    "    # Add topic features to test data\n",
    "    for i in range(N_TOPICS):\n",
    "        test_df[f'topic_{i+1}'] = test_topics[:, i]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# XGBOOST FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def load_and_prep(filepath):\n",
    "    \"\"\"Load and prep data.\"\"\"\n",
    "    print(f\"\\nLoading {filepath}...\")\n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df = df.rename(columns=COLUMN_MAP)\n",
    "\n",
    "    price_col = next((c for c in ['sale_price', 'currentsalesprice', 'price'] if c in df.columns), None)\n",
    "    if not price_col:\n",
    "        raise ValueError(\"No price column found\")\n",
    "\n",
    "    if 'living_sqft' not in df.columns:\n",
    "        raise ValueError(\"living_sqft not found\")\n",
    "\n",
    "    df = df[df[price_col] >= MIN_PRICE].copy()\n",
    "    print(f\"Records after price filter: {len(df):,}\")\n",
    "\n",
    "    # Engineer features\n",
    "    df['sqft_per_bedroom'] = df['living_sqft'] / (df['bedrooms'] + 1)\n",
    "    df['lot_to_living_ratio'] = df['lot_sqft'] / (df['living_sqft'] + 1)\n",
    "    df['property_age'] = 2024 - df['year_built']\n",
    "    df['has_garage'] = (df['garage_spaces'] > 0).astype('int8')\n",
    "    df['log_sqft'] = np.log1p(df['living_sqft'])\n",
    "\n",
    "    if 'basement_finished_sqft' in df.columns and 'basement_sqft' in df.columns:\n",
    "        df['basement_finished_ratio'] = df['basement_finished_sqft'] / (df['basement_sqft'] + 1)\n",
    "\n",
    "    if 'garage_sqft' in df.columns:\n",
    "        df['has_large_garage'] = (df['garage_sqft'] > 500).astype('int8')\n",
    "\n",
    "    if 'gross_sqft' in df.columns:\n",
    "        df['living_to_gross_ratio'] = df['living_sqft'] / (df['gross_sqft'] + 1)\n",
    "\n",
    "    # Geo clusters\n",
    "    valid = df[['latitude', 'longitude']].notna().all(axis=1)\n",
    "    if valid.sum() >= 8:\n",
    "        kmeans = MiniBatchKMeans(n_clusters=8, random_state=RANDOM_STATE, batch_size=1000)\n",
    "        df.loc[valid, 'geo_cluster'] = kmeans.fit_predict(df.loc[valid, ['latitude', 'longitude']])\n",
    "        df['geo_cluster'] = df['geo_cluster'].fillna(0)\n",
    "    else:\n",
    "        df['geo_cluster'] = 0\n",
    "\n",
    "    # Assign price tiers\n",
    "    df['price_tier'] = df[price_col].apply(\n",
    "        lambda p: next((t for t, (l, h) in PRICE_TIERS.items() if l <= p < h), 'ultra_high'))\n",
    "\n",
    "    return df, price_col\n",
    "\n",
    "\n",
    "def train_tier(tier_df, features, price_col, tier_name, include_topics=True, has_nmf_columns=True):\n",
    "    \"\"\"Train one tier with or without topic features - NO DATA LEAKAGE.\"\"\"\n",
    "\n",
    "    # Split data FIRST\n",
    "    train_df, test_df = train_test_split(tier_df, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Train NMF on training data only if topics are requested\n",
    "    if include_topics and has_nmf_columns:\n",
    "        train_df, test_df = train_nmf_on_split(train_df.copy(), test_df.copy())\n",
    "\n",
    "    # Calculate cluster features from training data only\n",
    "    if 'geo_cluster' in train_df.columns:\n",
    "        train_stats = train_df.groupby('geo_cluster')[price_col].agg(['mean', 'median']).reset_index()\n",
    "        train_stats.columns = ['geo_cluster', 'cluster_avg_price', 'cluster_med_price']\n",
    "\n",
    "        train_df = train_df.drop(['cluster_avg_price', 'cluster_med_price'], axis=1, errors='ignore')\n",
    "        train_df = train_df.merge(train_stats, on='geo_cluster', how='left')\n",
    "\n",
    "        test_df = test_df.drop(['cluster_avg_price', 'cluster_med_price'], axis=1, errors='ignore')\n",
    "        test_df = test_df.merge(train_stats, on='geo_cluster', how='left')\n",
    "\n",
    "        train_median = train_df[price_col].median()\n",
    "        train_df[['cluster_avg_price', 'cluster_med_price']] = train_df[\n",
    "            ['cluster_avg_price', 'cluster_med_price']].fillna(train_median)\n",
    "        test_df[['cluster_avg_price', 'cluster_med_price']] = test_df[\n",
    "            ['cluster_avg_price', 'cluster_med_price']].fillna(train_median)\n",
    "\n",
    "        features_with_cluster = features + ['cluster_avg_price', 'cluster_med_price']\n",
    "    else:\n",
    "        features_with_cluster = features\n",
    "\n",
    "    # Impute using training data only\n",
    "    train_medians = train_df[features_with_cluster].median()\n",
    "    train_df[features_with_cluster] = train_df[features_with_cluster].fillna(train_medians)\n",
    "    test_df[features_with_cluster] = test_df[features_with_cluster].fillna(train_medians)\n",
    "\n",
    "    X_train, y_train = train_df[features_with_cluster].values, train_df[price_col].values\n",
    "    X_test, y_test = test_df[features_with_cluster].values, test_df[price_col].values\n",
    "\n",
    "    # Log transform\n",
    "    y_train_model = np.log1p(y_train)\n",
    "\n",
    "    # Train quantile model (median)\n",
    "    model = XGBRegressor(\n",
    "        objective='reg:quantileerror',\n",
    "        quantile_alpha=0.5,\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        tree_method='hist',\n",
    "        verbosity=0\n",
    "    )\n",
    "    model.fit(X_train, y_train_model, verbose=False)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = np.expm1(model.predict(X_test))\n",
    "\n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'metrics': {\n",
    "            'n_train': len(X_train),\n",
    "            'n_test': len(X_test),\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'r2': r2\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution with and without NMF topics - NO DATA LEAKAGE.\"\"\"\n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STRATIFIED AVM WITH NMF TOPICS COMPARISON\")\n",
    "    print(\"✓ NMF trained on TRAINING data only per tier\")\n",
    "    print(\"✓ Uses CountVectorizer (like LDA) to avoid vocabulary mismatch\")\n",
    "    print(\"✓ No data leakage: test data never seen during NMF training\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Load data\n",
    "    df, price_col = load_and_prep(UNIFIED_DATA_PATH)\n",
    "\n",
    "    # Debug: Check what columns we have\n",
    "    print(f\"\\n🔍 DEBUG: Checking for NMF columns...\")\n",
    "    print(f\"Looking for: '{COMMENTS_COL}' and '{PARSED_OUTPUT_COL}'\")\n",
    "\n",
    "    comment_cols = [c for c in df.columns if 'comment' in c.lower()]\n",
    "    parsed_cols = [c for c in df.columns if 'parsed' in c.lower() or 'output' in c.lower()]\n",
    "\n",
    "    if comment_cols:\n",
    "        print(f\"Found comment-related columns: {comment_cols}\")\n",
    "    if parsed_cols:\n",
    "        print(f\"Found parsed/output-related columns: {parsed_cols}\")\n",
    "\n",
    "    if not comment_cols and not parsed_cols:\n",
    "        print(f\"No comment or parsed columns found in dataframe\")\n",
    "        print(f\"First 10 columns: {df.columns.tolist()[:10]}\")\n",
    "\n",
    "    # Check if we have necessary columns for NMF\n",
    "    has_nmf_columns = (COMMENTS_COL in df.columns and PARSED_OUTPUT_COL in df.columns)\n",
    "\n",
    "    if not has_nmf_columns:\n",
    "        print(f\"\\n⚠ Missing {COMMENTS_COL} or {PARSED_OUTPUT_COL} - skipping NMF\")\n",
    "        print(\"Running baseline model only...\")\n",
    "\n",
    "    # Get base features\n",
    "    all_base_features = BASE_FEATURES + CENSUS_FEATURES + IMAGE_CONDITION + IMAGE_BOOLEAN + [\n",
    "        'sqft_per_bedroom', 'lot_to_living_ratio', 'property_age', 'has_garage',\n",
    "        'log_sqft', 'geo_cluster', 'cluster_avg_price', 'cluster_med_price',\n",
    "        'basement_finished_ratio', 'has_large_garage', 'living_to_gross_ratio'\n",
    "    ]\n",
    "    base_features = [f for f in all_base_features if f in df.columns]\n",
    "\n",
    "    # Topic features will be added per-tier during training (no leakage)\n",
    "    topic_features = [f'topic_{i+1}' for i in range(N_TOPICS)]\n",
    "    features_with_topics = base_features + topic_features\n",
    "\n",
    "    # NO global imputation - all imputation happens per-tier after split\n",
    "    # Only drop rows with missing price (needed for tier assignment)\n",
    "    df = df.dropna(subset=[price_col])\n",
    "\n",
    "    print(f\"\\nBase features: {len(base_features)}\")\n",
    "    if has_nmf_columns:\n",
    "        print(f\"NMF topic features (added per-tier): {len(topic_features)}\")\n",
    "        print(f\"Total with topics: {len(features_with_topics)}\")\n",
    "\n",
    "    # Train both models (with and without topics)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING MODELS BY PRICE TIER\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results_baseline = {}\n",
    "    results_with_topics = {} if has_nmf_columns else None\n",
    "\n",
    "    comparison_data = []\n",
    "\n",
    "    for tier_name, (low, high) in PRICE_TIERS.items():\n",
    "        tier_df = df[df['price_tier'] == tier_name].copy()\n",
    "\n",
    "        if len(tier_df) < 50:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{tier_name} (${low/1000:.0f}K-${high/1000:.0f}K): {len(tier_df):,} samples\")\n",
    "\n",
    "        # Train baseline (without topics)\n",
    "        result_baseline = train_tier(tier_df, base_features, price_col, tier_name,\n",
    "                                     include_topics=False, has_nmf_columns=has_nmf_columns)\n",
    "        results_baseline[tier_name] = result_baseline\n",
    "\n",
    "        mae_base = result_baseline['metrics']['mae']\n",
    "        mape_base = result_baseline['metrics']['mape']\n",
    "        r2_base = result_baseline['metrics']['r2']\n",
    "\n",
    "        print(f\"  Baseline:     MAE=${mae_base:,.0f} | MAPE={mape_base:.2f}% | R²={r2_base:.3f}\")\n",
    "\n",
    "        # Train with topics if available\n",
    "        if has_nmf_columns:\n",
    "            result_topics = train_tier(tier_df, features_with_topics, price_col, tier_name,\n",
    "                                      include_topics=True, has_nmf_columns=has_nmf_columns)\n",
    "            results_with_topics[tier_name] = result_topics\n",
    "\n",
    "            mae_topics = result_topics['metrics']['mae']\n",
    "            mape_topics = result_topics['metrics']['mape']\n",
    "            r2_topics = result_topics['metrics']['r2']\n",
    "\n",
    "            # Calculate improvement\n",
    "            mape_improvement = ((mape_base - mape_topics) / mape_base) * 100\n",
    "\n",
    "            print(f\"  With NMF:     MAE=${mae_topics:,.0f} | MAPE={mape_topics:.2f}% | R²={r2_topics:.3f}\")\n",
    "\n",
    "            improvement_symbol = \"↓\" if mape_topics < mape_base else \"↑\"\n",
    "            print(f\"  Δ MAPE:       {improvement_symbol} {abs(mape_improvement):.2f}%\")\n",
    "\n",
    "            comparison_data.append({\n",
    "                'tier': tier_name,\n",
    "                'n_samples': len(tier_df),\n",
    "                'baseline_mape': mape_base,\n",
    "                'nmf_mape': mape_topics,\n",
    "                'mape_improvement_pct': mape_improvement,\n",
    "                'baseline_mae': mae_base,\n",
    "                'nmf_mae': mae_topics,\n",
    "                'baseline_r2': r2_base,\n",
    "                'nmf_r2': r2_topics\n",
    "            })\n",
    "\n",
    "    # Overall summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"OVERALL COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if has_nmf_columns and comparison_data:\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "        avg_baseline_mape = comparison_df['baseline_mape'].mean()\n",
    "        avg_nmf_mape = comparison_df['nmf_mape'].mean()\n",
    "        overall_improvement = ((avg_baseline_mape - avg_nmf_mape) / avg_baseline_mape) * 100\n",
    "\n",
    "        print(f\"\\nAverage MAPE (Baseline):  {avg_baseline_mape:.2f}%\")\n",
    "        print(f\"Average MAPE (With NMF):  {avg_nmf_mape:.2f}%\")\n",
    "        print(f\"Overall Improvement:      {overall_improvement:+.2f}%\")\n",
    "\n",
    "        # Save comparison\n",
    "        import os\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        comparison_df.to_csv(f\"{OUTPUT_DIR}/nmf_comparison.csv\", index=False)\n",
    "\n",
    "        print(f\"\\n✓ Saved comparison to {OUTPUT_DIR}/nmf_comparison.csv\")\n",
    "\n",
    "    print(f\"\\nTotal time: {time.time() - start:.1f}s\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"NMF vs LDA DIFFERENCES:\")\n",
    "    print(\"• NMF uses non-negative matrix factorization\")\n",
    "    print(\"• NMF finds additive parts-based topics\")\n",
    "    print(\"• NMF often more interpretable for text\")\n",
    "    print(\"• Both now use CountVectorizer for consistency\")\n",
    "    print(\"\\n\" + \"DATA LEAKAGE PREVENTION:\")\n",
    "    print(\"✓ NO global imputation - all per-tier after split\")\n",
    "    print(\"✓ Train/test split happens FIRST\")\n",
    "    print(\"✓ NMF trained only on training data per tier\")\n",
    "    print(\"✓ Test data transformed using trained NMF (never seen during fit)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "b4ee368da5e70bce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STRATIFIED AVM WITH NMF TOPICS COMPARISON\n",
      "✓ NMF trained on TRAINING data only per tier\n",
      "✓ Uses CountVectorizer (like LDA) to avoid vocabulary mismatch\n",
      "✓ No data leakage: test data never seen during NMF training\n",
      "============================================================\n",
      "\n",
      "Loading /Users/jenny.lin/ImageDataParser/XGBoost_with_ImageData/data/input_census_neighborhood_image_data_12K.csv...\n",
      "Records after price filter: 9,366\n",
      "\n",
      "🔍 DEBUG: Checking for NMF columns...\n",
      "Looking for: 'public_listing_comments' and 'parsed_output'\n",
      "Found comment-related columns: ['public_listing_comments']\n",
      "Found parsed/output-related columns: ['parsed_output']\n",
      "\n",
      "Base features: 26\n",
      "NMF topic features (added per-tier): 8\n",
      "Total with topics: 34\n",
      "\n",
      "============================================================\n",
      "TRAINING MODELS BY PRICE TIER\n",
      "============================================================\n",
      "\n",
      "very_low ($0K-$200K): 3,183 samples\n",
      "  Baseline:     MAE=$10,205 | MAPE=6.85% | R²=0.569\n",
      "  ⚠ Warning: NMF transform failed, using zero topics for test set\n",
      "  With NMF:     MAE=$10,027 | MAPE=6.72% | R²=0.572\n",
      "  Δ MAPE:       ↓ 1.79%\n",
      "\n",
      "low ($200K-$300K): 2,526 samples\n",
      "  Baseline:     MAE=$8,272 | MAPE=3.30% | R²=0.614\n",
      "  ⚠ Warning: NMF transform failed, using zero topics for test set\n",
      "  With NMF:     MAE=$8,148 | MAPE=3.25% | R²=0.616\n",
      "  Δ MAPE:       ↓ 1.43%\n",
      "\n",
      "lower_mid ($300K-$400K): 1,255 samples\n",
      "  Baseline:     MAE=$9,055 | MAPE=2.58% | R²=0.558\n",
      "  ⚠ Warning: NMF transform failed, using zero topics for test set\n",
      "  With NMF:     MAE=$8,692 | MAPE=2.48% | R²=0.581\n",
      "  Δ MAPE:       ↓ 3.88%\n",
      "\n",
      "mid ($400K-$500K): 809 samples\n",
      "  Baseline:     MAE=$9,319 | MAPE=2.11% | R²=0.615\n",
      "  ⚠ Warning: NMF transform failed, using zero topics for test set\n",
      "  With NMF:     MAE=$9,651 | MAPE=2.19% | R²=0.580\n",
      "  Δ MAPE:       ↑ 3.45%\n",
      "\n",
      "upper_mid ($500K-$650K): 679 samples\n",
      "  Baseline:     MAE=$10,768 | MAPE=1.80% | R²=0.656\n",
      "  ⚠ Warning: NMF transform failed, using zero topics for test set\n",
      "  With NMF:     MAE=$10,470 | MAPE=1.75% | R²=0.679\n",
      "  Δ MAPE:       ↓ 2.81%\n",
      "\n",
      "high ($650K-$850K): 412 samples\n",
      "  Baseline:     MAE=$12,477 | MAPE=1.75% | R²=0.754\n",
      "  ⚠ Warning: NMF transform failed, using zero topics for test set\n",
      "  With NMF:     MAE=$10,323 | MAPE=1.43% | R²=0.795\n",
      "  Δ MAPE:       ↓ 18.17%\n",
      "\n",
      "very_high ($850K-$1500K): 259 samples\n",
      "  Baseline:     MAE=$63,501 | MAPE=5.26% | R²=0.642\n",
      "  ⚠ Warning: NMF transform failed, using zero topics for test set\n",
      "  With NMF:     MAE=$60,310 | MAPE=4.98% | R²=0.664\n",
      "  Δ MAPE:       ↓ 5.36%\n",
      "\n",
      "ultra_high ($1500K-$infK): 243 samples\n",
      "  Baseline:     MAE=$5,087,976 | MAPE=24.22% | R²=0.205\n",
      "  ⚠ Warning: NMF transform failed, using zero topics for test set\n",
      "  With NMF:     MAE=$4,436,497 | MAPE=27.36% | R²=0.346\n",
      "  Δ MAPE:       ↑ 12.99%\n",
      "\n",
      "============================================================\n",
      "OVERALL COMPARISON\n",
      "============================================================\n",
      "\n",
      "Average MAPE (Baseline):  5.98%\n",
      "Average MAPE (With NMF):  6.27%\n",
      "Overall Improvement:      -4.80%\n",
      "\n",
      "✓ Saved comparison to /Users/jenny.lin/BASIS_AVM_Onboarding/cate_scenario_analyses/model_outputs/nmf_comparison.csv\n",
      "\n",
      "Total time: 16.1s\n",
      "\n",
      "============================================================\n",
      "NMF vs LDA DIFFERENCES:\n",
      "• NMF uses non-negative matrix factorization\n",
      "• NMF finds additive parts-based topics\n",
      "• NMF often more interpretable for text\n",
      "• Both now use CountVectorizer for consistency\n",
      "\n",
      "DATA LEAKAGE PREVENTION:\n",
      "✓ NO global imputation - all per-tier after split\n",
      "✓ Train/test split happens FIRST\n",
      "✓ NMF trained only on training data per tier\n",
      "✓ Test data transformed using trained NMF (never seen during fit)\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Overall: -4.80% degradation - NMF is actively hurting because XGBoost is fitting to 8 useless zero columns!",
   "id": "b3f4003b8c4ba092"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
